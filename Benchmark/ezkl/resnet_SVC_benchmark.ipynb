{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataset import Dataset\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import hinge_loss\n",
    "\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# EZKL use 3.9.3 ?\n",
    "import ezkl, onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Dictionary for CIFAR10\n",
    "classDict = {'plane': 0, 'car': 1, 'bird': 2, 'cat': 3, 'deer': 4,\n",
    "             'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
    "\n",
    "binaryClasses = {0:'Machine', 1:'Animal'} # Machine , Animal\n",
    "\n",
    "data_mean = (0.4914, 0.4822, 0.4465)\n",
    "data_std = (0.2470, 0.2435, 0.2616)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.transformer import TransformerDecoderLayer\n",
    "# Overwrite getitem method to obtain the index of the images when iterating through the images\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CIFAR10(Dataset):\n",
    "    def __init__(self, train, transform):\n",
    "        self.cifar10 = torchvision.datasets.CIFAR10(\n",
    "                        root='./data', train=train, download=True, transform=transform)\n",
    "        self.targets = self.cifar10.targets\n",
    "        self.classes = self.cifar10.classes\n",
    "        self.data = self.cifar10.data\n",
    "\n",
    "\n",
    "    # Overloaded the getitem method to return index as well\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.cifar10[index]\n",
    "        return data, target, index\n",
    "\n",
    "    # Method to get all images' indices from a certain class without iterating through the loader\n",
    "    def get_index(self, target_label):\n",
    "      index_list = []\n",
    "      for index, label in enumerate(self.targets):\n",
    "        if label == target_label:\n",
    "          index_list.append(index)\n",
    "      return index_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar10)\n",
    "\n",
    "    def remove(self, remove_list):\n",
    "      mask = np.ones(len(self.cifar10), dtype=bool)\n",
    "      mask[remove_list] = False\n",
    "      data = self.data[mask]\n",
    "\n",
    "# Data Prep.\n",
    "inv_normalize = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(data_mean, data_std)],\n",
    "   std= [1/s for s in data_std]\n",
    ")\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(data_mean, data_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(data_mean, data_std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = CIFAR10(train=True, transform=transform_train)\n",
    "testset = CIFAR10(train=False, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(torchvision.models.ResNet):\n",
    "    \"\"\"ResNet generalization for CIFAR-like thingies.\n",
    "\n",
    "    This is a minor modification of\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py,\n",
    "    adding additional options.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=2, zero_init_residual=False,\n",
    "                 groups=1, base_width=64, replace_stride_with_dilation=[False, False, False, False],\n",
    "                 norm_layer=torch.nn.BatchNorm2d, strides=[1, 2, 2, 2], initial_conv=[3, 1, 1]):\n",
    "        \"\"\"Initialize as usual. Layers and strides are scriptable.\"\"\"\n",
    "        super(torchvision.models.ResNet, self).__init__()  # torch.nn.Module\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.dilation = 1\n",
    "        if len(replace_stride_with_dilation) != 4:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 4-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "\n",
    "        self.inplanes = base_width\n",
    "        self.base_width = 64  # Do this to circumvent BasicBlock errors. The value is not actually used.\n",
    "        self.conv1 = torch.nn.Conv2d(3, self.inplanes, kernel_size=initial_conv[0],\n",
    "                                     stride=initial_conv[1], padding=initial_conv[2], bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "        layer_list = []\n",
    "        width = self.inplanes\n",
    "        for idx, layer in enumerate(layers):\n",
    "            layer_list.append(self._make_layer(block, width, layer, stride=strides[idx], dilate=replace_stride_with_dilation[idx]))\n",
    "            width *= 2\n",
    "        self.layers = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = torch.nn.Linear(width // 2 * block.expansion, num_classes)\n",
    "        #self.predict = nn.Sigmoid()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (torch.nn.BatchNorm2d, torch.nn.GroupNorm)):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the arch by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "\n",
    "\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x) # Sigmoid\n",
    "        #x = self.predict(x)\n",
    "        return x\n",
    "\n",
    "initial_conv = [3, 1, 1]\n",
    "NN_model = ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=10, base_width=64, initial_conv=initial_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    x = torch.ones(1, device=device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"Running on a CPU...Uhh, are you sure you want to do this?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up training params\n",
    "epochs = 21\n",
    "eta = 0.01\n",
    "optimizer = torch.optim.SGD(params = NN_model.parameters(), lr = eta, weight_decay = 5e-4, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "NN_model.to(device)\n",
    "NN_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for NN\n",
    "DATASET = 'CIFAR10'      # Choose between 'CIFAR2', 'CIFAR10'\n",
    "MODEL = 'RESNET18'       # Choose between 'RESNET18', 'VGG11'\n",
    "AUGMENTS = True          # Use Data Augmentation\n",
    "SAVEMODEL = False         # Save Clean Model\n",
    "# LOADMODEL = False        # Load Clean Model\n",
    "\n",
    "# Save or Load Clean Model\n",
    "\n",
    "import os\n",
    "PATH = \"./pre_trained\"\n",
    "os.makedirs(PATH, exist_ok = True)\n",
    "PATH += \"/resnet_cifar.ptr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, valid_losses = [], correct = 0, total = 0):\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate Model\n",
    "    for inputs, labels, index in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "            if DATASET == 'CIFAR2':\n",
    "                labels = labels.to(torch.float32)\n",
    "                output = output.flatten()\n",
    "\n",
    "        # negative labels: when using hinge embedding loss only\n",
    "        flipped_labels = labels # * -1\n",
    "        loss = loss_fun(output, flipped_labels)   # Calculate loss\n",
    "\n",
    "        valid_loss = loss_fun(output, labels)\n",
    "        valid_losses.append(valid_loss.item())\n",
    "\n",
    "        #predictions = torch.argmax(output, dim=1)\n",
    "        if DATASET == 'CIFAR2':\n",
    "            predictions = torch.where(output < 0, 0, 1)\n",
    "        else:\n",
    "            predictions = torch.argmax(output.data, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return valid_losses, correct, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.6206142967939376, Accuracy: 0.8369\n"
     ]
    }
   ],
   "source": [
    "# if local model is not supported\n",
    "NN_model.load_state_dict(torch.load(\"./pre_trained/cuda_resnet_cifar.ptr\", map_location='cpu'))\n",
    "NN_model.to('mps')\n",
    "#torch.save(NN_model.state_dict, PATH)\n",
    "\n",
    "valid_losses, correct, total = evaluate_model(testloader, NN_model)\n",
    "print(\"Valid loss: {}, Accuracy: {}\".format(np.mean(valid_losses), correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def rgb_to_gray(images):\n",
    "    # Assuming images are (C, H, W) and C=3 for RGB\n",
    "    return np.dot(images.numpy().transpose((1, 2, 0)), [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "def process_dataset(dataset, label_map=[2, 3, 4, 5, 6, 7]):\n",
    "    x_processed = []\n",
    "    y_processed = []\n",
    "    img_norms = []\n",
    "    for index in range(len(dataset)):\n",
    "        # Retrieve data and target from dataset\n",
    "        data, target, _ = dataset[index]\n",
    "\n",
    "        # Convert image to grayscale\n",
    "        img_grayscale = rgb_to_gray(data)\n",
    "\n",
    "        # Normalize grayscale image\n",
    "        norm = np.linalg.norm(img_grayscale, ord=2)\n",
    "        img_normalized = img_grayscale / norm\n",
    "\n",
    "        # Store the processed image and label\n",
    "        x_processed.append(img_normalized.flatten())\n",
    "        img_norms.append(norm)\n",
    "        y_processed.append(target in label_map)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    x_processed = np.array(x_processed)\n",
    "    y_processed = np.array(y_processed, dtype=np.float32)\n",
    "    img_norms = np.array(img_norms)\n",
    "\n",
    "    return x_processed, y_processed, img_norms\n",
    "\n",
    "# Process trainset and testset\n",
    "x_train, y_train, norms_train = process_dataset(trainset)\n",
    "x_test, y_test, norms_test = process_dataset(testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed[s] :  7.093708038330078\n",
      "Train : 0.72116\n",
      "Test : 0.7106\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#model_SVC = SVC(kernel = 'linear', max_iter=100, probability=True)\n",
    "SVC_model = LinearSVC(loss='hinge', max_iter=30000, dual=True)\n",
    "#fit\n",
    "SVC_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Elapsed[s] : \", time.time() - start_time)\n",
    "print(\"Train :\", SVC_model.score(x_train, y_train))\n",
    "print(\"Test :\", SVC_model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Target Class/Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_imgages(img1, img2, size, inv_normalize=None):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # If an inverse normalization function is provided, apply it\n",
    "    if inv_normalize:\n",
    "        img_to_show = inv_normalize(img1.cpu()).permute(1, 2, 0)\n",
    "    else:\n",
    "        img_to_show = img1.numpy()\n",
    "\n",
    "    # Show original image\n",
    "    axs[0].imshow(img_to_show)  # Transpose back to HxWxC format\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Show normalized grayscale image\n",
    "    axs[1].imshow(img2.reshape(size,size,1), cmap='gray')\n",
    "    axs[1].set_title('Grayscale Normalized Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_image(dataset, norms, ind=0, inv_normalize=None):\n",
    "    # Assuming 'ind' is a valid index for the dataset\n",
    "    # Retrieve data, target, and the index from the dataset\n",
    "    img, target, _ = dataset[ind]\n",
    "\n",
    "    # Convert PyTorch tensor to numpy array if necessary\n",
    "    img_np = img.numpy()\n",
    "\n",
    "    # Convert RGB to grayscale\n",
    "    img_grayscale = np.dot(img_np.transpose((1, 2, 0)), [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "    # Normalize grayscale image using the norm provided\n",
    "    img_normalized = img_grayscale / norms[ind]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # If an inverse normalization function is provided, apply it\n",
    "    if inv_normalize:\n",
    "        img_to_show = inv_normalize(img).permute(1, 2, 0)\n",
    "    else:\n",
    "        img_to_show = img_np\n",
    "\n",
    "    # Show original image\n",
    "    axs[0].imshow(img_to_show)  # Transpose back to HxWxC format\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Show normalized grayscale image\n",
    "    axs[1].imshow(img_normalized, cmap='gray')\n",
    "    axs[1].set_title('Grayscale Normalized Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = [2,3,5] # bird, cat, dog\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "def gen_targetloader(dataset, target_class, num_samples = 100):\n",
    "    target_index = dataset.get_index(target_class)\n",
    "\n",
    "    if len(target_index) > num_samples:\n",
    "        target_index = random.sample(target_index, num_samples)\n",
    "\n",
    "    targetset = data.Subset(dataset, target_index)\n",
    "    targetloader = torch.utils.data.DataLoader(targetset)\n",
    "\n",
    "    return targetloader, target_index\n",
    "\n",
    "def gen_random_subset_dataloader(dataset, num_samples = 100):\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    random_subset = Subset(dataset, indices)\n",
    "\n",
    "    return DataLoader(random_subset), indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_targetloader, bird_indices = gen_targetloader(testset, target_class = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(testset, norms_test, bird_indices[3], inv_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ZKML for Resnet and SVC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create and (potentially train a model)\n",
    "\n",
    "# make sure you have the dependencies required here already installed\n",
    "import json\n",
    "#import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import sk2torch\n",
    "#import torch\n",
    "import ezkl\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './tmp/' created successfully\n"
     ]
    }
   ],
   "source": [
    "async def async_function(data_path, model_path, settings_path, resource_string):\n",
    "    res = await ezkl.calibrate_settings(data_path, model_path, settings_path, resource_string)\n",
    "    assert res == True\n",
    "\n",
    "folder = \"./tmp/\"\n",
    "\n",
    "# Create the directory 'tmp' in the current working directory\n",
    "try:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"Directory '{folder}' created successfully\")\n",
    "except OSError as error:\n",
    "    print(f\"Directory '{folder}' cannot be created. Error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_SVC_verifier(model, grayscale_img, agg = False):\n",
    "    model_path = os.path.join(folder, 'network.onnx')\n",
    "    compiled_model_path = os.path.join(folder, 'network.compiled')\n",
    "    settings_path = os.path.join(folder, 'settings.json') \n",
    "    witness_path = os.path.join(folder, 'witness.json')\n",
    "    data_path = os.path.join(folder, 'input.json')\n",
    "    cal_data_path = os.path.join(folder, 'cal_data.json')\n",
    "\n",
    "    #srs_path = os.path.join(folder, 'kzg.srs')\n",
    "\n",
    "    pk_path = os.path.join(folder, 'test.pk')\n",
    "    vk_path = os.path.join(folder, 'test.vk')\n",
    "    proof_path = os.path.join(folder, 'proof.json')\n",
    "\n",
    "    grayscale_img = grayscale_img.reshape(1,-1)\n",
    "    model = sk2torch.wrap(model)\n",
    "    x = torch.from_numpy(grayscale_img)\n",
    "    torch_out = model.predict(x)\n",
    "\n",
    "    torch.onnx.export(model,               # model being run\n",
    "                    # model input (or a tuple for multiple inputs)\n",
    "                    x,\n",
    "                    # where to save the model (can be a file or file-like object)\n",
    "                    model_path,\n",
    "                    export_params=True,        # store the trained parameter weights inside the model file\n",
    "                    opset_version=10,          # the ONNX version to export the model to\n",
    "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                    input_names=['input'],   # the model's input names\n",
    "                    output_names=['output'],  # the model's output names\n",
    "                    dynamic_axes={'input': {0: 'batch_size'},    # variable length axes\n",
    "                                    'output': {0: 'batch_size'}})\n",
    "\n",
    "    d = ((x).detach().numpy()).reshape([-1]).tolist()\n",
    "\n",
    "    data = dict(input_shapes=[grayscale_img.shape[1:]],\n",
    "                input_data=[d],\n",
    "                output_data=[o.reshape([-1]).tolist() for o in torch_out])\n",
    "\n",
    "    # Serialize data into file:\n",
    "    json.dump(data, open(data_path, 'w'))\n",
    "\n",
    "    #cal_data = dict(input_data = )\n",
    "\n",
    "    !RUST_LOG=trace\n",
    "    # TODO: Dictionary outputs\n",
    "    res = ezkl.gen_settings(model_path, settings_path)\n",
    "    assert res == True\n",
    "\n",
    "    res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\")  # Optimize for resources\n",
    "    #res = async_function(data_path, model_path, settings_path, \"resource\")\n",
    "\n",
    "    res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "    assert res == True\n",
    "\n",
    "    # srs path\n",
    "    res = ezkl.get_srs(settings_path)\n",
    "\n",
    "    res = ezkl.setup(\n",
    "        compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "    )\n",
    "\n",
    "    assert res == True\n",
    "    assert os.path.isfile(vk_path)\n",
    "    assert os.path.isfile(pk_path)\n",
    "    assert os.path.isfile(settings_path)\n",
    "    \n",
    "    # now generate the witness file\n",
    "    res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "    assert os.path.isfile(witness_path)\n",
    "\n",
    "    # GENERATE A PROOF\n",
    "    res = ezkl.prove(\n",
    "            witness_path,\n",
    "            compiled_model_path,\n",
    "            pk_path,\n",
    "            proof_path,\n",
    "\n",
    "            \"single\", # for aggregated EVM proof only ELSE 'evm'\n",
    "        )\n",
    "    print ('Successfully generate ZK Proof!')\n",
    "    #print(res)\n",
    "    assert os.path.isfile(proof_path)\n",
    "\n",
    "    # verify our proof\n",
    "    res = ezkl.verify(\n",
    "            proof_path,\n",
    "            settings_path,\n",
    "            vk_path,\n",
    "        )\n",
    "\n",
    "    assert res == True\n",
    "    print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_SVC_verifier(SVC_model, x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC_model\n",
    "grayscale_img = x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(folder, 'network.onnx')\n",
    "compiled_model_path = os.path.join(folder, 'network.compiled')\n",
    "settings_path = os.path.join(folder, 'settings.json') \n",
    "witness_path = os.path.join(folder, 'witness.json')\n",
    "data_path = os.path.join(folder, 'input.json')\n",
    "cal_data_path = os.path.join(folder, 'cal_data.json')\n",
    "\n",
    "srs_path = os.path.join(folder, 'kzg.srs')\n",
    "\n",
    "pk_path = os.path.join(folder, 'test.pk')\n",
    "vk_path = os.path.join(folder, 'test.vk')\n",
    "proof_path = os.path.join(folder, 'proof.json')\n",
    "\n",
    "grayscale_img = grayscale_img.reshape(1,-1)\n",
    "model = sk2torch.wrap(model)\n",
    "x = torch.from_numpy(grayscale_img)\n",
    "torch_out = model.predict(x)\n",
    "\n",
    "torch.onnx.export(model,               # model being run\n",
    "                # model input (or a tuple for multiple inputs)\n",
    "                x,\n",
    "                # where to save the model (can be a file or file-like object)\n",
    "                model_path,\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=10,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names=['input'],   # the model's input names\n",
    "                output_names=['output'],  # the model's output names\n",
    "                dynamic_axes={'input': {0: 'batch_size'},    # variable length axes\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "d = ((x).detach().numpy()).reshape([-1]).tolist()\n",
    "\n",
    "data = dict(input_shapes=[grayscale_img.shape[1:]],\n",
    "            input_data=[d],\n",
    "            output_data=[o.reshape([-1]).tolist() for o in torch_out])\n",
    "\n",
    "# Serialize data into file:\n",
    "json.dump(data, open(data_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!RUST_LOG=trace\n",
    "# TODO: Dictionary outputs\n",
    "res = ezkl.gen_settings(model_path, settings_path)\n",
    "assert res == True\n",
    "\n",
    "res = async_function(data_path, model_path, settings_path, \"resource\")\n",
    "#assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#await res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ezkl.get_srs(srs_path, settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 2\n",
      "spawning module 2\n"
     ]
    }
   ],
   "source": [
    "res = ezkl.setup(\n",
    "        compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path,\n",
    "    )\n",
    " \n",
    "\n",
    "assert res == True\n",
    "assert os.path.isfile(vk_path)\n",
    "assert os.path.isfile(pk_path)\n",
    "assert os.path.isfile(settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "assert os.path.isfile(witness_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [[[8895048820941913435, 12590578388187041751, 10644156957292843428, 485414809211466155]]], 'proof': '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', 'transcript_type': 'EVM'}\n"
     ]
    }
   ],
   "source": [
    "proof = ezkl.prove(\n",
    "        witness_path,\n",
    "        compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "print(proof)\n",
    "assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified\n"
     ]
    }
   ],
   "source": [
    "# verify our proof\n",
    "res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "assert res == True\n",
    "print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
