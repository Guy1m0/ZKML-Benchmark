{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 21:40:08.791675: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-23 21:40:08.812082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-23 21:40:08.812103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-23 21:40:08.812680: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-23 21:40:08.816062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-23 21:40:09.203421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 21:45:14.818795: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 21:45:14.868079: I external/local_xla/xla/service/service.cc:168] XLA service 0x5600ec013ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-23 21:45:14.868098: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-01-23 21:45:14.872936: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706017514.918089   71597 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.8231 - accuracy: 0.7373 - val_loss: 0.2165 - val_accuracy: 0.9373\n",
      "Epoch 2/3\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2051 - accuracy: 0.9391 - val_loss: 0.1294 - val_accuracy: 0.9618\n",
      "Epoch 3/3\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1313 - accuracy: 0.9601 - val_loss: 0.0896 - val_accuracy: 0.9747\n",
      "313/313 - 0s - loss: 0.1025 - accuracy: 0.9677 - 208ms/epoch - 665us/step\n",
      "\n",
      "Test accuracy: 0.9677000045776367\n"
     ]
    }
   ],
   "source": [
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_tf.fit(train_images, train_labels, epochs=3, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.conv1(x))\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Transfer weights for TensorFlow\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for PyTorch\n",
    "with torch.no_grad():\n",
    "    model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "    model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "TensorFlow Basic Model Output: [[[[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]]\n",
      "PyTorch Basic Model Output: [[[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().permute(3,2,0,1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[1].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "TensorFlow Basic Model Output: [[[[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]]]\n",
      "PyTorch Basic Model Output: [[[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().permute(3,2,0,1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AvgPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[2].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "TensorFlow Basic Model Output: [[[[8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]]\n",
      "\n",
      "  [[8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]]\n",
      "\n",
      "  [[8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]]\n",
      "\n",
      "  [[8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]\n",
      "   [8.4262574e-01 4.7286350e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221945e-02 6.9187993e-01\n",
      "    6.5178031e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340494e-01 3.6525252e-01 1.5889616e-01]]]]\n",
      "PyTorch Basic Model Output: [[[[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]]\n",
      "\n",
      "\n",
      " [[[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]]\n",
      "\n",
      "\n",
      " [[[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]]\n",
      "\n",
      "\n",
      " [[[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]\n",
      "\n",
      "  [[8.4262580e-01 4.7286355e-01 9.8882359e-01 7.0327699e-01\n",
      "    1.5978925e-01 3.1689417e-01 2.0221934e-02 6.9187993e-01\n",
      "    6.5178043e-01 8.5156304e-01 9.5023102e-01 6.4571332e-03\n",
      "    7.1215414e-04 6.0340488e-01 3.6525261e-01 1.5889619e-01]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().permute(3,2,0,1).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
