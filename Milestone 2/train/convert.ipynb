{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 21:40:08.791675: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-23 21:40:08.812082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-23 21:40:08.812103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-23 21:40:08.812680: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-23 21:40:08.816062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-23 21:40:09.203421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 21:45:14.818795: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 21:45:14.868079: I external/local_xla/xla/service/service.cc:168] XLA service 0x5600ec013ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-23 21:45:14.868098: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-01-23 21:45:14.872936: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706017514.918089   71597 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.8231 - accuracy: 0.7373 - val_loss: 0.2165 - val_accuracy: 0.9373\n",
      "Epoch 2/3\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2051 - accuracy: 0.9391 - val_loss: 0.1294 - val_accuracy: 0.9618\n",
      "Epoch 3/3\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1313 - accuracy: 0.9601 - val_loss: 0.0896 - val_accuracy: 0.9747\n",
      "313/313 - 0s - loss: 0.1025 - accuracy: 0.9677 - 208ms/epoch - 665us/step\n",
      "\n",
      "Test accuracy: 0.9677000045776367\n"
     ]
    }
   ],
   "source": [
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_tf.fit(train_images, train_labels, epochs=3, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.conv1(x))\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Transfer weights for TensorFlow\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for PyTorch\n",
    "with torch.no_grad():\n",
    "    model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "    model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "TensorFlow Basic Model Output: [[[[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   ...\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]\n",
      "   [9.9660897e-01 2.1650463e-01 4.8949671e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]]\n",
      "PyTorch Basic Model Output: [[[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]\n",
      "\n",
      "\n",
      " [[[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]\n",
      "\n",
      "  [[9.9660897e-01 2.1650463e-01 4.8949677e-01 8.8328234e-04\n",
      "    7.6754862e-01 3.5114321e-05]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().permute(3,2,0,1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[1].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "TensorFlow Basic Model Output: [[[[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]\n",
      "\n",
      "  [[0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   ...\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]\n",
      "   [0.84262574 0.4728635  0.9888236  ... 0.60340494 0.36525252\n",
      "    0.15889616]]]]\n",
      "PyTorch Basic Model Output: [[[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]\n",
      "\n",
      "\n",
      " [[[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]\n",
      "\n",
      "  [[0.8426258  0.47286355 0.9888236  ... 0.6034049  0.3652526\n",
      "    0.1588962 ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().permute(3,2,0,1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AvgPool + Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)).permute(3,2,0,1) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # Flattening the tensor to the correct size\n",
    "        x = x.reshape(x.size(0),-1)  # Reshape to [batch_size, 16*4*4]\n",
    "\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[2].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "TensorFlow Basic Model Output: [[0.58721    0.2632927  0.5074984  0.5040425  0.41871366 0.52975184\n",
      "  0.524632   0.39369583 0.33130762 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.2632927\n",
      "  0.5074984  0.5040425  0.41871366 0.52975184 0.524632   0.39369583\n",
      "  0.33130762 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.2632927  0.5074984  0.5040425\n",
      "  0.41871366 0.52975184 0.524632   0.39369583 0.33130762 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.2632927  0.5074984  0.5040425  0.41871366 0.52975184\n",
      "  0.524632   0.39369583 0.33130762 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.2632927\n",
      "  0.5074984  0.5040425  0.41871366 0.52975184 0.524632   0.39369583\n",
      "  0.33130762 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.2632927  0.5074984  0.5040425\n",
      "  0.41871366 0.52975184 0.524632   0.39369583 0.33130762 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.2632927  0.5074984  0.5040425  0.41871366 0.52975184\n",
      "  0.524632   0.39369583 0.33130762 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.2632927\n",
      "  0.5074984  0.5040425  0.41871366 0.52975184 0.524632   0.39369583\n",
      "  0.33130762 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.2632927  0.5074984  0.5040425\n",
      "  0.41871366 0.52975184 0.524632   0.39369583 0.33130762 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.2632927  0.5074984  0.5040425  0.41871366 0.52975184\n",
      "  0.524632   0.39369583 0.33130762 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.2632927\n",
      "  0.5074984  0.5040425  0.41871366 0.52975184 0.524632   0.39369583\n",
      "  0.33130762 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.2632927  0.5074984  0.5040425\n",
      "  0.41871366 0.52975184 0.524632   0.39369583 0.33130762 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.2632927  0.5074984  0.5040425  0.41871366 0.52975184\n",
      "  0.524632   0.39369583 0.33130762 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.2632927\n",
      "  0.5074984  0.5040425  0.41871366 0.52975184 0.524632   0.39369583\n",
      "  0.33130762 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.2632927  0.5074984  0.5040425\n",
      "  0.41871366 0.52975184 0.524632   0.39369583 0.33130762 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.2632927  0.5074984  0.5040425  0.41871366 0.52975184\n",
      "  0.524632   0.39369583 0.33130762 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509 ]]\n",
      "PyTorch Basic Model Output: [[0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.26329267\n",
      "  0.5074984  0.5040425  0.41871366 0.5297518  0.524632   0.3936958\n",
      "  0.33130765 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.26329267 0.5074984  0.5040425\n",
      "  0.41871366 0.5297518  0.524632   0.3936958  0.33130765 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509 ]\n",
      " [0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.26329267\n",
      "  0.5074984  0.5040425  0.41871366 0.5297518  0.524632   0.3936958\n",
      "  0.33130765 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.26329267 0.5074984  0.5040425\n",
      "  0.41871366 0.5297518  0.524632   0.3936958  0.33130765 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509 ]\n",
      " [0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.26329267\n",
      "  0.5074984  0.5040425  0.41871366 0.5297518  0.524632   0.3936958\n",
      "  0.33130765 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.26329267 0.5074984  0.5040425\n",
      "  0.41871366 0.5297518  0.524632   0.3936958  0.33130765 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509 ]\n",
      " [0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509  0.58721    0.26329267\n",
      "  0.5074984  0.5040425  0.41871366 0.5297518  0.524632   0.3936958\n",
      "  0.33130765 0.30565107 0.53413635 0.41139778 0.6813877  0.51692677\n",
      "  0.41209933 0.5522509  0.58721    0.26329267 0.5074984  0.5040425\n",
      "  0.41871366 0.5297518  0.524632   0.3936958  0.33130765 0.30565107\n",
      "  0.53413635 0.41139778 0.6813877  0.51692677 0.41209933 0.5522509\n",
      "  0.58721    0.26329267 0.5074984  0.5040425  0.41871366 0.5297518\n",
      "  0.524632   0.3936958  0.33130765 0.30565107 0.53413635 0.41139778\n",
      "  0.6813877  0.51692677 0.41209933 0.5522509 ]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetPT, self).__init__()\n",
    "        # Convolutional encoder\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
    "\n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120) # 256 * 120\n",
    "        self.fc2 = nn.Linear(120, 84)         # 120 inputs, 84 outputs\n",
    "        self.fc3 = nn.Linear(84, 10)          # 84 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)).permute(0,3,1,2) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # Flattening the tensor to the correct size\n",
    "        x = x.reshape(x.size(0), -1)  # Reshape to [batch_size, 16*4*4]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, will use CrossEntropyLoss later\n",
    "        return x\n",
    "\n",
    "model_pt = LeNetPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))).reshape(-1, 120))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[6].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))).reshape(-1, 84))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the third dense layer (fc3) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[7].get_weights()\n",
    "model_pt.fc3.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc3.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x256 and 120x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_pt\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set PyTorch model to evaluation mode\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     output_pt \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrolled_input_pt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compare outputs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow Basic Model Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_tf)\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[109], line 22\u001b[0m, in \u001b[0;36mLeNetPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to [batch_size, 16*4*4]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Fully connected layers\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)  \u001b[38;5;66;03m# No activation function here, will use CrossEntropyLoss later\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x256 and 120x256)"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf = model_tf.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
