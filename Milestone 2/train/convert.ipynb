{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 21:40:08.791675: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-23 21:40:08.812082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-23 21:40:08.812103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-23 21:40:08.812680: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-23 21:40:08.816062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-23 21:40:09.203421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert to PyTorch format [batch_size, channels, height, width]\n",
    "train_images_pt = torch.tensor(train_images).permute(0, 3, 1, 2).float()\n",
    "test_images_pt = torch.tensor(test_images).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# If additional normalization is required for your PyTorch model, apply it here\n",
    "# For example, if you use transforms.Normalize((0.1307,), (0.3081,)) in PyTorch, apply similar normalization\n",
    "mean, std = 0.1307, 0.3081\n",
    "train_images_pt = (train_images_pt - mean) / std\n",
    "test_images_pt = (test_images_pt - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.8276 - accuracy: 0.7365 - val_loss: 0.2379 - val_accuracy: 0.9313\n",
      "Epoch 2/3\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2291 - accuracy: 0.9300 - val_loss: 0.1407 - val_accuracy: 0.9608\n",
      "Epoch 3/3\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1550 - accuracy: 0.9533 - val_loss: 0.1077 - val_accuracy: 0.9685\n",
      "313/313 - 0s - loss: 0.1234 - accuracy: 0.9614 - 209ms/epoch - 667us/step\n",
      "\n",
      "Test accuracy: 0.9613999724388123\n"
     ]
    }
   ],
   "source": [
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_tf.fit(train_images, train_labels, epochs=3, batch_size=32, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.1234 - accuracy: 0.9614 - 197ms/epoch - 629us/step\n",
      "\n",
      "Test accuracy: 0.9613999724388123\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class LeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetPT, self).__init__()\n",
    "        # Convolutional encoder\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
    "\n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(256,120) # 256 * 120\n",
    "        self.fc2 = nn.Linear(120, 84)         # 120 inputs, 84 outputs\n",
    "        self.fc3 = nn.Linear(84, 10)          # 84 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # TODO: figure out the resize, currently work on batch_size = 1\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(x.size(0),-1).reshape(-1,16)  # Reshape to [batch_size, 16*4*4]\n",
    "        x = np.transpose(x, (1,0))\n",
    "        x = x.reshape(batch_size,-1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, will use CrossEntropyLoss later\n",
    "        return x\n",
    "    \n",
    "\n",
    "model_pt = LeNetPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[6].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the third dense layer (fc3) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[7].get_weights()\n",
    "model_pt.fc3.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc3.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a controlled input (e.g., an array of ones)\n",
    "# controlled_input_tf = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "# controlled_input_pt = torch.from_numpy(controlled_input_tf).permute(0, 3, 1, 2)  # For PyTorch\n",
    "\n",
    "# Select the image for TensorFlow\n",
    "controlled_input_tf = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "TensorFlow Basic Model Output: [[-2.4110148  -2.7046077   2.7064452   1.2696762  -4.830148   -2.670598\n",
      "  -9.548576    6.460438   -2.2545085   0.18971601]]\n",
      "PyTorch Basic Model Output: [[-2.4110146  -2.7046077   2.7064452   1.269676   -4.8301497  -2.6705985\n",
      "  -9.548576    6.4604383  -2.254509    0.18971601]]\n"
     ]
    }
   ],
   "source": [
    "# Test PyTorch Basic Model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "\n",
    "output_tf = model_tf.predict(controlled_input_tf) \n",
    "print(\"TensorFlow Basic Model Output:\", output_tf)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 96.14%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming the TensorFlow MNIST data has already been loaded\n",
    "# Convert test_images to PyTorch tensor and permute\n",
    "test_images_pt = torch.tensor(test_images).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# Assuming test_labels are already loaded\n",
    "test_dataset = TensorDataset(test_images_pt, torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = evaluate_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096/ 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8192 / 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.conv1(x))\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Transfer weights for TensorFlow\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for PyTorch\n",
    "with torch.no_grad():\n",
    "    model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "    model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the image for TensorFlow\n",
    "controlled_input = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "TensorFlow Basic Model Output: [[[[0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   ...\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]]\n",
      "\n",
      "  [[0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   ...\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]]\n",
      "\n",
      "  [[0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   ...\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   ...\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]]\n",
      "\n",
      "  [[0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   ...\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]]\n",
      "\n",
      "  [[0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   ...\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]\n",
      "   [0.3405072  0.37062928 0.6257144  0.33070144 0.6428052  0.62785596]]]]\n",
      "PyTorch Basic Model Output: [[[[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]]\n",
      "\n",
      "\n",
      " [[[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]]\n",
      "\n",
      "\n",
      " [[[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]]\n",
      "\n",
      "\n",
      " [[[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]]\n",
      "\n",
      "\n",
      " [[[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]\n",
      "\n",
      "  [[0.34050718 0.37062928 0.6257144  0.33070144 0.6428052  0.627856  ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().permute(3,2,0,1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[1].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the image for TensorFlow\n",
    "controlled_input = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "TensorFlow Basic Model Output: [[[[2.57586502e-02 6.63225278e-02 3.52370068e-02 ... 2.83964783e-01\n",
      "    3.78392786e-01 1.71559360e-02]\n",
      "   [3.18357944e-02 9.97148827e-02 3.43728028e-02 ... 4.42336261e-01\n",
      "    4.03742552e-01 2.45979186e-02]\n",
      "   [4.90795746e-02 1.39458686e-01 3.03143449e-02 ... 5.38973391e-01\n",
      "    4.56746191e-01 3.97939608e-02]\n",
      "   ...\n",
      "   [4.77694720e-02 6.36636093e-02 3.90103497e-02 ... 3.89785856e-01\n",
      "    2.98766255e-01 4.16699834e-02]\n",
      "   [3.69360596e-02 6.63337857e-02 3.34584303e-02 ... 2.79973477e-01\n",
      "    3.29799414e-01 2.47441996e-02]\n",
      "   [3.33412029e-02 6.84268177e-02 3.02623659e-02 ... 2.24366948e-01\n",
      "    3.36898059e-01 1.78801026e-02]]\n",
      "\n",
      "  [[5.97592928e-02 1.66600794e-01 2.22794805e-02 ... 6.77564323e-01\n",
      "    2.82137245e-01 2.98533738e-02]\n",
      "   [2.17131585e-01 3.99079263e-01 1.07417926e-02 ... 8.41790795e-01\n",
      "    1.99286669e-01 6.10251874e-02]\n",
      "   [6.20349586e-01 6.13339841e-01 3.73514532e-03 ... 8.61602783e-01\n",
      "    9.75781530e-02 9.39734802e-02]\n",
      "   ...\n",
      "   [1.52095690e-01 8.14687684e-02 2.45576277e-02 ... 6.36375546e-01\n",
      "    1.23061843e-01 7.08035752e-02]\n",
      "   [6.11019693e-02 6.93565235e-02 3.26103307e-02 ... 4.33324426e-01\n",
      "    2.50361592e-01 4.17261422e-02]\n",
      "   [3.77909318e-02 6.92977309e-02 3.11484374e-02 ... 2.64988542e-01\n",
      "    3.28791142e-01 2.22917497e-02]]\n",
      "\n",
      "  [[4.54244375e-01 6.34751558e-01 6.93472940e-03 ... 8.90267313e-01\n",
      "    6.14309534e-02 5.34798540e-02]\n",
      "   [9.40072060e-01 9.21375930e-01 1.55194674e-03 ... 8.59239280e-01\n",
      "    8.29542615e-03 6.66806772e-02]\n",
      "   [9.95082974e-01 9.78631854e-01 2.49795878e-04 ... 6.99787557e-01\n",
      "    5.25148644e-04 3.16865630e-02]\n",
      "   ...\n",
      "   [5.31833708e-01 1.59883171e-01 4.85916715e-03 ... 6.84716821e-01\n",
      "    1.49780158e-02 2.01880261e-02]\n",
      "   [1.44799292e-01 9.11357999e-02 1.81292444e-02 ... 5.52146673e-01\n",
      "    1.06342860e-01 3.54126468e-02]\n",
      "   [5.24092503e-02 7.51833916e-02 2.80127507e-02 ... 3.23216081e-01\n",
      "    2.70679325e-01 2.54853796e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   [3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   [3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   ...\n",
      "   [6.24977704e-03 2.38206275e-02 1.07319355e-01 ... 8.59376848e-01\n",
      "    9.35030997e-01 4.85371172e-01]\n",
      "   [2.07196195e-02 4.03216407e-02 6.50104582e-02 ... 5.74824274e-01\n",
      "    6.84800029e-01 1.87931776e-01]\n",
      "   [3.15484479e-02 5.39612882e-02 4.36594374e-02 ... 2.75653362e-01\n",
      "    4.19311076e-01 5.43564409e-02]]\n",
      "\n",
      "  [[3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   [3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   [3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   ...\n",
      "   [2.31769219e-01 6.55437261e-02 4.88672964e-03 ... 2.39547536e-01\n",
      "    6.23090900e-02 4.83483728e-03]\n",
      "   [1.37919381e-01 7.05952495e-02 1.10017816e-02 ... 1.64588302e-01\n",
      "    8.84932950e-02 6.79437164e-03]\n",
      "   [7.02039152e-02 6.92757890e-02 1.94949787e-02 ... 1.49356633e-01\n",
      "    1.54885590e-01 9.42626130e-03]]\n",
      "\n",
      "  [[3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   [3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   [3.26236002e-02 6.87427521e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967482e-01 1.70121044e-02]\n",
      "   ...\n",
      "   [1.49817616e-01 7.92565718e-02 9.20786057e-03 ... 1.06855400e-01\n",
      "    7.06348866e-02 3.20416456e-03]\n",
      "   [8.63435417e-02 7.42657036e-02 1.64080802e-02 ... 1.25924468e-01\n",
      "    1.26676038e-01 6.00850303e-03]\n",
      "   [5.47261871e-02 7.06891865e-02 2.33658031e-02 ... 1.58357978e-01\n",
      "    2.03250334e-01 9.71523207e-03]]]]\n",
      "PyTorch Basic Model Output: [[[[2.57586502e-02 6.63224906e-02 3.52370068e-02 ... 2.83964783e-01\n",
      "    3.78392875e-01 1.71559360e-02]]\n",
      "\n",
      "  [[5.97592779e-02 1.66600764e-01 2.22794805e-02 ... 6.77564263e-01\n",
      "    2.82137245e-01 2.98533775e-02]]\n",
      "\n",
      "  [[4.54244316e-01 6.34751618e-01 6.93472940e-03 ... 8.90267313e-01\n",
      "    6.14309087e-02 5.34798391e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]]\n",
      "\n",
      "\n",
      " [[[3.18357795e-02 9.97148678e-02 3.43728028e-02 ... 4.42336202e-01\n",
      "    4.03742582e-01 2.45979261e-02]]\n",
      "\n",
      "  [[2.17131525e-01 3.99079233e-01 1.07417936e-02 ... 8.41790795e-01\n",
      "    1.99286610e-01 6.10251650e-02]]\n",
      "\n",
      "  [[9.40072060e-01 9.21375930e-01 1.55194756e-03 ... 8.59239280e-01\n",
      "    8.29542242e-03 6.66806549e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]]\n",
      "\n",
      "\n",
      " [[[4.90795560e-02 1.39458671e-01 3.03143486e-02 ... 5.38973391e-01\n",
      "    4.56746191e-01 3.97939570e-02]]\n",
      "\n",
      "  [[6.20349526e-01 6.13339841e-01 3.73514532e-03 ... 8.61602783e-01\n",
      "    9.75781158e-02 9.39734504e-02]]\n",
      "\n",
      "  [[9.95082974e-01 9.78631854e-01 2.49795616e-04 ... 6.99787498e-01\n",
      "    5.25148644e-04 3.16865630e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]\n",
      "\n",
      "  [[3.26235890e-02 6.87427223e-02 2.99267173e-02 ... 2.14654878e-01\n",
      "    3.28967601e-01 1.70121137e-02]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[4.77694571e-02 6.36635795e-02 3.90103497e-02 ... 3.89785856e-01\n",
      "    2.98766285e-01 4.16699983e-02]]\n",
      "\n",
      "  [[1.52095675e-01 8.14687535e-02 2.45576296e-02 ... 6.36375487e-01\n",
      "    1.23061873e-01 7.08035752e-02]]\n",
      "\n",
      "  [[5.31833708e-01 1.59883186e-01 4.85916482e-03 ... 6.84716761e-01\n",
      "    1.49780223e-02 2.01880205e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.24977658e-03 2.38206275e-02 1.07319355e-01 ... 8.59376967e-01\n",
      "    9.35030997e-01 4.85371083e-01]]\n",
      "\n",
      "  [[2.31769204e-01 6.55437112e-02 4.88673383e-03 ... 2.39547491e-01\n",
      "    6.23091199e-02 4.83484007e-03]]\n",
      "\n",
      "  [[1.49817556e-01 7.92565420e-02 9.20786895e-03 ... 1.06855378e-01\n",
      "    7.06349164e-02 3.20416573e-03]]]\n",
      "\n",
      "\n",
      " [[[3.69360521e-02 6.63337708e-02 3.34584452e-02 ... 2.79973477e-01\n",
      "    3.29799443e-01 2.47442108e-02]]\n",
      "\n",
      "  [[6.11019470e-02 6.93565086e-02 3.26103456e-02 ... 4.33324426e-01\n",
      "    2.50361621e-01 4.17261459e-02]]\n",
      "\n",
      "  [[1.44799292e-01 9.11357775e-02 1.81292575e-02 ... 5.52146614e-01\n",
      "    1.06342912e-01 3.54126357e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.07196139e-02 4.03216407e-02 6.50104210e-02 ... 5.74824274e-01\n",
      "    6.84800148e-01 1.87931776e-01]]\n",
      "\n",
      "  [[1.37919426e-01 7.05952719e-02 1.10017927e-02 ... 1.64588258e-01\n",
      "    8.84932950e-02 6.79437444e-03]]\n",
      "\n",
      "  [[8.63435268e-02 7.42657036e-02 1.64080877e-02 ... 1.25924423e-01\n",
      "    1.26676068e-01 6.00850908e-03]]]\n",
      "\n",
      "\n",
      " [[[3.33412029e-02 6.84268028e-02 3.02623659e-02 ... 2.24366948e-01\n",
      "    3.36898178e-01 1.78801119e-02]]\n",
      "\n",
      "  [[3.77909243e-02 6.92977309e-02 3.11484300e-02 ... 2.64988512e-01\n",
      "    3.28791171e-01 2.22917609e-02]]\n",
      "\n",
      "  [[5.24092391e-02 7.51833618e-02 2.80127656e-02 ... 3.23216081e-01\n",
      "    2.70679414e-01 2.54853796e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.15484479e-02 5.39612770e-02 4.36594374e-02 ... 2.75653362e-01\n",
      "    4.19311136e-01 5.43564595e-02]]\n",
      "\n",
      "  [[7.02039376e-02 6.92757964e-02 1.94949880e-02 ... 1.49356574e-01\n",
      "    1.54885575e-01 9.42626130e-03]]\n",
      "\n",
      "  [[5.47261871e-02 7.06892014e-02 2.33658254e-02 ... 1.58358023e-01\n",
      "    2.03250408e-01 9.71522741e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().permute(3,2,0,1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AvgPool + Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # Flattening the tensor to the correct size\n",
    "        #print ('x:', x.shape)\n",
    "        #x = x.permute(3,2,0,1)\n",
    "        x = x.reshape(x.size(0),-1).reshape(16,-1)  # Reshape to [batch_size, 16*4*4]\n",
    "        x = np.transpose(x, (1,0))\n",
    "        x = x.reshape(1,-1)\n",
    "        #x = x.reshape(1,-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[2].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt_basic\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the image for TensorFlow\n",
    "controlled_input = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "TensorFlow Basic Model Output: [[5.48884988e-01 7.87806988e-01 5.87700084e-02 8.25578198e-02\n",
      "  6.52495623e-01 8.31024814e-03 2.97849402e-02 4.15998220e-01\n",
      "  5.69971740e-01 2.39701167e-01 9.86830413e-01 8.89949560e-01\n",
      "  5.16457073e-02 7.28018768e-03 6.34472398e-03 8.25152849e-04\n",
      "  4.76424456e-01 7.10247993e-01 4.32754517e-01 1.06224250e-02\n",
      "  9.03416932e-01 3.78386199e-01 2.25163270e-02 4.04280424e-01\n",
      "  5.69598019e-01 1.15150232e-02 9.22504008e-01 5.83597064e-01\n",
      "  1.64265539e-02 5.89975063e-03 3.06279749e-01 1.18738087e-02\n",
      "  4.98429477e-01 9.35147524e-01 2.74252802e-01 6.47156872e-03\n",
      "  3.33572328e-01 1.86515957e-01 3.30121629e-02 5.46118207e-02\n",
      "  4.28845465e-01 3.18194866e-01 9.84396935e-01 3.65795642e-02\n",
      "  7.54113719e-02 2.42829397e-02 2.14797065e-01 8.91184807e-02\n",
      "  4.86817956e-01 4.67130125e-01 3.00586456e-03 5.47560573e-01\n",
      "  9.75016475e-01 7.92300142e-03 6.00912757e-02 2.13113964e-01\n",
      "  2.77048886e-01 7.67062962e-01 9.48068202e-01 3.12039435e-01\n",
      "  3.80830131e-02 4.64134440e-02 2.57697180e-02 5.71226701e-04\n",
      "  7.19489908e-05 3.78673114e-02 8.19154024e-01 4.75005805e-01\n",
      "  9.97187495e-01 9.73455012e-01 4.14828837e-01 6.95419967e-01\n",
      "  9.91173029e-01 3.99507850e-01 1.60720557e-01 1.10897616e-01\n",
      "  3.89569581e-01 4.90378886e-01 9.94811654e-01 5.87962806e-01\n",
      "  2.71353200e-02 2.55037963e-01 5.52123785e-01 4.65674520e-01\n",
      "  6.92131162e-01 6.17195487e-01 8.84752512e-01 3.75078380e-01\n",
      "  9.88146067e-01 6.42968953e-01 1.85868666e-01 2.34133331e-04\n",
      "  9.09982502e-01 9.35147703e-01 7.93067455e-01 8.46179664e-01\n",
      "  5.79472929e-02 7.13653088e-01 4.74765003e-01 1.12979755e-01\n",
      "  7.18087181e-02 4.40566033e-01 3.97967100e-01 1.70627356e-01\n",
      "  9.95990396e-01 6.97051764e-01 8.74447525e-01 2.31134206e-01\n",
      "  5.21187782e-01 3.27978432e-01 5.76652408e-01 6.87390327e-01\n",
      "  1.15040960e-02 1.56256244e-01 3.90548185e-02 8.94083738e-01\n",
      "  9.99859273e-01 9.81820375e-02 9.65187177e-02 5.48800588e-01\n",
      "  5.43496132e-01 4.29874957e-01 7.04627514e-01 2.30802357e-01\n",
      "  1.80156812e-01 3.56175005e-01 4.54063982e-01 3.96619216e-02\n",
      "  1.08389042e-01 1.47099689e-01 1.40654985e-02 8.49696040e-01\n",
      "  8.91473293e-01 5.50113656e-02 9.30270910e-01 3.34637985e-02\n",
      "  9.07703161e-01 8.73402238e-01 2.67883450e-01 7.00645614e-03\n",
      "  8.03314567e-01 5.60403466e-01 3.32528710e-01 3.73993032e-02\n",
      "  7.95747697e-01 8.45297813e-01 2.59025805e-02 1.86976701e-01\n",
      "  1.32311136e-01 7.83214718e-03 8.75104904e-01 1.53267644e-02\n",
      "  9.06637490e-01 8.80715370e-01 8.72922182e-01 1.24594886e-02\n",
      "  9.75773752e-01 6.02770209e-01 1.00866426e-02 1.35347366e-01\n",
      "  6.53181672e-01 9.02533233e-01 1.41055761e-02 2.64800817e-01\n",
      "  4.17332470e-01 6.05552085e-03 1.06460042e-01 3.93363357e-01\n",
      "  7.66313195e-01 7.24578619e-01 9.95476544e-01 5.23891449e-01\n",
      "  2.17432976e-01 2.00156383e-02 8.94752238e-03 6.32489175e-02\n",
      "  4.02258247e-01 3.98432016e-01 3.95900197e-03 7.54478991e-01\n",
      "  9.98980939e-01 3.10044326e-02 1.28839880e-01 2.41571784e-01\n",
      "  2.64322758e-01 8.09045613e-01 8.09647620e-01 6.74605221e-02\n",
      "  2.95346648e-01 4.46892262e-01 8.69710147e-02 3.04033067e-02\n",
      "  1.27547473e-01 2.61840940e-01 6.95203245e-02 4.67835248e-01\n",
      "  6.06955349e-01 7.93547034e-02 8.89167786e-01 6.82384670e-02\n",
      "  9.57020640e-01 5.20501554e-01 4.86942798e-01 2.01212876e-02\n",
      "  7.55840242e-01 3.20581555e-01 1.31838277e-01 3.80068533e-02\n",
      "  3.30185920e-01 9.02435303e-01 3.91326785e-01 4.35575005e-03\n",
      "  9.31262563e-04 1.22034386e-01 4.12657678e-01 2.13854611e-01\n",
      "  9.83030200e-01 1.47163406e-01 9.69156802e-01 4.73931968e-01\n",
      "  5.89451373e-01 1.28788456e-01 1.02490731e-01 2.15335816e-01\n",
      "  4.72608432e-02 5.34167171e-01 3.64812911e-01 1.35978714e-01\n",
      "  7.99253583e-01 4.05086040e-01 2.87377462e-03 8.36300910e-01\n",
      "  7.49523640e-01 3.12969238e-02 9.29385483e-01 7.62627423e-01\n",
      "  1.99268758e-03 1.93505157e-02 4.22303259e-01 4.66209538e-02\n",
      "  7.71952327e-03 1.80273950e-01 2.93069214e-01 4.22930300e-01\n",
      "  9.98557866e-01 4.68883991e-01 1.81370169e-01 2.56766111e-01\n",
      "  5.99458218e-01 2.70721555e-01 4.34507132e-01 6.06126338e-02\n",
      "  1.86283201e-01 3.01352382e-01 5.88446379e-01 2.28736952e-01]]\n",
      "PyTorch Basic Model Output: [[5.48884988e-01 7.87807047e-01 5.87700009e-02 8.25578272e-02\n",
      "  6.52495623e-01 8.31024442e-03 2.97849439e-02 4.15998220e-01\n",
      "  5.69971681e-01 2.39701167e-01 9.86830354e-01 8.89949560e-01\n",
      "  5.16456999e-02 7.28018628e-03 6.34473190e-03 8.25152383e-04\n",
      "  4.76424485e-01 7.10248053e-01 4.32754517e-01 1.06224287e-02\n",
      "  9.03416932e-01 3.78386229e-01 2.25163363e-02 4.04280424e-01\n",
      "  5.69598019e-01 1.15150241e-02 9.22504008e-01 5.83597064e-01\n",
      "  1.64265539e-02 5.89974690e-03 3.06279808e-01 1.18738115e-02\n",
      "  4.98429507e-01 9.35147524e-01 2.74252772e-01 6.47156732e-03\n",
      "  3.33572298e-01 1.86515942e-01 3.30121629e-02 5.46118207e-02\n",
      "  4.28845525e-01 3.18194866e-01 9.84396875e-01 3.65795754e-02\n",
      "  7.54113942e-02 2.42829267e-02 2.14797080e-01 8.91184807e-02\n",
      "  4.86817986e-01 4.67130095e-01 3.00586526e-03 5.47560513e-01\n",
      "  9.75016475e-01 7.92300701e-03 6.00912720e-02 2.13113979e-01\n",
      "  2.77048886e-01 7.67063022e-01 9.48068261e-01 3.12039435e-01\n",
      "  3.80829982e-02 4.64134514e-02 2.57697254e-02 5.71226352e-04\n",
      "  7.19489763e-05 3.78673077e-02 8.19153965e-01 4.75005865e-01\n",
      "  9.97187436e-01 9.73455012e-01 4.14828807e-01 6.95419967e-01\n",
      "  9.91173029e-01 3.99507880e-01 1.60720542e-01 1.10897630e-01\n",
      "  3.89569581e-01 4.90378857e-01 9.94811654e-01 5.87962866e-01\n",
      "  2.71353200e-02 2.55037963e-01 5.52123785e-01 4.65674520e-01\n",
      "  6.92131162e-01 6.17195427e-01 8.84752512e-01 3.75078380e-01\n",
      "  9.88146067e-01 6.42968953e-01 1.85868680e-01 2.34133215e-04\n",
      "  9.09982502e-01 9.35147703e-01 7.93067455e-01 8.46179605e-01\n",
      "  5.79472855e-02 7.13653207e-01 4.74765033e-01 1.12979740e-01\n",
      "  7.18087107e-02 4.40566033e-01 3.97967041e-01 1.70627370e-01\n",
      "  9.95990396e-01 6.97051764e-01 8.74447525e-01 2.31134221e-01\n",
      "  5.21187723e-01 3.27978462e-01 5.76652408e-01 6.87390387e-01\n",
      "  1.15040895e-02 1.56256229e-01 3.90548259e-02 8.94083738e-01\n",
      "  9.99859333e-01 9.81820747e-02 9.65187103e-02 5.48800528e-01\n",
      "  5.43496132e-01 4.29874986e-01 7.04627454e-01 2.30802327e-01\n",
      "  1.80156827e-01 3.56174976e-01 4.54063952e-01 3.96619104e-02\n",
      "  1.08389042e-01 1.47099659e-01 1.40655003e-02 8.49696159e-01\n",
      "  8.91473293e-01 5.50113693e-02 9.30270910e-01 3.34637985e-02\n",
      "  9.07703102e-01 8.73402238e-01 2.67883450e-01 7.00645847e-03\n",
      "  8.03314567e-01 5.60403466e-01 3.32528740e-01 3.73993032e-02\n",
      "  7.95747697e-01 8.45297813e-01 2.59025842e-02 1.86976701e-01\n",
      "  1.32311106e-01 7.83214998e-03 8.75104964e-01 1.53267672e-02\n",
      "  9.06637490e-01 8.80715489e-01 8.72922182e-01 1.24594858e-02\n",
      "  9.75773752e-01 6.02770269e-01 1.00866435e-02 1.35347366e-01\n",
      "  6.53181672e-01 9.02533293e-01 1.41055752e-02 2.64800847e-01\n",
      "  4.17332441e-01 6.05552318e-03 1.06460050e-01 3.93363386e-01\n",
      "  7.66313195e-01 7.24578619e-01 9.95476544e-01 5.23891509e-01\n",
      "  2.17432946e-01 2.00156458e-02 8.94752238e-03 6.32489249e-02\n",
      "  4.02258158e-01 3.98432016e-01 3.95900058e-03 7.54479051e-01\n",
      "  9.98980999e-01 3.10044382e-02 1.28839895e-01 2.41571784e-01\n",
      "  2.64322788e-01 8.09045553e-01 8.09647679e-01 6.74604923e-02\n",
      "  2.95346648e-01 4.46892262e-01 8.69710594e-02 3.04033272e-02\n",
      "  1.27547398e-01 2.61840940e-01 6.95203096e-02 4.67835277e-01\n",
      "  6.06955290e-01 7.93546736e-02 8.89167726e-01 6.82384595e-02\n",
      "  9.57020700e-01 5.20501554e-01 4.86942768e-01 2.01212838e-02\n",
      "  7.55840242e-01 3.20581555e-01 1.31838322e-01 3.80068570e-02\n",
      "  3.30185950e-01 9.02435303e-01 3.91326785e-01 4.35574958e-03\n",
      "  9.31262504e-04 1.22034386e-01 4.12657738e-01 2.13854641e-01\n",
      "  9.83030081e-01 1.47163421e-01 9.69156802e-01 4.73931968e-01\n",
      "  5.89451373e-01 1.28788412e-01 1.02490738e-01 2.15335786e-01\n",
      "  4.72608469e-02 5.34167171e-01 3.64812911e-01 1.35978699e-01\n",
      "  7.99253583e-01 4.05086011e-01 2.87377182e-03 8.36300910e-01\n",
      "  7.49523640e-01 3.12969238e-02 9.29385483e-01 7.62627363e-01\n",
      "  1.99268758e-03 1.93505175e-02 4.22303259e-01 4.66209538e-02\n",
      "  7.71952374e-03 1.80273965e-01 2.93069243e-01 4.22930300e-01\n",
      "  9.98557866e-01 4.68883991e-01 1.81370169e-01 2.56766081e-01\n",
      "  5.99458158e-01 2.70721555e-01 4.34507161e-01 6.06126450e-02\n",
      "  1.86283156e-01 3.01352382e-01 5.88446438e-01 2.28736967e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(256, 120) # 120 * 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        x = x.reshape(x.size(0),-1).reshape(16,-1)  # Reshape to [batch_size, 16*4*4]\n",
    "        x = np.transpose(x, (1,0))\n",
    "        x = x.reshape(1,-1)\n",
    "        \n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[ 0.0060004   0.09459859 -0.00982087 ...  0.4026808  -0.18616\n",
      "  -0.0760118 ]\n",
      " [ 0.1680993   0.00600922  0.19005074 ...  0.16594718 -0.06219631\n",
      "   0.05193466]\n",
      " [ 0.05982314 -0.2828379  -0.10398509 ... -0.2969587   0.3439928\n",
      "   0.23956047]\n",
      " ...\n",
      " [ 0.14957191  0.01632433  0.04348282 ...  0.2801885  -0.09770978\n",
      "   0.07889169]\n",
      " [-0.04528046 -0.11519922 -0.13521452 ...  0.35203415 -0.15684576\n",
      "   0.07581601]\n",
      " [ 0.16932055  0.06789297 -0.24664897 ...  0.16121444  0.31199127\n",
      "  -0.06370709]]\n",
      "w_T: torch.Size([120, 256])\n"
     ]
    }
   ],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[2].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_tf_basic.layers[5].set_weights([weights, biases])\n",
    "\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "print ('w:',weights)\n",
    "model_pt_basic.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1,0))))\n",
    "print ('w_T:',model_pt_basic.fc1.weight.shape)\n",
    "model_pt_basic.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the image for TensorFlow\n",
    "controlled_input = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "TensorFlow Basic Model Output: [[2.5085372e-01 6.4726716e-01 4.8868883e-02 3.3194184e-01 2.9337928e-02\n",
      "  1.0934564e-03 3.5817081e-01 9.5409507e-01 1.1309944e-01 1.6625345e-02\n",
      "  3.0560943e-01 4.1540496e-02 9.8110253e-01 6.2015608e-02 2.9901081e-01\n",
      "  8.9301074e-01 5.8131945e-01 9.0093009e-02 9.7839260e-01 2.0575029e-01\n",
      "  1.0464395e-02 3.7924361e-01 9.9337846e-01 4.2498612e-01 2.8738566e-02\n",
      "  1.1527718e-01 3.5135803e-01 1.6455298e-02 6.4079426e-02 2.0648109e-02\n",
      "  3.4988154e-02 8.5316998e-01 1.3921203e-01 9.6092708e-03 9.7262537e-01\n",
      "  9.8974645e-01 1.3170831e-01 9.6172369e-01 9.9841106e-01 1.6231669e-01\n",
      "  3.6521304e-02 9.2579347e-01 7.3157918e-01 4.5203529e-02 7.4042553e-01\n",
      "  3.7248302e-01 4.2575136e-01 9.8524725e-01 3.8875315e-02 5.3248071e-04\n",
      "  8.7046909e-01 5.2938116e-01 5.6007969e-01 9.7692323e-01 8.4818944e-02\n",
      "  8.3015889e-01 1.5019216e-01 1.9550809e-01 9.8865926e-01 9.3609399e-01\n",
      "  7.3582703e-01 9.3234396e-01 9.8643106e-01 7.7345460e-03 4.4231966e-02\n",
      "  2.4032652e-01 9.0061176e-01 9.9088585e-01 2.4575658e-01 1.6972609e-03\n",
      "  8.7064165e-01 6.5478694e-01 1.1128928e-01 3.5190981e-02 7.8663212e-01\n",
      "  6.2012686e-03 1.1762388e-03 6.2428486e-01 6.1594397e-01 4.3503544e-01\n",
      "  7.1596390e-01 6.0518067e-02 7.7912557e-01 2.6116052e-01 9.8988962e-01\n",
      "  3.5708487e-01 4.0595476e-03 2.9234794e-01 3.6797982e-01 7.7990949e-01\n",
      "  4.8800498e-02 9.3451029e-01 7.0763677e-01 1.3736373e-01 1.2853058e-01\n",
      "  1.8426554e-01 3.7720475e-02 2.9520907e-03 7.6201648e-01 1.5465035e-01\n",
      "  3.1117842e-01 9.4181240e-01 9.8138475e-01 8.7585515e-01 8.6071408e-01\n",
      "  9.1489011e-01 8.1951946e-01 3.5561404e-01 1.0937078e-01 8.8389367e-01\n",
      "  2.5706577e-01 7.6249188e-01 7.5306378e-02 6.7370081e-01 9.9519289e-01\n",
      "  9.7701812e-01 2.0854431e-01 9.6281773e-01 4.9019152e-01 7.0712095e-01]]\n",
      "PyTorch Basic Model Output: [[2.50853747e-01 6.47267342e-01 4.88689058e-02 3.31941992e-01\n",
      "  2.93379240e-02 1.09346001e-03 3.58170837e-01 9.54094946e-01\n",
      "  1.13099381e-01 1.66253503e-02 3.05609465e-01 4.15404961e-02\n",
      "  9.81102526e-01 6.20156080e-02 2.99010873e-01 8.93010736e-01\n",
      "  5.81319451e-01 9.00929198e-02 9.78392601e-01 2.05750272e-01\n",
      "  1.04643796e-02 3.79243702e-01 9.93378460e-01 4.24986154e-01\n",
      "  2.87385471e-02 1.15277030e-01 3.51357996e-01 1.64552983e-02\n",
      "  6.40794337e-02 2.06481386e-02 3.49881798e-02 8.53169918e-01\n",
      "  1.39212012e-01 9.60927084e-03 9.72625375e-01 9.89746332e-01\n",
      "  1.31708384e-01 9.61723685e-01 9.98411059e-01 1.62316740e-01\n",
      "  3.65212820e-02 9.25793529e-01 7.31579125e-01 4.52035181e-02\n",
      "  7.40425527e-01 3.72483045e-01 4.25751448e-01 9.85247254e-01\n",
      "  3.88752930e-02 5.32482751e-04 8.70469213e-01 5.29381156e-01\n",
      "  5.60079634e-01 9.76923227e-01 8.48189220e-02 8.30158830e-01\n",
      "  1.50192201e-01 1.95508003e-01 9.88659263e-01 9.36093926e-01\n",
      "  7.35827208e-01 9.32343960e-01 9.86431062e-01 7.73454597e-03\n",
      "  4.42319699e-02 2.40326613e-01 9.00611877e-01 9.90885854e-01\n",
      "  2.45756462e-01 1.69726333e-03 8.70641768e-01 6.54786944e-01\n",
      "  1.11289158e-01 3.51910144e-02 7.86632121e-01 6.20125700e-03\n",
      "  1.17623818e-03 6.24284923e-01 6.15943849e-01 4.35035378e-01\n",
      "  7.15964019e-01 6.05180971e-02 7.79125571e-01 2.61160582e-01\n",
      "  9.89889622e-01 3.57084841e-01 4.05954756e-03 2.92347878e-01\n",
      "  3.67979705e-01 7.79909313e-01 4.88004982e-02 9.34510291e-01\n",
      "  7.07636595e-01 1.37363777e-01 1.28530651e-01 1.84265539e-01\n",
      "  3.77205350e-02 2.95208930e-03 7.62016356e-01 1.54650345e-01\n",
      "  3.11178386e-01 9.41812396e-01 9.81384635e-01 8.75855148e-01\n",
      "  8.60713959e-01 9.14890110e-01 8.19519460e-01 3.55613977e-01\n",
      "  1.09370805e-01 8.83893669e-01 2.57065624e-01 7.62491882e-01\n",
      "  7.53064007e-02 6.73700750e-01 9.95192885e-01 9.77018237e-01\n",
      "  2.08544344e-01 9.62817729e-01 4.90191460e-01 7.07121074e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define a basic LeNet model in PyTorch with two convolutional layers\n",
    "class BasicLeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicLeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(120, 256) # 256 * 120\n",
    "        self.fc2 = nn.Linear(120,84)         # 120 inputs, 84 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)).permute(3,2,0,1) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # Flattening the tensor to the correct size\n",
    "        x = x.reshape(x.size(0),-1).reshape(1,-1)  # Reshape to [batch_size, 16*4*4]\n",
    "        \n",
    "        # Print the shape of x before and after each layer\n",
    "        print(\"Before fc1:\", x.shape)\n",
    "        \n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        \n",
    "        print(\"After fc1:\", x.shape)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        print(\"After fc2:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "model_pt_basic = BasicLeNetPT()\n",
    "\n",
    "# Create a basic LeNet model in TensorFlow with two convolutional layers\n",
    "model_tf_basic = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_tf_basic.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_tf_basic.layers[2].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_tf_basic.layers[5].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from the original model_tf\n",
    "weights, biases = model_tf.layers[6].get_weights()\n",
    "model_tf_basic.layers[6].set_weights([weights, biases])\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt_basic.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt_basic.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt_basic.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_pt_basic.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1,0))))\n",
    "model_pt_basic.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[6].get_weights()\n",
    "model_pt_basic.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1,0))))\n",
    "model_pt_basic.fc2.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "Before fc1: torch.Size([1, 256])\n",
      "After fc1: torch.Size([1, 120])\n",
      "After fc2: torch.Size([1, 84])\n",
      "TensorFlow Basic Model Output: [[0.7160134  0.8385422  0.0253077  0.42892176 0.6184138  0.9969837\n",
      "  0.09171487 0.01182156 0.04299246 0.4810178  0.9516474  0.09384901\n",
      "  0.8192958  0.55231595 0.10896716 0.00647116 0.04556889 0.8126056\n",
      "  0.82992846 0.5289272  0.894237   0.08265778 0.9708183  0.24809329\n",
      "  0.79544413 0.00390048 0.02803985 0.7674697  0.04256167 0.06272971\n",
      "  0.3221807  0.43452185 0.6216037  0.04312156 0.97908485 0.72737664\n",
      "  0.9310374  0.14843705 0.9657128  0.7040428  0.14954226 0.7755475\n",
      "  0.2979573  0.5029429  0.85015994 0.19273664 0.04996477 0.02114206\n",
      "  0.9176138  0.9626188  0.39863586 0.9899365  0.97770965 0.0713995\n",
      "  0.02932361 0.17433517 0.99833655 0.8545451  0.11654612 0.21318336\n",
      "  0.83583117 0.02372023 0.0214206  0.978412   0.852728   0.12194854\n",
      "  0.23547943 0.55145293 0.06977712 0.49735156 0.98175037 0.28126732\n",
      "  0.9684286  0.8804305  0.87427825 0.25589195 0.05280343 0.29675412\n",
      "  0.08128658 0.0786276  0.9940673  0.6950344  0.8771883  0.98410296]]\n",
      "PyTorch Basic Model Output: [[0.7160134  0.83854216 0.02530768 0.4289217  0.6184138  0.9969837\n",
      "  0.09171487 0.01182156 0.04299246 0.48101768 0.9516474  0.09384903\n",
      "  0.8192958  0.552316   0.10896713 0.00647116 0.04556889 0.8126055\n",
      "  0.82992846 0.52892715 0.894237   0.08265776 0.9708183  0.24809335\n",
      "  0.795444   0.00390048 0.02803986 0.7674695  0.04256171 0.06272968\n",
      "  0.32218087 0.43452194 0.6216038  0.04312156 0.97908485 0.72737664\n",
      "  0.93103755 0.14843704 0.9657128  0.7040428  0.14954224 0.77554744\n",
      "  0.29795715 0.502943   0.8501598  0.19273663 0.04996475 0.02114206\n",
      "  0.9176138  0.9626187  0.39863595 0.9899365  0.97770965 0.0713995\n",
      "  0.02932359 0.17433527 0.99833655 0.85454524 0.11654617 0.21318336\n",
      "  0.8358312  0.02372022 0.02142061 0.978412   0.852728   0.12194851\n",
      "  0.23547933 0.55145293 0.06977714 0.4973515  0.98175037 0.28126743\n",
      "  0.9684285  0.8804305  0.87427825 0.25589207 0.05280342 0.2967542\n",
      "  0.08128664 0.07862758 0.9940673  0.69503444 0.8771883  0.98410296]]\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow Basic Model\n",
    "output_tf_basic = model_tf_basic.predict(controlled_input)\n",
    "\n",
    "# Test PyTorch Basic Model\n",
    "model_pt_basic.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt_basic = model_pt_basic(controlled_input_pt)\n",
    "\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Basic Model Output:\", output_tf_basic)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt_basic.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
