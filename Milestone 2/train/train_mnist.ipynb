{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetPT, self).__init__()\n",
    "        # Convolutional encoder\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
    "\n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120) # 256 * 120\n",
    "        self.fc2 = nn.Linear(120, 84)         # 120 inputs, 84 outputs\n",
    "        self.fc3 = nn.Linear(84, 10)          # 84 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # Flattening\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, will use CrossEntropyLoss later\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_(nn.Module):\n",
    "    def __init__(self, features=(6, 16, 120, 84)):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        features = list(features)\n",
    "        if len(features) == 3:\n",
    "            features.append(None)\n",
    "\n",
    "        f1, f2, f3, f4 = features\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, f1, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(f1, f2, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(f2 * 4 * 4, f3)\n",
    "        if f4 is not None:\n",
    "            self.fc2 = nn.Linear(f3, f4)\n",
    "            self.fc3 = nn.Linear(f4, 10)\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(f3, 10)\n",
    "            self.fc3 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note that tanh activation does a better job at keeping activations in a predictable\n",
    "        # range. This means that fewer bits are needed for quantization!\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = torch.nn.functional.avg_pool2d(x, 2)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = torch.nn.functional.avg_pool2d(x, 2)\n",
    "        x = x.flatten(1)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        if self.fc3 is not None:\n",
    "            x = self.fc3(torch.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def load_dataset(batch_size=256):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, transform=ToTensor(), download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def train(model, train_dataset, learning_rate=0.001, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # Wrap the train_dataset with tqdm for a progress bar\n",
    "        for images, labels in tqdm(train_dataset, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {total_loss / len(train_dataset):.4f}')\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataset):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for images, labels in test_dataset:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mnist_images(dataset, indices = []):\n",
    "\n",
    "    if not indices:\n",
    "        fig, axes = plt.subplots(1, len(dataset), figsize=(12, 4))\n",
    "        \n",
    "        for i in range(len(axes)):\n",
    "            image, label = dataset[i]\n",
    "            ax = axes[i]\n",
    "            \n",
    "            image = image.squeeze()  # Remove channel dimension\n",
    "\n",
    "            ax.imshow(image, cmap='gray')\n",
    "            ax.set_title(f'Label: {label}')\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(indices), figsize=(12, 4))\n",
    "    if len(indices) == 1:  # If only one index is provided, wrap axes in a list\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, idx in zip(axes, indices):\n",
    "        image, label = dataset[idx]\n",
    "        image = image.squeeze()  # Remove channel dimension\n",
    "\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f'Label: {label}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_loader, test_loader, train_dataset, test_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 235/235 [00:01<00:00, 148.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 2.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 235/235 [00:03<00:00, 64.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Average Loss: 0.6618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 235/235 [00:04<00:00, 48.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Average Loss: 0.3644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 235/235 [00:04<00:00, 48.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Average Loss: 0.2794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 235/235 [00:04<00:00, 48.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Average Loss: 0.2283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 235/235 [00:01<00:00, 138.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Average Loss: 0.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 235/235 [00:01<00:00, 158.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Average Loss: 0.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 235/235 [00:01<00:00, 166.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Average Loss: 0.1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 235/235 [00:01<00:00, 167.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Average Loss: 0.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 235/235 [00:01<00:00, 157.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Average Loss: 0.1157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model (in a separate cell)\n",
    "model_pt = LeNetPT().to(device)\n",
    "train(model_pt, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.85%\n"
     ]
    }
   ],
   "source": [
    "model_pt = model_pt.to(device)\n",
    "evaluate(model_pt, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pt.state_dict(), \"../model/LeNet_pt_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mnist_images(test_dataset, [0,1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert 2 Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_weights = []\n",
    "for layer in model_pt.children():\n",
    "    if isinstance(layer, torch.nn.Conv2d) or isinstance(layer, torch.nn.Linear):\n",
    "        weights = layer.weight.data.cpu().numpy()\n",
    "        biases = layer.bias.data.cpu().numpy()\n",
    "        pytorch_weights.append((weights, biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([120, 256])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([10, 84])\n"
     ]
    }
   ],
   "source": [
    "for layer in model_pt.children():\n",
    "    if isinstance(layer, torch.nn.Conv2d) or isinstance(layer, torch.nn.Linear):\n",
    "        print(layer.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "pt_layers = [layer for layer in model_pt.children() if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear))]\n",
    "tf_layers = [layer for layer in model_tf.layers if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense))]\n",
    "\n",
    "def transfer_weights(model_pt, model_tf):\n",
    "    model_pt.eval()\n",
    "    model_tf.trainable = False\n",
    "\n",
    "    for i, (pt_layer, tf_layer) in enumerate(zip(pt_layers, tf_layers)):\n",
    "        pt_weights = pt_layer.state_dict()\n",
    "        pt_weights = [w.cpu().numpy() for w in pt_weights.values()]\n",
    "\n",
    "        if isinstance(tf_layer, tf.keras.layers.Conv2D) and isinstance(pt_layer, torch.nn.Conv2d):\n",
    "            transposed_weights = np.transpose(pt_weights[0], [2, 3, 1, 0])\n",
    "            assert transposed_weights.shape == tf_layer.weights[0].shape, \"Shape mismatch in Conv2D layer\"\n",
    "            tf_layer.set_weights([transposed_weights, pt_weights[1]])\n",
    "\n",
    "        elif isinstance(tf_layer, tf.keras.layers.Dense) and isinstance(pt_layer, torch.nn.Linear):\n",
    "            assert pt_weights[0].T.shape == tf_layer.weights[0].shape, \"Shape mismatch in Dense layer\"\n",
    "            tf_layer.set_weights([pt_weights[0].T, pt_weights[1]])  # Transpose weights for dense layers\n",
    "\n",
    "        else:\n",
    "            print(f\"Layer mismatch or configuration issue in layer {i}: {tf_layer.name}\")\n",
    "\n",
    "\n",
    "transfer_weights(model_pt, model_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_weights(pt_layer, tf_layer):\n",
    "    pt_weights = [w.cpu().detach().numpy() for w in pt_layer.state_dict().values()]\n",
    "    tf_weights = tf_layer.get_weights()\n",
    "\n",
    "    if isinstance(pt_layer, torch.nn.Conv2d):\n",
    "        pt_weights[0] = np.transpose(pt_weights[0], [2, 3, 1, 0])  # Transpose for Conv2D\n",
    "    elif isinstance(pt_layer, torch.nn.Linear):\n",
    "        pt_weights[0] = pt_weights[0].T  # Transpose for Linear\n",
    "\n",
    "    for pt_w, tf_w in zip(pt_weights, tf_weights):\n",
    "        if not np.allclose(pt_w, tf_w, atol=1e-6):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "for pt_layer, tf_layer in zip(pt_layers, tf_layers):\n",
    "    if not compare_weights(pt_layer, tf_layer):\n",
    "        print(f\"Weight mismatch detected in layer: {type(pt_layer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 5.7077 - accuracy: 0.0790 - 280ms/epoch - 894us/step\n",
      "\n",
      "Test accuracy: 0.07900000363588333\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cpu),\n",
      "      %conv1.weight : Float(6, 1, 5, 5, strides=[25, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(6, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(16, 6, 5, 5, strides=[150, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(16, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc1.weight : Float(120, 256, strides=[256, 1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(120, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.weight : Float(84, 120, strides=[120, 1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(84, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc3.weight : Float(10, 84, strides=[84, 1], requires_grad=1, device=cpu),\n",
      "      %fc3.bias : Float(10, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/conv1/Conv_output_0 : Float(1, 6, 24, 24, strides=[3456, 576, 24, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv1/Conv\"](%input, %conv1.weight, %conv1.bias), scope: __main__.LeNetPT::/torch.nn.modules.conv.Conv2d::conv1 # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/Sigmoid_output_0 : Float(1, 6, 24, 24, strides=[3456, 576, 24, 1], requires_grad=1, device=cpu) = onnx::Sigmoid[onnx_name=\"/Sigmoid\"](%/conv1/Conv_output_0), scope: __main__.LeNetPT:: # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/functional.py:1981:0\n",
      "  %/AveragePool_output_0 : Float(1, 6, 12, 12, strides=[864, 144, 12, 1], requires_grad=1, device=cpu) = onnx::AveragePool[ceil_mode=0, count_include_pad=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/AveragePool\"](%/Sigmoid_output_0), scope: __main__.LeNetPT:: # /tmp/ipykernel_35482/855546075.py:15:0\n",
      "  %/conv2/Conv_output_0 : Float(1, 16, 8, 8, strides=[1024, 64, 8, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv2/Conv\"](%/AveragePool_output_0, %conv2.weight, %conv2.bias), scope: __main__.LeNetPT::/torch.nn.modules.conv.Conv2d::conv2 # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/Sigmoid_1_output_0 : Float(1, 16, 8, 8, strides=[1024, 64, 8, 1], requires_grad=1, device=cpu) = onnx::Sigmoid[onnx_name=\"/Sigmoid_1\"](%/conv2/Conv_output_0), scope: __main__.LeNetPT:: # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/functional.py:1981:0\n",
      "  %/AveragePool_1_output_0 : Float(1, 16, 4, 4, strides=[256, 16, 4, 1], requires_grad=1, device=cpu) = onnx::AveragePool[ceil_mode=0, count_include_pad=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/AveragePool_1\"](%/Sigmoid_1_output_0), scope: __main__.LeNetPT:: # /tmp/ipykernel_35482/855546075.py:16:0\n",
      "  %/Constant_output_0 : Long(2, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 1 -1 [ CPULongType{2} ], onnx_name=\"/Constant\"](), scope: __main__.LeNetPT:: # /tmp/ipykernel_35482/855546075.py:19:0\n",
      "  %/Reshape_output_0 : Float(1, 256, strides=[256, 1], requires_grad=1, device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"/Reshape\"](%/AveragePool_1_output_0, %/Constant_output_0), scope: __main__.LeNetPT:: # /tmp/ipykernel_35482/855546075.py:19:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 120, strides=[120, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Reshape_output_0, %fc1.weight, %fc1.bias), scope: __main__.LeNetPT::/torch.nn.modules.linear.Linear::fc1 # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Sigmoid_2_output_0 : Float(1, 120, strides=[120, 1], requires_grad=1, device=cpu) = onnx::Sigmoid[onnx_name=\"/Sigmoid_2\"](%/fc1/Gemm_output_0), scope: __main__.LeNetPT:: # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/functional.py:1981:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 84, strides=[84, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Sigmoid_2_output_0, %fc2.weight, %fc2.bias), scope: __main__.LeNetPT::/torch.nn.modules.linear.Linear::fc2 # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Sigmoid_3_output_0 : Float(1, 84, strides=[84, 1], requires_grad=1, device=cpu) = onnx::Sigmoid[onnx_name=\"/Sigmoid_3\"](%/fc2/Gemm_output_0), scope: __main__.LeNetPT:: # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/functional.py:1981:0\n",
      "  %output : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Sigmoid_3_output_0, %fc3.weight, %fc3.bias), scope: __main__.LeNetPT::/torch.nn.modules.linear.Linear::fc3 # /home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "\n",
    "# Define some example input data (adjust the shape to match your input)\n",
    "input_data = torch.randn(1, 1, 28, 28) # Example input for LeNet\n",
    "\n",
    "# Export the PyTorch model to ONNX\n",
    "onnx_filename = \"../model/lenet_tf.onnx\"\n",
    "torch.onnx.export(model_pt.cpu(), input_data, onnx_filename, verbose=True, input_names=['input'], output_names=['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x with unsupported characters which will be renamed to transpose_12_x in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/converted_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/converted_model/assets\n",
      "INFO:absl:Writing fingerprint to ../model/converted_model/fingerprint.pb\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# Convert\n",
    "tf_rep = prepare(onnx_model)\n",
    "\n",
    "# Export the model to TensorFlow format\n",
    "tf_rep.export_graph('../model/converted_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.8261 - accuracy: 0.7352 - val_loss: 0.2401 - val_accuracy: 0.9295\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2187 - accuracy: 0.9342 - val_loss: 0.1607 - val_accuracy: 0.9542\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1424 - accuracy: 0.9558 - val_loss: 0.0980 - val_accuracy: 0.9720\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.1054 - accuracy: 0.9680 - val_loss: 0.0836 - val_accuracy: 0.9745\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0856 - accuracy: 0.9738 - val_loss: 0.0685 - val_accuracy: 0.9798\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0725 - accuracy: 0.9778 - val_loss: 0.0659 - val_accuracy: 0.9785\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0635 - accuracy: 0.9808 - val_loss: 0.0592 - val_accuracy: 0.9823\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0560 - accuracy: 0.9827 - val_loss: 0.0515 - val_accuracy: 0.9840\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0514 - accuracy: 0.9841 - val_loss: 0.0501 - val_accuracy: 0.9837\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0476 - accuracy: 0.9852 - val_loss: 0.0528 - val_accuracy: 0.9845\n",
      "313/313 - 0s - loss: 0.0513 - accuracy: 0.9832 - 255ms/epoch - 815us/step\n",
      "\n",
      "Test accuracy: 0.9832000136375427\n"
     ]
    }
   ],
   "source": [
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='sigmoid'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_tf.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TF model to PT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNetPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.avg_pool2d(torch.sigmoid(self.conv1(x)), 2)\n",
    "        x = F.avg_pool2d(torch.sigmoid(self.conv2(x)), 2)\n",
    "        x = x.reshape(x.size(0), -1)  # Changed from .view() to .reshape()\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt = LeNetPT()\n",
    "\n",
    "# Assuming `model_tf` is your trained TensorFlow model\n",
    "for pt_layer, tf_layer in zip(model_pt.children(), model_tf.layers):\n",
    "    if isinstance(pt_layer, nn.Conv2d) and isinstance(tf_layer, tf.keras.layers.Conv2D):\n",
    "        # Transpose the weights\n",
    "        weights, biases = tf_layer.get_weights()\n",
    "        pt_layer.weight.data = torch.from_numpy(np.transpose(weights, (3, 2, 0, 1)))\n",
    "        pt_layer.bias.data = torch.from_numpy(biases)\n",
    "    elif isinstance(pt_layer, nn.Linear) and isinstance(tf_layer, tf.keras.layers.Dense):\n",
    "        # Transpose the weights\n",
    "        weights, biases = tf_layer.get_weights()\n",
    "        pt_layer.weight.data = torch.from_numpy(weights.T)\n",
    "        pt_layer.bias.data = torch.from_numpy(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0513 - accuracy: 0.9832 - 292ms/epoch - 932us/step\n",
      "\n",
      "Test accuracy: 0.9832000136375427\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 8.92%\n"
     ]
    }
   ],
   "source": [
    "model_pt = model_pt.to(device)\n",
    "evaluate(model_pt, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model First Layer Weights:\n",
      "tensor([[[[-1.2311, -1.2511, -0.9231, -1.0632, -1.0987],\n",
      "          [-0.9168, -0.1972,  0.0386, -0.1594, -0.2573],\n",
      "          [-0.2432,  0.9284,  1.1481,  1.0099,  0.5899],\n",
      "          [ 0.5047,  1.6127,  1.7706,  1.3948,  0.8842],\n",
      "          [ 0.1234,  0.5074,  0.1300,  0.1284,  0.5038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1955, -0.3577, -0.9627, -0.7697, -0.0284],\n",
      "          [-0.1098, -0.8189, -2.1102, -0.7877,  0.3679],\n",
      "          [-0.0658, -1.8842, -1.4489, -0.1118,  0.3906],\n",
      "          [-0.3881, -1.8611, -0.7881, -0.0216,  0.6062],\n",
      "          [-0.5104, -1.2369, -0.2763,  0.6014,  1.1855]]],\n",
      "\n",
      "\n",
      "        [[[-0.5255,  0.0592, -0.0022,  0.3927,  0.2522],\n",
      "          [-0.0854,  1.0483,  1.4577,  1.2724,  1.1061],\n",
      "          [-0.2171,  0.7175,  1.7216,  1.4573,  0.7924],\n",
      "          [-0.5596,  0.0196,  0.1829, -0.0439, -0.5260],\n",
      "          [-0.9951, -1.0732, -1.1594, -1.1006, -1.1487]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8516,  0.5936,  0.3834,  0.5049,  0.2654],\n",
      "          [ 0.3114, -0.3511, -0.6621, -0.8568, -0.1697],\n",
      "          [ 0.1557, -0.6226, -1.8102, -1.2052, -0.2530],\n",
      "          [ 0.6202, -0.4552, -1.6607, -0.9330, -0.1995],\n",
      "          [ 0.9205, -0.1404, -0.6488,  0.0963,  0.4410]]],\n",
      "\n",
      "\n",
      "        [[[-0.7168, -0.9061, -0.8224, -1.2954, -1.2953],\n",
      "          [ 0.1710,  0.6449,  0.5020, -0.2897, -1.1697],\n",
      "          [ 0.9280,  1.4298,  1.3960,  0.0947, -0.7973],\n",
      "          [ 1.0839,  1.1949,  1.0271,  0.1881, -0.7331],\n",
      "          [ 0.5554,  0.8371,  0.6139, -0.2986, -0.6963]]],\n",
      "\n",
      "\n",
      "        [[[-1.0435, -0.7378, -0.5084, -0.1382, -0.3042],\n",
      "          [-0.6334,  0.6293,  1.0727,  0.9622,  0.7054],\n",
      "          [-0.1163,  0.8081,  1.9489,  1.6007,  0.7548],\n",
      "          [-0.6986,  0.4176,  0.8563,  0.7178,  0.0418],\n",
      "          [-1.0670, -0.6302, -0.7373, -0.6709, -1.0933]]]], device='cuda:0')\n",
      "\n",
      "TensorFlow Model First Layer Weights:\n",
      "[[[[-1.2310671  -1.2510966  -0.92312074 -1.0631896  -1.0986696 ]\n",
      "   [-0.9167744  -0.19719559  0.03855542 -0.1593727  -0.25730637]\n",
      "   [-0.24321523  0.9284239   1.1481216   1.0098598   0.58990604]\n",
      "   [ 0.50467604  1.6127068   1.7705784   1.3947912   0.8841664 ]\n",
      "   [ 0.12339102  0.50742847  0.13003759  0.12836581  0.50377434]]]\n",
      "\n",
      "\n",
      " [[[ 0.19548929 -0.35771593 -0.9627015  -0.7697073  -0.02838537]\n",
      "   [-0.10982331 -0.818904   -2.1101794  -0.78768855  0.36789683]\n",
      "   [-0.06584715 -1.8841753  -1.4488567  -0.11183313  0.39058754]\n",
      "   [-0.38808298 -1.8611041  -0.7880514  -0.02164413  0.6062    ]\n",
      "   [-0.5103807  -1.2368548  -0.27625746  0.6014013   1.1855236 ]]]\n",
      "\n",
      "\n",
      " [[[-0.52546877  0.05923315 -0.00216755  0.39274076  0.2522068 ]\n",
      "   [-0.0853617   1.0482635   1.4576595   1.272421    1.1061441 ]\n",
      "   [-0.21711865  0.7174625   1.7215687   1.4573057   0.7924267 ]\n",
      "   [-0.55961525  0.01964999  0.18288952 -0.04388454 -0.5260184 ]\n",
      "   [-0.995075   -1.0732385  -1.1594349  -1.1006143  -1.1486646 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.8516182   0.5936288   0.38343674  0.5049402   0.26544774]\n",
      "   [ 0.31140697 -0.35109952 -0.6621285  -0.8568377  -0.16974108]\n",
      "   [ 0.15574974 -0.6225613  -1.8102019  -1.2051783  -0.2530103 ]\n",
      "   [ 0.62019193 -0.4551747  -1.6607196  -0.93299097 -0.19947498]\n",
      "   [ 0.9205329  -0.14044805 -0.64882624  0.09630594  0.4409979 ]]]\n",
      "\n",
      "\n",
      " [[[-0.71680266 -0.9060826  -0.82238674 -1.2954488  -1.295343  ]\n",
      "   [ 0.17098571  0.64488006  0.5020327  -0.2897488  -1.1696959 ]\n",
      "   [ 0.9280065   1.4298427   1.3960444   0.09474082 -0.79732406]\n",
      "   [ 1.0839149   1.1948719   1.0271223   0.18809754 -0.73314923]\n",
      "   [ 0.5553614   0.8370707   0.6138632  -0.2985973  -0.69628614]]]\n",
      "\n",
      "\n",
      " [[[-1.0434844  -0.73775613 -0.50839156 -0.13821155 -0.30420315]\n",
      "   [-0.63339657  0.62927747  1.0726742   0.96218956  0.7054173 ]\n",
      "   [-0.11628907  0.80811554  1.9488521   1.6006633   0.75477076]\n",
      "   [-0.69857794  0.41757378  0.85630167  0.71780545  0.04175086]\n",
      "   [-1.0670286  -0.6302072  -0.7372861  -0.6709107  -1.0932962 ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Print weights for the first layer as an example\n",
    "print(\"PyTorch Model First Layer Weights:\")\n",
    "print(model_pt.conv1.weight.data)\n",
    "\n",
    "print(\"\\nTensorFlow Model First Layer Weights:\")\n",
    "tf_weights, tf_biases = model_tf.layers[0].get_weights()\n",
    "print(np.transpose(tf_weights, (3, 2, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "TensorFlow Output: [[ 3.9230046 -7.9354315  2.3756258 -4.4657454 -1.0101205 -8.120874\n",
      "  -3.2213542 -8.070989   3.961212   1.0927204]]\n",
      "PyTorch Output: [[ 0.02759585 -0.12497215  0.14346291 -0.10190661 -0.38895643  0.2020148\n",
      "  -0.07957983 -0.43531477  0.1875354  -0.71130717]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create a controlled input (e.g., an array of ones)\n",
    "controlled_input = np.ones((1, 28, 28, 1), dtype=np.float32)  # For TensorFlow\n",
    "controlled_input_pt = torch.from_numpy(controlled_input).permute(0, 3, 1, 2)  # For PyTorch\n",
    "\n",
    "# Get output from TensorFlow model\n",
    "output_tf = model_tf.predict(controlled_input)\n",
    "\n",
    "# Get output from PyTorch model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt.to(device))\n",
    "\n",
    "# Compare outputs\n",
    "print(\"TensorFlow Output:\", output_tf)\n",
    "print(\"PyTorch Output:\", output_pt.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Intermediate Output TensorFlow: [[[[9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   ...\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]]\n",
      "\n",
      "  [[9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   ...\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]]\n",
      "\n",
      "  [[9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   ...\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   ...\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]]\n",
      "\n",
      "  [[9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   ...\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]]\n",
      "\n",
      "  [[9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   ...\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]\n",
      "   [9.5932311e-01 2.0740599e-05 9.0009844e-01 1.5471795e-02\n",
      "    7.2575617e-01 7.8761309e-01]]]]\n",
      "Intermediate Output PyTorch: [[[[  3.1605673    3.1605673    3.1605673  ...   3.1605673\n",
      "      3.1605673    3.1605673 ]\n",
      "   [  3.1605673    3.1605673    3.1605673  ...   3.1605673\n",
      "      3.1605673    3.1605673 ]\n",
      "   [  3.1605673    3.1605673    3.1605673  ...   3.1605673\n",
      "      3.1605673    3.1605673 ]\n",
      "   ...\n",
      "   [  3.1605673    3.1605673    3.1605673  ...   3.1605673\n",
      "      3.1605673    3.1605673 ]\n",
      "   [  3.1605673    3.1605673    3.1605673  ...   3.1605673\n",
      "      3.1605673    3.1605673 ]\n",
      "   [  3.1605673    3.1605673    3.1605673  ...   3.1605673\n",
      "      3.1605673    3.1605673 ]]\n",
      "\n",
      "  [[-10.783397   -10.783397   -10.783397   ... -10.783397\n",
      "    -10.783397   -10.783397  ]\n",
      "   [-10.783397   -10.783397   -10.783397   ... -10.783397\n",
      "    -10.783397   -10.783397  ]\n",
      "   [-10.783397   -10.783397   -10.783397   ... -10.783397\n",
      "    -10.783397   -10.783397  ]\n",
      "   ...\n",
      "   [-10.783397   -10.783397   -10.783397   ... -10.783397\n",
      "    -10.783397   -10.783397  ]\n",
      "   [-10.783397   -10.783397   -10.783397   ... -10.783397\n",
      "    -10.783397   -10.783397  ]\n",
      "   [-10.783397   -10.783397   -10.783397   ... -10.783397\n",
      "    -10.783397   -10.783397  ]]\n",
      "\n",
      "  [[  2.1983192    2.1983192    2.1983192  ...   2.1983192\n",
      "      2.1983192    2.1983192 ]\n",
      "   [  2.1983192    2.1983192    2.1983192  ...   2.1983192\n",
      "      2.1983192    2.1983192 ]\n",
      "   [  2.1983192    2.1983192    2.1983192  ...   2.1983192\n",
      "      2.1983192    2.1983192 ]\n",
      "   ...\n",
      "   [  2.1983192    2.1983192    2.1983192  ...   2.1983192\n",
      "      2.1983192    2.1983192 ]\n",
      "   [  2.1983192    2.1983192    2.1983192  ...   2.1983192\n",
      "      2.1983192    2.1983192 ]\n",
      "   [  2.1983192    2.1983192    2.1983192  ...   2.1983192\n",
      "      2.1983192    2.1983192 ]]\n",
      "\n",
      "  [[ -4.153144    -4.153144    -4.153144   ...  -4.153144\n",
      "     -4.153144    -4.153144  ]\n",
      "   [ -4.153144    -4.153144    -4.153144   ...  -4.153144\n",
      "     -4.153144    -4.153144  ]\n",
      "   [ -4.153144    -4.153144    -4.153144   ...  -4.153144\n",
      "     -4.153144    -4.153144  ]\n",
      "   ...\n",
      "   [ -4.153144    -4.153144    -4.153144   ...  -4.153144\n",
      "     -4.153144    -4.153144  ]\n",
      "   [ -4.153144    -4.153144    -4.153144   ...  -4.153144\n",
      "     -4.153144    -4.153144  ]\n",
      "   [ -4.153144    -4.153144    -4.153144   ...  -4.153144\n",
      "     -4.153144    -4.153144  ]]\n",
      "\n",
      "  [[  0.97319674   0.97319674   0.97319674 ...   0.97319674\n",
      "      0.97319674   0.97319674]\n",
      "   [  0.97319674   0.97319674   0.97319674 ...   0.97319674\n",
      "      0.97319674   0.97319674]\n",
      "   [  0.97319674   0.97319674   0.97319674 ...   0.97319674\n",
      "      0.97319674   0.97319674]\n",
      "   ...\n",
      "   [  0.97319674   0.97319674   0.97319674 ...   0.97319674\n",
      "      0.97319674   0.97319674]\n",
      "   [  0.97319674   0.97319674   0.97319674 ...   0.97319674\n",
      "      0.97319674   0.97319674]\n",
      "   [  0.97319674   0.97319674   0.97319674 ...   0.97319674\n",
      "      0.97319674   0.97319674]]\n",
      "\n",
      "  [[  1.3105974    1.3105974    1.3105974  ...   1.3105974\n",
      "      1.3105974    1.3105974 ]\n",
      "   [  1.3105974    1.3105974    1.3105974  ...   1.3105974\n",
      "      1.3105974    1.3105974 ]\n",
      "   [  1.3105974    1.3105974    1.3105974  ...   1.3105974\n",
      "      1.3105974    1.3105974 ]\n",
      "   ...\n",
      "   [  1.3105974    1.3105974    1.3105974  ...   1.3105974\n",
      "      1.3105974    1.3105974 ]\n",
      "   [  1.3105974    1.3105974    1.3105974  ...   1.3105974\n",
      "      1.3105974    1.3105974 ]\n",
      "   [  1.3105974    1.3105974    1.3105974  ...   1.3105974\n",
      "      1.3105974    1.3105974 ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the output after the first convolutional layer as an example\n",
    "\n",
    "# TensorFlow\n",
    "intermediate_model_tf = tf.keras.Model(inputs=model_tf.inputs,\n",
    "                                       outputs=model_tf.layers[0].output)\n",
    "intermediate_output_tf = intermediate_model_tf.predict(controlled_input)\n",
    "\n",
    "# PyTorch\n",
    "with torch.no_grad():\n",
    "    intermediate_output_pt = model_pt.cpu().conv1(controlled_input_pt)\n",
    "\n",
    "# Print and compare intermediate outputs\n",
    "print(\"Intermediate Output TensorFlow:\", intermediate_output_tf)\n",
    "print(\"Intermediate Output PyTorch:\", intermediate_output_pt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow - Mean: 0.5647139 Std: 0.40090796\n",
      "PyTorch - Mean: -1.2156437635421753 Std: 4.868114471435547\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics for TensorFlow output\n",
    "mean_tf = np.mean(intermediate_output_tf)\n",
    "std_tf = np.std(intermediate_output_tf)\n",
    "\n",
    "# Compute statistics for PyTorch output\n",
    "mean_pt = intermediate_output_pt.mean().item()\n",
    "std_pt = intermediate_output_pt.std().item()\n",
    "\n",
    "# Print and compare statistics\n",
    "print(\"TensorFlow - Mean:\", mean_tf, \"Std:\", std_tf)\n",
    "print(\"PyTorch - Mean:\", mean_pt, \"Std:\", std_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow - Conv Layer Config:\n",
      "Filters: 6\n",
      "Kernel Size: (5, 5)\n",
      "Strides: (1, 1)\n",
      "Padding: valid\n",
      "Activation Function: sigmoid\n",
      "\n",
      "PyTorch - Conv Layer Config:\n",
      "Out Channels (Filters): 6\n",
      "Kernel Size: (5, 5)\n",
      "Stride: (1, 1)\n",
      "Padding: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Model Configuration\n",
    "tf_layer = model_tf.layers[0]\n",
    "print(\"TensorFlow - Conv Layer Config:\")\n",
    "print(\"Filters:\", tf_layer.filters)\n",
    "print(\"Kernel Size:\", tf_layer.kernel_size)\n",
    "print(\"Strides:\", tf_layer.strides)\n",
    "print(\"Padding:\", tf_layer.padding)\n",
    "print(\"Activation Function:\", tf_layer.activation.__name__)\n",
    "\n",
    "# PyTorch Model Configuration\n",
    "pt_layer = model_pt.conv1\n",
    "print(\"\\nPyTorch - Conv Layer Config:\")\n",
    "print(\"Out Channels (Filters):\", pt_layer.out_channels)\n",
    "print(\"Kernel Size:\", pt_layer.kernel_size)\n",
    "print(\"Stride:\", pt_layer.stride)\n",
    "print(\"Padding:\", pt_layer.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert 2 TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class LeNetTF(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LeNetTF, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(6, (5, 5), activation='relu', padding='valid', use_bias=True)\n",
    "        self.pool1 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(16, (5, 5), activation='relu', padding='valid', use_bias=True)\n",
    "        self.pool2 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = tf.keras.layers.Dense(120, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(84, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(10)  # No activation here, softmax will be applied in loss calculation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.pool1(self.conv1(inputs))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "model_tf = LeNetTF()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tf(model, test_loader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        # Convert PyTorch images to TensorFlow format (NCHW to NHWC) and labels to numpy\n",
    "        images_np = images.numpy().transpose(0, 2, 3, 1)\n",
    "        labels_np = labels.numpy()\n",
    "\n",
    "        # Get outputs from TensorFlow model\n",
    "        outputs = model.predict(images_np)\n",
    "\n",
    "        # Convert outputs to class predictions\n",
    "        predicted = np.argmax(outputs, axis=1)\n",
    "\n",
    "        # Update total and correct counts\n",
    "        total += labels_np.shape[0]\n",
    "        correct += np.sum(predicted == labels_np)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "def convert_loader_to_tf_dataset(loader):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x, y = batch\n",
    "        # Transpose the images from PyTorch format to TensorFlow format\n",
    "        x = x.numpy().transpose(0, 2, 3, 1)  # Change (B, C, H, W) to (B, H, W, C)\n",
    "        images.append(x)\n",
    "        labels.append(y.numpy())\n",
    "\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "# Convert PyTorch loaders to TensorFlow datasets\n",
    "#train_loader, test_loader, _, _ = load_dataset(batch_size=256)\n",
    "train_dataset_tf = convert_loader_to_tf_dataset(train_loader)\n",
    "test_dataset_tf = convert_loader_to_tf_dataset(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "train_images = train_images[..., tf.newaxis]\n",
    "test_images = test_images[..., tf.newaxis]\n",
    "\n",
    "# Prepare the training and testing datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(10000).batch(256)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 19:38:06.396220: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.498927: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.578211: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.732288: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.732303: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.732312: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.732678: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.732689: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.733161: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.734954: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.735450: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:06.736416: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-23 19:38:07.051724: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7cf46b6870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-23 19:38:07.051743: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-01-23 19:38:07.055770: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706009887.097740   35965 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 2s 2ms/step - loss: 2.4991 - accuracy: 0.2675\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.2491\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.2574\n",
      "Test accuracy: 25.74%\n"
     ]
    }
   ],
   "source": [
    "# Compile the TensorFlow model\n",
    "model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_tf.fit(train_dataset, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model_tf.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt = LeNetPT()\n",
    "model_pt.load_state_dict(torch.load('../model/LeNet_pt_model.pth'))\n",
    "\n",
    "# Make sure both models are in evaluation mode\n",
    "model_pt.eval()\n",
    "model_tf.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer mismatch or configuration issue in layer 0: conv2d_8\n",
      "Layer mismatch or configuration issue in layer 1: conv2d_9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transfer_weights(model_pt, model_tf):\n",
    "    model_pt.eval()\n",
    "    model_tf.trainable = False\n",
    "\n",
    "    pt_layers = [layer for layer in model_pt.children() if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear))]\n",
    "    tf_layers = [layer for layer in model_tf.layers if isinstance(layer, (layers.Conv2D, layers.Dense))]\n",
    "\n",
    "    for i, (pt_layer, tf_layer) in enumerate(zip(pt_layers, tf_layers)):\n",
    "        pt_weights = pt_layer.state_dict()\n",
    "        pt_weights = [w.cpu().numpy() for w in pt_weights.values()]\n",
    "\n",
    "        # ... [debugging prints]\n",
    "\n",
    "        if isinstance(tf_layer, layers.Conv2D) and isinstance(pt_layer, torch.nn.Conv2d):\n",
    "            transposed_weights = np.transpose(pt_weights[0], [2, 3, 1, 0])\n",
    "            \n",
    "            if pt_layer.bias is not None and len(tf_layer.weights) == 2:\n",
    "                # Transfer both weights and biases\n",
    "                tf_layer.set_weights([transposed_weights, pt_weights[1]])\n",
    "            elif pt_layer.bias is None and len(tf_layer.weights) == 1:\n",
    "                # Transfer only weights\n",
    "                tf_layer.set_weights([transposed_weights])\n",
    "            else:\n",
    "                print(f\"Layer mismatch or configuration issue in layer {i}: {tf_layer.name}\")\n",
    "\n",
    "\n",
    "\n",
    "transfer_weights(model_pt, model_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model:\n",
      "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "Linear(in_features=256, out_features=120, bias=True)\n",
      "Linear(in_features=120, out_features=84, bias=True)\n",
      "Linear(in_features=84, out_features=10, bias=True)\n",
      "\n",
      "TensorFlow Model:\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f7ebffe1810>\n",
      "<keras.src.layers.pooling.average_pooling2d.AveragePooling2D object at 0x7f7ebff102e0>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f7dac2283d0>\n",
      "<keras.src.layers.pooling.average_pooling2d.AveragePooling2D object at 0x7f7ebffd8400>\n",
      "<keras.src.layers.reshaping.flatten.Flatten object at 0x7f7dac22ad40>\n",
      "<keras.src.layers.core.dense.Dense object at 0x7f7dac2336a0>\n",
      "<keras.src.layers.core.dense.Dense object at 0x7f7dac232f50>\n",
      "<keras.src.layers.core.dense.Dense object at 0x7f7dac232a10>\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Model:\")\n",
    "for pt_layer in model_pt.children():\n",
    "    print(pt_layer)\n",
    "\n",
    "print(\"\\nTensorFlow Model:\")\n",
    "for tf_layer in model_tf.layers:\n",
    "    print(tf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.85%\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_pt, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tf_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Normalize and reshape the data\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "    x_test = np.expand_dims(x_test, -1)  # Reshape for the model (NHWC format)\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "x_test_tf, y_test_tf = load_tf_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/313 [..............................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 514us/step\n",
      "Test Accuracy: 0.10%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tf(model, x_test, y_test):\n",
    "    # Predict and calculate accuracy\n",
    "    predictions = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_classes == y_test)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "evaluate_tf(model_tf, x_test_tf, y_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_outputs(test_loader, model_pt, model_tf):\n",
    "    model_pt.eval()\n",
    "    model_tf.trainable = False\n",
    "    discrepancies = 0\n",
    "    total_images = 0\n",
    "\n",
    "    for images, _ in test_loader:\n",
    "        # Get outputs from PyTorch model\n",
    "        with torch.no_grad():\n",
    "            pt_outputs = model_pt(images).cpu().numpy()\n",
    "\n",
    "        # Convert PyTorch images to TensorFlow format (NCHW to NHWC)\n",
    "        images_np = images.numpy().transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Get outputs from TensorFlow model\n",
    "        tf_outputs = model_tf.predict(images_np)\n",
    "\n",
    "        # Compare predictions\n",
    "        pt_predictions = np.argmax(pt_outputs, axis=1)\n",
    "        tf_predictions = np.argmax(tf_outputs, axis=1)\n",
    "\n",
    "        discrepancies += np.sum(pt_predictions != tf_predictions)\n",
    "        total_images += len(images)\n",
    "\n",
    "    print(f\"Total images compared: {total_images}\")\n",
    "    print(f\"Total discrepancies: {discrepancies}\")\n",
    "\n",
    "    if discrepancies == 0:\n",
    "        print(\"All predictions match.\")\n",
    "    else:\n",
    "        print(\"There are discrepancies in the predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 856us/step\n",
      "8/8 [==============================] - 0s 733us/step\n",
      "8/8 [==============================] - 0s 602us/step\n",
      "8/8 [==============================] - 0s 864us/step\n",
      "8/8 [==============================] - 0s 852us/step\n",
      "8/8 [==============================] - 0s 836us/step\n",
      "8/8 [==============================] - 0s 830us/step\n",
      "8/8 [==============================] - 0s 691us/step\n",
      "8/8 [==============================] - 0s 661us/step\n",
      "8/8 [==============================] - 0s 862us/step\n",
      "8/8 [==============================] - 0s 832us/step\n",
      "8/8 [==============================] - 0s 595us/step\n",
      "8/8 [==============================] - 0s 685us/step\n",
      "8/8 [==============================] - 0s 630us/step\n",
      "8/8 [==============================] - 0s 719us/step\n",
      "8/8 [==============================] - 0s 638us/step\n",
      "8/8 [==============================] - 0s 563us/step\n",
      "8/8 [==============================] - 0s 511us/step\n",
      "8/8 [==============================] - 0s 574us/step\n",
      "8/8 [==============================] - 0s 603us/step\n",
      "8/8 [==============================] - 0s 554us/step\n",
      "8/8 [==============================] - 0s 527us/step\n",
      "8/8 [==============================] - 0s 507us/step\n",
      "8/8 [==============================] - 0s 544us/step\n",
      "8/8 [==============================] - 0s 498us/step\n",
      "8/8 [==============================] - 0s 521us/step\n",
      "8/8 [==============================] - 0s 589us/step\n",
      "8/8 [==============================] - 0s 621us/step\n",
      "8/8 [==============================] - 0s 612us/step\n",
      "8/8 [==============================] - 0s 618us/step\n",
      "8/8 [==============================] - 0s 527us/step\n",
      "8/8 [==============================] - 0s 669us/step\n",
      "8/8 [==============================] - 0s 593us/step\n",
      "8/8 [==============================] - 0s 547us/step\n",
      "8/8 [==============================] - 0s 552us/step\n",
      "8/8 [==============================] - 0s 658us/step\n",
      "8/8 [==============================] - 0s 622us/step\n",
      "8/8 [==============================] - 0s 623us/step\n",
      "8/8 [==============================] - 0s 623us/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Total images compared: 10000\n",
      "Total discrepancies: 9011\n",
      "There are discrepancies in the predictions.\n"
     ]
    }
   ],
   "source": [
    "_, test_loader, _, _ = load_dataset(batch_size=256)\n",
    "compare_model_outputs(test_loader, model_pt, model_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from ONNX model\n",
    "weights_dict = {init.name: onnx.numpy_helper.to_array(init) for init in onnx_model.graph.initializer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the TensorFlow representation\n",
    "tf_rep = prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`input.1` is not a valid tf.function parameter name. Sanitizing to `input_1`.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/handlers/handler.py\", line 59, in handle  *\n        return ver_handle(node, **kwargs)\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/handlers/backend/conv.py\", line 15, in version_11  *\n        return cls.conv(node, kwargs[\"tensor_dict\"])\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/handlers/backend/conv_mixin.py\", line 29, in conv  *\n        x = input_dict[node.inputs[0]]\n\n    KeyError: 'input.1'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Export model to SavedModel format\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtf_rep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../model/lenet_tf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/backend_rep.py:143\u001b[0m, in \u001b[0;36mTensorflowRep.export_graph\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Export backend representation to a Tensorflow proto file.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03mThis function obtains the graph proto corresponding to the ONNX\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m:returns: none.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_module\u001b[38;5;241m.\u001b[39mis_export \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    140\u001b[0m tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_module,\n\u001b[1;32m    142\u001b[0m     path,\n\u001b[0;32m--> 143\u001b[0m     signatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignatures\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_module\u001b[38;5;241m.\u001b[39mis_export \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1227\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1197\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1201\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:695\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    691\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    692\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    693\u001b[0m )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    700\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:598\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 598\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1695\u001b[0m, in \u001b[0;36mclass_method_to_instance_method.<locals>.bound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1690\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_fn(weak_instance(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;66;03m# If __wrapped__ was replaced, then it is always an unbound function.\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;66;03m# However, the replacer is still responsible for attaching self properly.\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Is it possible to do it here instead?\u001b[39;00m\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file74kvt0oa.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m curr_node_output_map \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurr_node_output_map\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m onnx_node \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monnx_node\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mdict\u001b[39m), (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_4\u001b[39m():\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:449\u001b[0m, in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iter_, distribute\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# TODO(b/162250181): Use _tf_iterator_for_stmt(iter(iter_)...\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     for_fn \u001b[38;5;241m=\u001b[39m _tf_distributed_iterable_for_stmt\n\u001b[0;32m--> 449\u001b[0m \u001b[43mfor_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:500\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m iter_:\n\u001b[0;32m--> 500\u001b[0m     \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:466\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprotected_body\u001b[39m(protected_iter):\n\u001b[0;32m--> 466\u001b[0m   \u001b[43moriginal_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotected_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m   after_iteration()\n\u001b[1;32m    468\u001b[0m   before_iteration()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file74kvt0oa.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     21\u001b[0m node \u001b[38;5;241m=\u001b[39m itr\n\u001b[1;32m     22\u001b[0m onnx_node \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(OnnxNode), (ag__\u001b[38;5;241m.\u001b[39mld(node),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 23\u001b[0m output_ops \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_onnx_node_to_tensorflow_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_node\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandlers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m curr_node_output_map \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mdict\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(onnx_node)\u001b[38;5;241m.\u001b[39moutputs, ag__\u001b[38;5;241m.\u001b[39mld(output_ops)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     25\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tensor_dict)\u001b[38;5;241m.\u001b[39mupdate, (ag__\u001b[38;5;241m.\u001b[39mld(curr_node_output_map),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5cxpipl3.py:62\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op\u001b[0;34m(cls, node, tensor_dict, handlers, opset, strict)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     61\u001b[0m handler \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhandler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandlers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1217\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1215\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1217\u001b[0m   \u001b[43m_py_if_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morelse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1270\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cond \u001b[38;5;28;01melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5cxpipl3.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m do_return, retval_\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1217\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1215\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1217\u001b[0m   \u001b[43m_py_if_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morelse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1270\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cond \u001b[38;5;28;01melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5cxpipl3.py:48\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_nalvyu0.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle\u001b[0;34m(cls, node, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m do_return, retval_\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(BackendIsNotSupposedToImplementIt), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m version \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not implemented.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat, (ag__\u001b[38;5;241m.\u001b[39mld(node)\u001b[38;5;241m.\u001b[39mop_type, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mSINCE_VERSION), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mver_handle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1217\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1215\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1217\u001b[0m   \u001b[43m_py_if_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morelse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1270\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cond \u001b[38;5;28;01melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_nalvyu0.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mver_handle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file9oc65cx2.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__version\u001b[0;34m(cls, node, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensor_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:441\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    439\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m   _attach_error_metadata(e, converted_f)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6dh2b6js.py:19\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__conv\u001b[0;34m(cls, node, input_dict, transpose)\u001b[0m\n\u001b[1;32m     17\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     18\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m x_rank \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(x)\u001b[38;5;241m.\u001b[39mget_shape, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     21\u001b[0m x_shape \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf_shape), (ag__\u001b[38;5;241m.\u001b[39mld(x), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mKeyError\u001b[0m: in user code:\n\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/handlers/handler.py\", line 59, in handle  *\n        return ver_handle(node, **kwargs)\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/handlers/backend/conv.py\", line 15, in version_11  *\n        return cls.conv(node, kwargs[\"tensor_dict\"])\n    File \"/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/onnx_tf/handlers/backend/conv_mixin.py\", line 29, in conv  *\n        x = input_dict[node.inputs[0]]\n\n    KeyError: 'input.1'\n"
     ]
    }
   ],
   "source": [
    "# Export model to SavedModel format\n",
    "tf_rep.export_graph(\"../model/lenet_tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl, os, json\n",
    "\n",
    "run_args = ezkl.PyRunArgs()\n",
    "run_args.input_visibility = \"public\"\n",
    "run_args.param_visibility = \"fixed\"\n",
    "run_args.output_visibility = \"public\"\n",
    "# run_args.num_inner_cols = 2\n",
    "run_args.variables = [(\"batch_size\", 0)]\n",
    "\n",
    "# Capture set of data points\n",
    "num_data_points = 8\n",
    "\n",
    "# Fetch data points from the train_dataset\n",
    "data_points = []\n",
    "for i, (data_point, _) in enumerate(test_loader):\n",
    "    if i >= num_data_points:\n",
    "        break\n",
    "    data_points.append(data_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(data_points).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_witness_LN(folder, model, data_point):\n",
    "    model_path = os.path.join(folder, 'network.onnx')\n",
    "    compiled_model_path = os.path.join(folder, 'network.compiled')\n",
    "    settings_path = os.path.join(folder, 'settings.json') \n",
    "    witness_path = os.path.join(folder, 'witness.json')\n",
    "    data_path = os.path.join(folder, 'input.json')\n",
    "\n",
    "    # cal_path = os.path.join(folder, \"cal_data.json\")\n",
    "    # srs_path = os.path.join(folder, 'kzg.srs')\n",
    "\n",
    "    model.eval()\n",
    "    # Verify the device (CPU or CUDA) and transfer the data point to the same device as the model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    data_point = data_point.to(device).unsqueeze(0)  # Add a batch dimension\n",
    "    # _, pred = torch.max(model(data_point),1)\n",
    "    # train_data_point = train_data_point.to(device)\n",
    "    # # Export the model to ONNX format\n",
    "    torch.onnx.export(model, \n",
    "                      data_point, \n",
    "                      model_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=10, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['input_0'], \n",
    "                      output_names=['output'])\n",
    "\n",
    "    # Convert the tensor to numpy array and reshape it for JSON serialization\n",
    "    x = (data_point.cpu().detach().numpy().reshape([-1])).tolist()\n",
    "    data = dict(input_data = [x])\n",
    "\n",
    "    # Serialize data into file:\n",
    "    json.dump(data, open(data_path, 'w'))\n",
    "\n",
    "    # data_points = []\n",
    "    # for i, (data_point, _) in enumerate(dataset):\n",
    "    #     if i >= num_data_points:\n",
    "    #         break\n",
    "    #     data_points.append(data_point)\n",
    "        \n",
    "    # # plot_mnist_images(data_points)\n",
    "\n",
    "    # # Stack the data points to create a batch\n",
    "    # train_data_batch = torch.stack(data_points)\n",
    "\n",
    "    # # Add a batch dimension if not already present\n",
    "    # if train_data_batch.dim() == 3: # dim == 5\n",
    "    #     train_data_batch = train_data_batch.unsqueeze(0)\n",
    "\n",
    "    # x = train_data_batch.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "\n",
    "    # data = dict(input_data = [x])\n",
    "    #res = ezkl.calibrate_settings(cal_data_path, model_path, settings_path, \"resources\", max_logrows = 12, scales = [2])\n",
    "\n",
    "    # cal_path = os.path.join('cal_data.json')\n",
    "    # # Serialize data into file:\n",
    "    # json.dump( data, open(cal_path, 'w' ))\n",
    "    os.environ['RUST_LOG'] = 'abc'\n",
    "    !RUST_LOG=none\n",
    "    res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "    assert res == True\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\", scales=[2,7])\n",
    "    assert res == True\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "    assert res == True\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    # srs path\n",
    "    res = ezkl.get_srs(settings_path)\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    # now generate the witness file\n",
    "    res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "    assert os.path.isfile(witness_path)\n",
    "\n",
    "    with open(witness_path, \"r\") as f:\n",
    "        wit = json.load(f)\n",
    "\n",
    "    with open(settings_path, \"r\") as f:\n",
    "        setting = json.load(f)\n",
    "\n",
    "    prediction_array = []\n",
    "    for value in wit[\"outputs\"]:\n",
    "        for field_element in value:\n",
    "            prediction_array.append(ezkl.vecu64_to_float(field_element, setting['model_output_scales'][0]))\n",
    "    return torch.argmax(torch.Tensor([prediction_array]), dim=1)\n",
    "    #print ('Prediction:', torch.argmax(torch.Tensor([prediction_array]), dim=1) == label.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_accuracy(folder, model, dataset, size):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    # folder = \"./tmp/\"\n",
    "    # Create the directory 'tmp' in the current working directory\n",
    "    try:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        print(f\"Directory '{folder}' created successfully\")\n",
    "    except OSError as error:\n",
    "        print(f\"Directory '{folder}' cannot be created. Error: {error}\")\n",
    "\n",
    "    for image, _ in dataset:\n",
    "        pred_quantized = gen_witness_LN(folder, model, image)\n",
    "        outputs = model(image.unsqueeze(0).to(device))\n",
    "        pred = torch.argmax(outputs, 1)\n",
    "\n",
    "        total += 1\n",
    "        correct += (pred == pred_quantized.to(device))\n",
    "\n",
    "        if total > size:\n",
    "            break\n",
    "    \n",
    "    return 100*correct/total\n",
    "    #print (f'Test Accuracy: {accuracy:.2f}% (quantized)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './tmp/' created successfully\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'none' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport RUST_LOG=none\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcomputer_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tmp/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mcomputer_accuracy\u001b[0;34m(folder, model, dataset, size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot be created. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, _ \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 14\u001b[0m     pred_quantized \u001b[38;5;241m=\u001b[39m \u001b[43mgen_witness_LN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     16\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 59\u001b[0m, in \u001b[0;36mgen_witness_LN\u001b[0;34m(folder, model, data_point)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# data_points = []\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# for i, (data_point, _) in enumerate(dataset):\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     if i >= num_data_points:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# # Serialize data into file:\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# json.dump( data, open(cal_path, 'w' ))\u001b[39;00m\n\u001b[1;32m     58\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRUST_LOG\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 59\u001b[0m RUST_LOG\u001b[38;5;241m=\u001b[39m\u001b[43mnone\u001b[49m\n\u001b[1;32m     60\u001b[0m res \u001b[38;5;241m=\u001b[39m ezkl\u001b[38;5;241m.\u001b[39mgen_settings(model_path, settings_path, py_run_args\u001b[38;5;241m=\u001b[39mrun_args)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'none' is not defined"
     ]
    }
   ],
   "source": [
    "!export RUST_LOG=none\n",
    "computer_accuracy('./tmp/', model, test_dataset,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.], device='cuda:0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_LN_proof():\n",
    "    compiled_model_path = os.path.join(folder, 'network.compiled')\n",
    "    settings_path = os.path.join(folder, 'settings.json') \n",
    "    witness_path = os.path.join(folder, 'witness.json')\n",
    "    proof_path = os.path.join(folder, 'proof.json')\n",
    "    srs_path = os.path.join(folder, 'kzg.srs')\n",
    "\n",
    "    pk_path = os.path.join(folder, 'test.pk')\n",
    "    vk_path = os.path.join(folder, 'test.vk')\n",
    "\n",
    "\n",
    "    res = ezkl.mock(witness_path, compiled_model_path)\n",
    "    assert res == True\n",
    "\n",
    "    res = ezkl.setup(\n",
    "            compiled_model_path,\n",
    "            vk_path,\n",
    "            pk_path,\n",
    "        )\n",
    "\n",
    "\n",
    "    assert res == True\n",
    "    assert os.path.isfile(vk_path)\n",
    "    assert os.path.isfile(pk_path)\n",
    "    assert os.path.isfile(settings_path)\n",
    "\n",
    "    # Generate the proof\n",
    "    proof = ezkl.prove(\n",
    "            witness_path,\n",
    "            compiled_model_path,\n",
    "            pk_path,\n",
    "            proof_path,\n",
    "            \"single\",\n",
    "        )\n",
    "    print(proof)\n",
    "    assert os.path.isfile(proof_path)\n",
    "\n",
    "    # verify our proof\n",
    "    res = ezkl.verify(\n",
    "            proof_path,\n",
    "            settings_path,\n",
    "            vk_path,\n",
    "        )\n",
    "\n",
    "    assert res == True\n",
    "    print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 2\n",
      "spawning module 2\n",
      "spawning module 2\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [[[10799958826997019694, 13732764088618192092, 9080164469342476020, 2707268858854354691], [14424721036421401993, 1925070949615222816, 15662063748651391231, 2658040235317113047], [1215616954971237343, 12781145858640582370, 3205897164118176512, 2866280021565530741], [16586418689708852361, 7719159825332846314, 11409209055533372999, 2915207530297009380], [6517656510695314680, 15914426195874827226, 4307566795511864752, 3240703511491837769], [4164547517116989137, 10669376283223625029, 6844937459063939938, 3458232284123655186], [10397520845017782036, 13129438026871282772, 16302847471085186583, 1825458023941667617], [6523463390768162622, 11459244905309910398, 17257037644834070972, 1430799349594821442], [13327914677011761971, 3991458216124611539, 4353392374763218262, 501949994929955151], [11035042354946947612, 2049718776522464053, 8894110146517096806, 3168295732512543866]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "verify_LN_proof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
