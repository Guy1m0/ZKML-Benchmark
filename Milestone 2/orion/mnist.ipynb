{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import torch\n",
    "\n",
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "folder = \"../model/\"\n",
    "#model_tf = tf.keras.models.load_model(folder + 'model_tf')\n",
    "\n",
    "model_tf = tf.keras.models.load_model(folder + 'model_tf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (60000,196)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m x_test \u001b[38;5;241m=\u001b[39m resize_images(test_images)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Then reshape\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m x_test \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m14\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[1;32m     12\u001b[0m x_train \u001b[38;5;241m=\u001b[39m train_images\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (60000,196)"
     ]
    }
   ],
   "source": [
    "# Resizing function\n",
    "def resize_images(images):\n",
    "    return np.array([zoom(image, 0.5) for image in images])\n",
    "\n",
    "# Resize\n",
    "x_train = resize_images(train_images)\n",
    "x_test = resize_images(test_images)\n",
    "\n",
    "# Then reshape\n",
    "x_train = x_train.reshape(60000, 14*14)\n",
    "x_test = x_test.reshape(10000, 14*14)\n",
    "x_train = train_images.astype('float32')\n",
    "x_test = test_images.astype('float32')\n",
    "\n",
    "# normalize to range [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 28, 28, 1)         3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_conv2d_4 (QuantizeWr  (None, 24, 24, 6)         171       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_average_pooling2d_4   (None, 12, 12, 6)         3         \n",
      " (QuantizeWrapperV2)                                             \n",
      "                                                                 \n",
      " quant_conv2d_5 (QuantizeWr  (None, 8, 8, 16)          2451      \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_average_pooling2d_5   (None, 4, 4, 16)          3         \n",
      " (QuantizeWrapperV2)                                             \n",
      "                                                                 \n",
      " quant_flatten_2 (QuantizeW  (None, 256)               1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_6 (QuantizeWra  (None, 120)               30845     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_7 (QuantizeWra  (None, 84)                10169     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_8 (QuantizeWra  (None, 10)                855       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44501 (173.83 KB)\n",
      "Trainable params: 44426 (173.54 KB)\n",
      "Non-trainable params: 75 (300.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Apply quantization to the layers\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model_tf)\n",
    "\n",
    "# 'quantize_model' requires a recompile\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to TFLite Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/k7/qnxvvp9d0gv2p52w6qt29gsr0000gn/T/tmpss__xolb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/k7/qnxvvp9d0gv2p52w6qt29gsr0000gn/T/tmpss__xolb/assets\n",
      "/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:947: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-01-25 11:46:10.024932: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-25 11:46:10.024944: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-25 11:46:10.025062: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/k7/qnxvvp9d0gv2p52w6qt29gsr0000gn/T/tmpss__xolb\n",
      "2024-01-25 11:46:10.027060: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-25 11:46:10.027066: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/k7/qnxvvp9d0gv2p52w6qt29gsr0000gn/T/tmpss__xolb\n",
      "2024-01-25 11:46:10.033016: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-25 11:46:10.081220: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/k7/qnxvvp9d0gv2p52w6qt29gsr0000gn/T/tmpss__xolb\n",
      "2024-01-25 11:46:10.099013: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 73951 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51600"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "\n",
    "# Indicate that you want to perform default optimizations,\n",
    "# which include quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Define a generator function that provides your test data's numpy arrays\n",
    "def representative_data_gen():\n",
    "  for i in range(500):\n",
    "    yield [np.array(train_images[i:i+1], dtype=np.float32)]\n",
    "\n",
    "# Use the generator function to guide the quantization process\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"../model/q_aware_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"../model/q_aware_model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
