{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Dependencies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define Hyperparameters\n",
    "\n",
    "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
    "hidden_size = 20 # number of nodes at hidden layer\n",
    "num_classes = 10 # number of output classes discrete range [0,9]\n",
    "num_epochs = 20 # number of times which the entire dataset is passed throughout the model\n",
    "batch_size = 100 # the size of input data took for one iteration\n",
    "lr = 1e-3 # size of step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 28402590.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 6406584.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 7325341.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 12057296.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Downloading MNIST data\n",
    "\n",
    "train_data = dsets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "test_data = dsets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Loading the data\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                             batch_size = batch_size,\n",
    "                                             shuffle = True)\n",
    "\n",
    "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                      batch_size = batch_size, \n",
    "                                      shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define model class\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Build the model\n",
    "\n",
    "net = Net(input_size, hidden_size, num_classes)\n",
    "if torch.cuda.is_available():\n",
    "  net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define loss-function & optimizer\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam( net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/600], Loss: 0.7586\n",
      "Epoch [1/20], Step [200/600], Loss: 0.6578\n",
      "Epoch [1/20], Step [300/600], Loss: 0.4132\n",
      "Epoch [1/20], Step [400/600], Loss: 0.2653\n",
      "Epoch [1/20], Step [500/600], Loss: 0.3760\n",
      "Epoch [1/20], Step [600/600], Loss: 0.2321\n",
      "Epoch [2/20], Step [100/600], Loss: 0.2010\n",
      "Epoch [2/20], Step [200/600], Loss: 0.2502\n",
      "Epoch [2/20], Step [300/600], Loss: 0.2265\n",
      "Epoch [2/20], Step [400/600], Loss: 0.1894\n",
      "Epoch [2/20], Step [500/600], Loss: 0.3196\n",
      "Epoch [2/20], Step [600/600], Loss: 0.1802\n",
      "Epoch [3/20], Step [100/600], Loss: 0.3340\n",
      "Epoch [3/20], Step [200/600], Loss: 0.2048\n",
      "Epoch [3/20], Step [300/600], Loss: 0.1734\n",
      "Epoch [3/20], Step [400/600], Loss: 0.3398\n",
      "Epoch [3/20], Step [500/600], Loss: 0.1892\n",
      "Epoch [3/20], Step [600/600], Loss: 0.2637\n",
      "Epoch [4/20], Step [100/600], Loss: 0.2435\n",
      "Epoch [4/20], Step [200/600], Loss: 0.3686\n",
      "Epoch [4/20], Step [300/600], Loss: 0.3716\n",
      "Epoch [4/20], Step [400/600], Loss: 0.3624\n",
      "Epoch [4/20], Step [500/600], Loss: 0.2705\n",
      "Epoch [4/20], Step [600/600], Loss: 0.2090\n",
      "Epoch [5/20], Step [100/600], Loss: 0.1542\n",
      "Epoch [5/20], Step [200/600], Loss: 0.1493\n",
      "Epoch [5/20], Step [300/600], Loss: 0.2130\n",
      "Epoch [5/20], Step [400/600], Loss: 0.1685\n",
      "Epoch [5/20], Step [500/600], Loss: 0.2899\n",
      "Epoch [5/20], Step [600/600], Loss: 0.1895\n",
      "Epoch [6/20], Step [100/600], Loss: 0.2628\n",
      "Epoch [6/20], Step [200/600], Loss: 0.2071\n",
      "Epoch [6/20], Step [300/600], Loss: 0.0898\n",
      "Epoch [6/20], Step [400/600], Loss: 0.1123\n",
      "Epoch [6/20], Step [500/600], Loss: 0.1715\n",
      "Epoch [6/20], Step [600/600], Loss: 0.2295\n",
      "Epoch [7/20], Step [100/600], Loss: 0.1155\n",
      "Epoch [7/20], Step [200/600], Loss: 0.1513\n",
      "Epoch [7/20], Step [300/600], Loss: 0.1155\n",
      "Epoch [7/20], Step [400/600], Loss: 0.1920\n",
      "Epoch [7/20], Step [500/600], Loss: 0.2464\n",
      "Epoch [7/20], Step [600/600], Loss: 0.0735\n",
      "Epoch [8/20], Step [100/600], Loss: 0.1250\n",
      "Epoch [8/20], Step [200/600], Loss: 0.1276\n",
      "Epoch [8/20], Step [300/600], Loss: 0.1443\n",
      "Epoch [8/20], Step [400/600], Loss: 0.0967\n",
      "Epoch [8/20], Step [500/600], Loss: 0.1119\n",
      "Epoch [8/20], Step [600/600], Loss: 0.1230\n",
      "Epoch [9/20], Step [100/600], Loss: 0.1142\n",
      "Epoch [9/20], Step [200/600], Loss: 0.1825\n",
      "Epoch [9/20], Step [300/600], Loss: 0.1516\n",
      "Epoch [9/20], Step [400/600], Loss: 0.2317\n",
      "Epoch [9/20], Step [500/600], Loss: 0.1516\n",
      "Epoch [9/20], Step [600/600], Loss: 0.0816\n",
      "Epoch [10/20], Step [100/600], Loss: 0.1645\n",
      "Epoch [10/20], Step [200/600], Loss: 0.1152\n",
      "Epoch [10/20], Step [300/600], Loss: 0.1192\n",
      "Epoch [10/20], Step [400/600], Loss: 0.1058\n",
      "Epoch [10/20], Step [500/600], Loss: 0.2072\n",
      "Epoch [10/20], Step [600/600], Loss: 0.1733\n",
      "Epoch [11/20], Step [100/600], Loss: 0.1161\n",
      "Epoch [11/20], Step [200/600], Loss: 0.1378\n",
      "Epoch [11/20], Step [300/600], Loss: 0.1265\n",
      "Epoch [11/20], Step [400/600], Loss: 0.2290\n",
      "Epoch [11/20], Step [500/600], Loss: 0.1156\n",
      "Epoch [11/20], Step [600/600], Loss: 0.0995\n",
      "Epoch [12/20], Step [100/600], Loss: 0.1722\n",
      "Epoch [12/20], Step [200/600], Loss: 0.0980\n",
      "Epoch [12/20], Step [300/600], Loss: 0.1267\n",
      "Epoch [12/20], Step [400/600], Loss: 0.0467\n",
      "Epoch [12/20], Step [500/600], Loss: 0.1382\n",
      "Epoch [12/20], Step [600/600], Loss: 0.1339\n",
      "Epoch [13/20], Step [100/600], Loss: 0.1389\n",
      "Epoch [13/20], Step [200/600], Loss: 0.0930\n",
      "Epoch [13/20], Step [300/600], Loss: 0.0770\n",
      "Epoch [13/20], Step [400/600], Loss: 0.0875\n",
      "Epoch [13/20], Step [500/600], Loss: 0.0931\n",
      "Epoch [13/20], Step [600/600], Loss: 0.1588\n",
      "Epoch [14/20], Step [100/600], Loss: 0.0850\n",
      "Epoch [14/20], Step [200/600], Loss: 0.2115\n",
      "Epoch [14/20], Step [300/600], Loss: 0.0677\n",
      "Epoch [14/20], Step [400/600], Loss: 0.1456\n",
      "Epoch [14/20], Step [500/600], Loss: 0.1269\n",
      "Epoch [14/20], Step [600/600], Loss: 0.1360\n",
      "Epoch [15/20], Step [100/600], Loss: 0.2047\n",
      "Epoch [15/20], Step [200/600], Loss: 0.1644\n",
      "Epoch [15/20], Step [300/600], Loss: 0.0949\n",
      "Epoch [15/20], Step [400/600], Loss: 0.0733\n",
      "Epoch [15/20], Step [500/600], Loss: 0.0711\n",
      "Epoch [15/20], Step [600/600], Loss: 0.1456\n",
      "Epoch [16/20], Step [100/600], Loss: 0.0946\n",
      "Epoch [16/20], Step [200/600], Loss: 0.1493\n",
      "Epoch [16/20], Step [300/600], Loss: 0.1525\n",
      "Epoch [16/20], Step [400/600], Loss: 0.0556\n",
      "Epoch [16/20], Step [500/600], Loss: 0.2276\n",
      "Epoch [16/20], Step [600/600], Loss: 0.1088\n",
      "Epoch [17/20], Step [100/600], Loss: 0.0487\n",
      "Epoch [17/20], Step [200/600], Loss: 0.0929\n",
      "Epoch [17/20], Step [300/600], Loss: 0.0809\n",
      "Epoch [17/20], Step [400/600], Loss: 0.1210\n",
      "Epoch [17/20], Step [500/600], Loss: 0.0739\n",
      "Epoch [17/20], Step [600/600], Loss: 0.1376\n",
      "Epoch [18/20], Step [100/600], Loss: 0.1401\n",
      "Epoch [18/20], Step [200/600], Loss: 0.1457\n",
      "Epoch [18/20], Step [300/600], Loss: 0.0723\n",
      "Epoch [18/20], Step [400/600], Loss: 0.2226\n",
      "Epoch [18/20], Step [500/600], Loss: 0.0641\n",
      "Epoch [18/20], Step [600/600], Loss: 0.1450\n",
      "Epoch [19/20], Step [100/600], Loss: 0.1496\n",
      "Epoch [19/20], Step [200/600], Loss: 0.1327\n",
      "Epoch [19/20], Step [300/600], Loss: 0.0711\n",
      "Epoch [19/20], Step [400/600], Loss: 0.1269\n",
      "Epoch [19/20], Step [500/600], Loss: 0.0667\n",
      "Epoch [19/20], Step [600/600], Loss: 0.0898\n",
      "Epoch [20/20], Step [100/600], Loss: 0.0569\n",
      "Epoch [20/20], Step [200/600], Loss: 0.1008\n",
      "Epoch [20/20], Step [300/600], Loss: 0.0970\n",
      "Epoch [20/20], Step [400/600], Loss: 0.1094\n",
      "Epoch [20/20], Step [500/600], Loss: 0.0969\n",
      "Epoch [20/20], Step [600/600], Loss: 0.0764\n"
     ]
    }
   ],
   "source": [
    "#@title Training the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for i ,(images,labels) in enumerate(train_gen):\n",
    "    images = Variable(images.view(-1,28*28)).cuda()\n",
    "    labels = Variable(labels).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(images)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 96.100 %\n"
     ]
    }
   ],
   "source": [
    "#@title Evaluating the accuracy of the model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images,labels in test_gen:\n",
    "  images = Variable(images.view(-1,28*28)).cuda()\n",
    "  labels = labels.cuda()\n",
    "  \n",
    "  output = net(images)\n",
    "  _, predicted = torch.max(output,1)\n",
    "  correct += (predicted == labels).sum()\n",
    "  total += labels.size(0)\n",
    "\n",
    "print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([20, 784])\n",
      "fc1.bias \t torch.Size([20])\n",
      "fc2.weight \t torch.Size([10, 20])\n",
      "fc2.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"../models/mnist/mnist-small.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
