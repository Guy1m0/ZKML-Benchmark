{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b70084b-44da-4142-9e24-c9c8231828db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7533193-266d-4a59-b9aa-54179d40aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "EXPONENT = 15\n",
    "\n",
    "class SeparableConv2D(nn.Module):\n",
    "    '''Separable convolution'''\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SeparableConv2D, self).__init__()\n",
    "        self.dw_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.pw_conv =  nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw_conv(x)\n",
    "        x = self.pw_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dec98ed-cd14-442b-93b5-8f3660726773",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 5, 5))\n",
    "model = SeparableConv2D(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b62f7c-5ac1-4c69-9add-9a859d66c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides, col*strides, k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                str_out[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    return out, str_out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c664ba3-b722-482b-84f4-f83bab1d1bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized_image.shape=(5, 5, 3)\n",
      "quantized_weights.shape=(3, 6)\n",
      "expected.shape=(1, 6, 5, 5)\n",
      "expected.shape=(5, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "weights = model.pw_conv.weight.detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = model.pw_conv(input).detach().numpy()\n",
    "\n",
    "weights = weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1, 2, 0)) * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "print(f\"{quantized_image.shape=}\")\n",
    "print(f\"{quantized_weights.shape=}\")\n",
    "\n",
    "actual, str_actual, remainder = PointwiseConv2d(5, 5, 3, 6, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "actual_scaled = [[[actual[i][j][k] / 10**EXPONENT for k in range(6)] for j in range(5)] for i in range(5)]\n",
    "\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(expected, actual_scaled, atol=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b595d39-ca92-4c45-b04e-e7eb85b4c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_in = quantized_image.round().astype(int).astype(str).tolist()\n",
    "circuit_weights = quantized_weights.round().astype(int).astype(str).tolist()\n",
    "circuit_bias = bias.round().astype(int).astype(str).tolist()\n",
    "\n",
    "input_json_path = \"pointwiseConv2D_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": circuit_in,\n",
    "               \"weights\": circuit_weights,\n",
    "               \"remainder\": remainder,\n",
    "               \"out\": str_actual,\n",
    "               \"bias\": circuit_bias,\n",
    "              },\n",
    "              input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f84d3-4d70-421c-9067-5ed314c967a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
