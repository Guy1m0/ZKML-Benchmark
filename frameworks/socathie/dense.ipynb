{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 21888242871839275222246405745257275088548364400416034343698204186575808495617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and flatten the images\n",
    "train_images_tf = train_images.reshape((-1, 28*28)) / 255.0\n",
    "test_images_tf = test_images.reshape((-1, 28*28)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_35 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 56)                7224      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 10)                570       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108274 (422.95 KB)\n",
      "Trainable params: 108274 (422.95 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "model_tf = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28*28,)),  # Adjusted for 28x28 images\n",
    "    keras.layers.Dense(128, activation='relu'),     # Increased number of neurons\n",
    "    keras.layers.Dense(56, activation='relu'),      # Additional hidden layer\n",
    "    keras.layers.Dense(num_classes, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "model_tf.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2548 - accuracy: 0.9257 - val_loss: 0.1077 - val_accuracy: 0.9710\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.9673 - val_loss: 0.0914 - val_accuracy: 0.9735\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0743 - accuracy: 0.9768 - val_loss: 0.0848 - val_accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0567 - accuracy: 0.9830 - val_loss: 0.0884 - val_accuracy: 0.9768\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0454 - accuracy: 0.9855 - val_loss: 0.0735 - val_accuracy: 0.9787\n",
      "313/313 - 0s - loss: 0.0787 - accuracy: 0.9762 - 173ms/epoch - 552us/step\n",
      "\n",
      "Test accuracy: 0.9761999845504761\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_tf.fit(train_images_tf, train_labels, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming test_images_tf is your flattened and normalized test dataset\n",
    "scaled_test_images_tf = np.array([int(x * 1e36) for x in test_images_tf.flatten()]).reshape(test_images_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "1/1 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = test_images_tf[35:36]\n",
    "print (X.shape)\n",
    "pred = model_tf.predict(X)\n",
    "X_in = scaled_test_images_tf[35]\n",
    "\n",
    "np.argmax(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784: input\n",
    "# 56: first layer\n",
    "\n",
    "# weights = [[None for _ in range(56)] for _ in range(784)]\n",
    "# for i in range(784):\n",
    "#     for j in range(56):\n",
    "#         weights[i][j] = int(model_tf.get_weights()[0][i][j]*1e36)\n",
    "#         #print (i,j)\n",
    "# bias = [int(b*1e72) for b in model_tf.get_weights()[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new layer\n",
      "new layer\n",
      "new layer\n"
     ]
    }
   ],
   "source": [
    "# Scale the weights and biases\n",
    "weights_ = []\n",
    "biases_= []\n",
    "for layer in model_tf.layers:\n",
    "    print ('new layer')\n",
    "    weight, bias = layer.get_weights()\n",
    "    scaled_weights = weight * 1e36\n",
    "    scaled_biases = bias * 1e72\n",
    "    weights_.append(scaled_weights)\n",
    "    biases_.append(scaled_biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_mod(x, p):\n",
    "    return x if x < p // 2 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseInt(nInputs, nOutputs, n, input, weights, bias, act = False):\n",
    "    Input = [str(input[i] % p) for i in range(nInputs)]\n",
    "    Weights = [[str(weights[i][j] % p) for j in range(nOutputs)] for i in range(nInputs)]\n",
    "    Bias = [str(bias[i] % p) for i in range(nOutputs)]\n",
    "\n",
    "    out = [0 for _ in range(nOutputs)]\n",
    "    remainder = [None for _ in range(nOutputs)]\n",
    "    \n",
    "    for j in range(nOutputs):\n",
    "        for i in range(nInputs):\n",
    "            out[j] += int(input[i] * weights[i][j])\n",
    "        out[j] += int(bias[j])\n",
    "\n",
    "        remainder[j] = str(out[j] % n)\n",
    "        out[j] = out[j] // n % p\n",
    "\n",
    "    if act:\n",
    "        out = [int(relu_mod(out[i], p)) for i in range(nOutputs)]\n",
    "\n",
    "    return Input, Weights, Bias, out, remainder\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.375382  , 0.49137157, 0.        , 0.        , 0.        ,\n",
       "        2.7461276 , 0.        , 1.3943845 , 0.        , 0.        ,\n",
       "        0.27399603, 1.8222868 , 0.18728077, 0.        , 0.1530143 ,\n",
       "        0.        , 3.81958   , 0.        , 0.        , 2.005937  ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.0538322 , 0.        , 0.        ,\n",
       "        0.        , 2.4923813 , 0.        , 3.947798  , 0.        ,\n",
       "        4.751608  , 1.5282661 , 3.248456  , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.114225  , 0.        ,\n",
       "        0.        , 0.        , 0.77713984, 0.        , 0.        ,\n",
       "        0.        , 1.5331594 , 0.        , 1.0734144 , 0.        ,\n",
       "        0.        , 0.        , 3.0525808 , 0.        , 3.0161211 ,\n",
       "        0.        , 1.844801  , 0.        , 0.52690274, 0.        ,\n",
       "        1.366203  , 0.        , 0.29555848, 0.25899604, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14221986, 0.        , 0.        , 0.        , 2.701122  ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 3.2877893 , 0.        , 0.        , 0.9460192 ,\n",
       "        0.67913955, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 2.2855563 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 3.8106854 , 0.        , 0.        , 0.        ,\n",
       "        1.9057763 , 0.34704232, 0.        , 0.        , 0.9394212 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.2669081 ,\n",
       "        0.        , 0.        , 0.        , 0.03923531, 2.1830091 ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Assuming model_tf is your original model\n",
    "# Extract the output of the second-to-last layer (the last dense layer before softmax)\n",
    "layer_output = model_tf.layers[0].output\n",
    "\n",
    "# Create a new model that gives the output of the last dense layer\n",
    "intermediate_model = keras.Model(inputs=model_tf.input, outputs=layer_output)\n",
    "\n",
    "intermediate_model.predict(X.reshape(1, -1))  # Reshape if necessary\n",
    "\n",
    "# 'output' now contains the output of the last dense layer before softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3375381873201985182738022848202033381,\n",
       " 491371555592277808098225530817208883,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2746127526227803897179978336235470710,\n",
       " 0,\n",
       " 1394384400855458558398448095356236748,\n",
       " 0,\n",
       " 0,\n",
       " 273996002975310980172948651308810750,\n",
       " 1822286767713078156796429369278216340,\n",
       " 187280799281492287654653559384707185,\n",
       " 0,\n",
       " 153014248503443167407922129459733718,\n",
       " 0,\n",
       " 3819580052070898577451593029051574115,\n",
       " 0,\n",
       " 0,\n",
       " 2005936983366423942113982039584815054,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1053832040409885290566714602019741974,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2492381362501049651179951818598837780,\n",
       " 0,\n",
       " 3947797906256473202135487429671326501,\n",
       " 0,\n",
       " 4751607755044985092345537469015455317,\n",
       " 1528266096741350072767256649772834509,\n",
       " 3248455904660749750213052330145580302,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1114224854925594976220068868130715693,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 777139742241511025851038690606591247,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1533159159239233791519814402303068134,\n",
       " 0,\n",
       " 1073414412194954880877307747578461577,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3052580616941182045652282756194233641,\n",
       " 0,\n",
       " 3016120960953380211057518546480664606,\n",
       " 0,\n",
       " 1844800806399714921356244471171613541,\n",
       " 0,\n",
       " 526902607638930723758169949858171107,\n",
       " 0,\n",
       " 1366202926964411102679301675833379696,\n",
       " 0,\n",
       " 295558486306162548714093297250317997,\n",
       " 258996100576708784476375013227507520,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 142219813078374643241698941410870821,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2701121850556077730357215331610943994,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3287789058830176486173137782672971940,\n",
       " 0,\n",
       " 0,\n",
       " 946019136214524697250806530475599424,\n",
       " 679139493323479672926946584644146323,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2285556395811669747861010703001406719,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3810685219907751708664401114671960216,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1905776001222699791049001607218263459,\n",
       " 347042280831993612777313583769145507,\n",
       " 0,\n",
       " 0,\n",
       " 939421146143294425434566454161424018,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 266908092930379939381998675083023218,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 39235313393402798965483491616169635,\n",
       " 2183009053853928651821093476694068978,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, dense_out_1, remainder = DenseInt(784, 128, 10**36, X_in, weights_[0], biases_[0], act=True)\n",
    "dense_out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.7998114e+00, 3.5995545e+00, 1.0065154e+00, 1.1879711e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 3.7056834e-01, 7.3919344e+00,\n",
       "        5.1079183e+00, 4.2184820e+00, 6.3148885e+00, 0.0000000e+00,\n",
       "        3.1377190e-01, 0.0000000e+00, 5.2447896e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 2.1777198e+00, 6.4400892e+00,\n",
       "        0.0000000e+00, 1.7147572e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 4.8927170e-01, 5.3289118e+00, 0.0000000e+00,\n",
       "        1.4893204e-01, 1.7732893e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.7414149e+00, 0.0000000e+00, 3.9342020e+00, 0.0000000e+00,\n",
       "        3.3948030e+00, 0.0000000e+00, 9.8584837e-01, 0.0000000e+00,\n",
       "        2.9554210e+00, 3.7334976e+00, 2.0726490e+00, 0.0000000e+00,\n",
       "        2.9684455e+00, 1.5995535e+00, 4.5307167e-03, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.8244205e+00, 5.4183393e+00, 1.4669970e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 2.1465123e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming model_tf is your original model\n",
    "# Extract the output of the second-to-last layer (the last dense layer before softmax)\n",
    "layer_output = model_tf.layers[1].output\n",
    "\n",
    "# Create a new model that gives the output of the last dense layer\n",
    "intermediate_model = keras.Model(inputs=model_tf.input, outputs=layer_output)\n",
    "\n",
    "intermediate_model.predict(X.reshape(1, -1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1799811168945183391133649237475388384,\n",
       " 3599554304825136730115198564114784573,\n",
       " 1006515242317991796096482474376390670,\n",
       " 1187970724993925820933614262698385501,\n",
       " 0,\n",
       " 0,\n",
       " 370568271791621067211333227519760671,\n",
       " 7391933162643583765253476260882512862,\n",
       " 5107917561764041572772999759861393723,\n",
       " 4218480951854453722475344016106765901,\n",
       " 6314887591317372780477050983263928089,\n",
       " 0,\n",
       " 313771887277411738014051918201356106,\n",
       " 0,\n",
       " 5244789399721479966223305066478368041,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2177719625521800653722373668660703582,\n",
       " 6440087986697802688884084173845061739,\n",
       " 0,\n",
       " 1714757121351988555720966217734536444,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 489271733204142408972072936366789643,\n",
       " 5328911268936352025078491450399479805,\n",
       " 0,\n",
       " 148931628646414118502103920634336159,\n",
       " 1773289136588075492350491395702313299,\n",
       " 0,\n",
       " 0,\n",
       " 1741414891746437963859783522430598821,\n",
       " 0,\n",
       " 3934201519614926224046119429234402797,\n",
       " 0,\n",
       " 3394802909549501723115129918067359101,\n",
       " 0,\n",
       " 985848316349012788372846618805729953,\n",
       " 0,\n",
       " 2955420608492172885712415240469815566,\n",
       " 3733497313348043299389387778409922530,\n",
       " 2072649069269288869194975410650362424,\n",
       " 0,\n",
       " 2968445579207242679908015262074411724,\n",
       " 1599553018279133055428478023201817336,\n",
       " 4530865895025807809035302466392592,\n",
       " 0,\n",
       " 0,\n",
       " 1824420283428225724270893285434212056,\n",
       " 5418338608796433374929511612936436702,\n",
       " 1466996591169037839126215544419228396,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2146511826426252152908113121045707526]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, dense_out_2, remainder = DenseInt(128, 56, 10**36, dense_out_1, weights_[1], biases_[1], act=True)\n",
    "dense_out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 5847633043705220765213876701035121626,\n",
       " 12790728714326361487264012783993893283,\n",
       " 319710414755301294575150273149469515,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 269239701297592828236715006057703100,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, out_, remainder_ = DenseInt(56, 10, 10**36, dense_out_2, weights_[2], biases_[2], act=True)\n",
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_exp(x, p):\n",
    "    return pow(int(x), 1, p)  # Modular exponentiation\n",
    "\n",
    "def mod_softmax(out, p):\n",
    "    # Apply modular exponentiation and then normalize\n",
    "    exp_out = [mod_exp(x, p) for x in out]\n",
    "    sum_exp_out = sum(exp_out) % p\n",
    "    softmax_out = [x * pow(sum_exp_out, -1, p) % p for x in exp_out]  # Modular division\n",
    "    return [int(x) for x in softmax_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.7557297e-09, 9.6433697e-04, 9.9902785e-01, 3.8324997e-06,\n",
       "        4.0801211e-14, 5.9987293e-08, 8.9261287e-10, 3.6438721e-06,\n",
       "        3.5281343e-07, 2.5885273e-13]], dtype=float32)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf.predict(X.reshape(1, -1))  # Reshape if necessary\n",
    "\n",
    "# 'output' now contains the output of the last dense layer before softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_json = {\n",
    "    \"in\": X_in,\n",
    "    \"weights\": weights,\n",
    "    \"bias\": bias,\n",
    "    \"out\": out,\n",
    "    \"remainder\": remainder\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dense_input.json\", \"w\") as f:\n",
    "    json.dump(in_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
