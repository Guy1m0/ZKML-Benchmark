{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 23:58:38.039221: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 23:58:38.072168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 23:58:38.072197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 23:58:38.072918: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 23:58:38.077703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 23:58:38.608930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and flatten the images\n",
    "train_images_tf = train_images.reshape((-1, 28*28)) / 255.0\n",
    "test_images_tf = test_images.reshape((-1, 28*28)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 23:58:50.860708: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.861750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.861863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.862871: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.862981: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.863061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.952429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.952543: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.952634: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 23:58:50.952712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20840 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "num_classes = 10\n",
    "\n",
    "model_tf = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28*28,)),  # Adjusted for 28x28 images\n",
    "#    keras.layers.Dense(128, activation='relu'),     # Increased number of neurons\n",
    "    keras.layers.Dense(56, activation='relu'),      # Additional hidden layer\n",
    "    keras.layers.Dense(num_classes, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "model_tf.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 23:58:57.556004: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-30 23:58:57.597950: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3270003070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-30 23:58:57.597968: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-01-30 23:58:57.602834: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-30 23:58:57.615763: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706630337.675971  398306 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3203 - accuracy: 0.9100 - val_loss: 0.1511 - val_accuracy: 0.9580\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1595 - accuracy: 0.9542 - val_loss: 0.1119 - val_accuracy: 0.9673\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1159 - accuracy: 0.9664 - val_loss: 0.1066 - val_accuracy: 0.9670\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0918 - accuracy: 0.9723 - val_loss: 0.0940 - val_accuracy: 0.9727\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0758 - accuracy: 0.9768 - val_loss: 0.0907 - val_accuracy: 0.9747\n",
      "313/313 - 0s - loss: 0.0918 - accuracy: 0.9722 - 229ms/epoch - 731us/step\n",
      "\n",
      "Test accuracy: 0.9721999764442444\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_tf.fit(train_images_tf, train_labels, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvk4n6cbj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvk4n6cbj/assets\n",
      "2024-01-31 00:29:59.152855: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-31 00:29:59.152873: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-31 00:29:59.153013: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpvk4n6cbj\n",
      "2024-01-31 00:29:59.153472: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-31 00:29:59.153481: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpvk4n6cbj\n",
      "2024-01-31 00:29:59.154789: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-31 00:29:59.180267: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpvk4n6cbj\n",
      "2024-01-31 00:29:59.186787: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 33774 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 4, Total Ops 10, % non-converted = 40.00 %\n",
      " * 4 ARITH ops\n",
      "\n",
      "- arith.constant:    4 occurrences  (f32: 4)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model_tf.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 784)               3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrapp  (None, 56)                43965     \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWra  (None, 10)                575       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44543 (174.00 KB)\n",
      "Trainable params: 44530 (173.95 KB)\n",
      "Non-trainable params: 13 (52.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Apply quantization to the layers\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model_tf)\n",
    "\n",
    "# 'quantize_model' requires a recompile\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.0656 - accuracy: 0.9801 - val_loss: 0.0917 - val_accuracy: 0.9742\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0544 - accuracy: 0.9836 - val_loss: 0.0998 - val_accuracy: 0.9743\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0474 - accuracy: 0.9850 - val_loss: 0.0880 - val_accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0406 - accuracy: 0.9875 - val_loss: 0.0907 - val_accuracy: 0.9767\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0361 - accuracy: 0.9884 - val_loss: 0.0969 - val_accuracy: 0.9763\n",
      "313/313 - 0s - loss: 0.1000 - accuracy: 0.9713 - 254ms/epoch - 811us/step\n",
      "\n",
      "Test accuracy: 0.9713000059127808\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "q_aware_model.fit(train_images_tf, train_labels, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = q_aware_model.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_hiyp0no/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_hiyp0no/assets\n",
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-01-31 00:03:37.720070: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-31 00:03:37.720084: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-31 00:03:37.720262: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp_hiyp0no\n",
      "2024-01-31 00:03:37.721044: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-31 00:03:37.721054: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp_hiyp0no\n",
      "2024-01-31 00:03:37.723067: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-01-31 00:03:37.723678: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-31 00:03:37.749987: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp_hiyp0no\n",
      "2024-01-31 00:03:37.756937: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 36675 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 0, Total Ops 12, % non-converted = 0.00 %\n",
      " * \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2, uq_32: 2)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46896"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "\n",
    "# Indicate that you want to perform default optimizations,\n",
    "# which include quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Define a generator function that provides your test data's numpy arrays\n",
    "def representative_data_gen():\n",
    "  for i in range(500):\n",
    "    yield [np.array(train_images[i:i+1], dtype=np.float32)]\n",
    "\n",
    "# Use the generator function to guide the quantization process\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"q_aware_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to tflite and msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_path = \"./tools/converter.py\"\n",
    "model_in_path = \"model_tf.tflite\"\n",
    "model_out_path = \"converted_model.msgpack\"\n",
    "config_out_path = \"config.msgpack\"\n",
    "\n",
    "scale_factor = 512\n",
    "k = 17\n",
    "num_cols = 10 # not understand\n",
    "num_randoms = 1024 # not understand\n",
    "\n",
    "command_1 = [\"python\", f\"{convert_path}\", \"--model\", f\"{model_in_path}\",\n",
    "           \"--model_output\", f\"{model_out_path}\", \"--config_output\",\n",
    "           f\"{config_out_path}\", \"--scale_factor\", str(scale_factor),\n",
    "           \"--k\", str(k), \"--num_cols\", str(num_cols), \"--num_randoms\",\n",
    "           str(num_randoms)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 00:30:12.759571: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-31 00:30:12.779368: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-31 00:30:12.779391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-31 00:30:12.779905: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-31 00:30:12.782972: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 00:30:13.149245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'layer_type': 'FullyConnected', 'inp_idxes': [0, 3, 2], 'inp_shapes': [[1, 784], [56, 784], [56]], 'out_idxes': [5], 'out_shapes': [[1, 56]], 'params': [1], 'mask': []}, {'layer_type': 'FullyConnected', 'inp_idxes': [5, 4, 1], 'inp_shapes': [[1, 56], [10, 56], [10]], 'out_idxes': [6], 'out_shapes': [[1, 10]], 'params': [0], 'mask': []}, {'layer_type': 'Softmax', 'inp_idxes': [6], 'inp_shapes': [[1, 10]], 'out_idxes': [7], 'out_shapes': [[1, 10]], 'params': [], 'mask': []}]\n",
      "\n",
      "keep tensors: {0, 1, 2, 3, 4, 5, 6}\n",
      "skipping generated tensor: 0, b'serving_default_input_1:0'\n",
      "skipping generated tensor: 5, b'sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd'\n",
      "skipping generated tensor: 6, b'sequential/dense_1/MatMul;sequential/dense_1/BiasAdd'\n",
      "\n",
      "{'layer_type': 'Softmax', 'inp_idxes': [6], 'inp_shapes': [[1, 10]], 'out_idxes': [7], 'out_shapes': [[1, 10]], 'params': [], 'mask': []}\n",
      "dict_keys(['global_sf', 'k', 'num_cols', 'num_random', 'inp_idxes', 'out_idxes', 'layers', 'tensors', 'use_selectors', 'commit_before', 'commit_after'])\n",
      "[7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', './tools/converter.py', '--model', 'model_tf.tflite', '--model_output', 'converted_model.msgpack', '--config_output', 'config.msgpack', '--scale_factor', '512', '--k', '17', '--num_cols', '10', '--num_randoms', '1024'], returncode=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess, re\n",
    "# Start the subprocess\n",
    "subprocess.run(command_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert img to npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 (784,)\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "x = test_images_tf[index]\n",
    "\n",
    "print(x.dtype, x.shape)\n",
    "np.save(str(index)+'.npy', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_path = \"./tools/input_converter.py\"\n",
    "model_in_path = \"model_tf.tflite\"\n",
    "model_out_path = \"converted_model.msgpack\"\n",
    "config_out_path = \"config.msgpack\"\n",
    "input_path = str(index)+ \".npy\"\n",
    "output_path = \"img_\" + str(index) + \".msgpack\"\n",
    "\n",
    "scale_factor = 512\n",
    "k = 17\n",
    "num_cols = 10 # not understand\n",
    "num_randoms = 1024 # not understand\n",
    "\n",
    "command_2 = [\"python\", f\"{convert_path}\", \"--model_config\", f\"{config_out_path}\",\n",
    "           \"--inputs\", input_path, \"--output\", output_path]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', './tools/input_converter.py', '--model_config', 'config.msgpack', '--inputs', '0.npy', '--output', 'img_0.msgpack'], returncode=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(command_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test, Compile and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer 0, type: FullyConnected, inp_idxes: [0, 3, 2], out_idxes: [5], layer_params: [1]\n",
      "Out 0 shape: [1, 56]\n",
      "\n",
      "Processing layer 1, type: FullyConnected, inp_idxes: [5, 4, 1], out_idxes: [6], layer_params: [0]\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "Processing layer 2, type: Softmax, inp_idxes: [6], out_idxes: [7], layer_params: []\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "final out[0] x: 0 (0)\n",
      "final out[1] x: 0 (0)\n",
      "final out[2] x: 0 (0)\n",
      "final out[3] x: 0 (0)\n",
      "final out[4] x: 0 (0)\n",
      "final out[5] x: 0 (0)\n",
      "final out[6] x: 0 (0)\n",
      "final out[7] x: 512 (1)\n",
      "final out[8] x: 0 (0)\n",
      "final out[9] x: 0 (0)\n",
      "final out idxes: [7]\n",
      "Processing layer 0, type: FullyConnected, inp_idxes: [0, 3, 2], out_idxes: [5], layer_params: [1]\n",
      "Out 0 shape: [1, 56]\n",
      "\n",
      "Processing layer 1, type: FullyConnected, inp_idxes: [5, 4, 1], out_idxes: [6], layer_params: [0]\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "Processing layer 2, type: Softmax, inp_idxes: [6], out_idxes: [7], layer_params: []\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "final out[0] x: 0 (0)\n",
      "final out[1] x: 0 (0)\n",
      "final out[2] x: 0 (0)\n",
      "final out[3] x: 0 (0)\n",
      "final out[4] x: 0 (0)\n",
      "final out[5] x: 0 (0)\n",
      "final out[6] x: 0 (0)\n",
      "final out[7] x: 512 (1)\n",
      "final out[8] x: 0 (0)\n",
      "final out[9] x: 0 (0)\n",
      "final out idxes: [7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['./bin/test_circuit', 'converted_model.msgpack', 'img_0.msgpack'], returncode=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"./bin/test_circuit\"\n",
    "model_out_path = \"converted_model.msgpack\"\n",
    "img_out_path = \"img_\" + str(index) + \".msgpack\"\n",
    "\n",
    "command_3 = [test, model_out_path, img_out_path]\n",
    "\n",
    "subprocess.run(command_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in params construction: 3.973671619s\n",
      "Processing layer 0, type: FullyConnected, inp_idxes: [0, 3, 2], out_idxes: [5], layer_params: [1]\n",
      "Out 0 shape: [1, 56]\n",
      "\n",
      "Processing layer 1, type: FullyConnected, inp_idxes: [5, 4, 1], out_idxes: [6], layer_params: [0]\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "Processing layer 2, type: Softmax, inp_idxes: [6], out_idxes: [7], layer_params: []\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "final out[0] x: 0 (0)\n",
      "final out[1] x: 0 (0)\n",
      "final out[2] x: 0 (0)\n",
      "final out[3] x: 0 (0)\n",
      "final out[4] x: 0 (0)\n",
      "final out[5] x: 0 (0)\n",
      "final out[6] x: 0 (0)\n",
      "final out[7] x: 0 (0)\n",
      "final out[8] x: 0 (0)\n",
      "final out[9] x: 0 (0)\n",
      "final out idxes: [7]\n",
      "Time elapsed in generating vkey: 1.831431293s\n",
      "vkey size: 148936 bytes\n",
      "Processing layer 0, type: FullyConnected, inp_idxes: [0, 3, 2], out_idxes: [5], layer_params: [1]\n",
      "Out 0 shape: [1, 56]\n",
      "\n",
      "Processing layer 1, type: FullyConnected, inp_idxes: [5, 4, 1], out_idxes: [6], layer_params: [0]\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "Processing layer 2, type: Softmax, inp_idxes: [6], out_idxes: [7], layer_params: []\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "final out[0] x: 0 (0)\n",
      "final out[1] x: 0 (0)\n",
      "final out[2] x: 0 (0)\n",
      "final out[3] x: 0 (0)\n",
      "final out[4] x: 0 (0)\n",
      "final out[5] x: 0 (0)\n",
      "final out[6] x: 0 (0)\n",
      "final out[7] x: 0 (0)\n",
      "final out[8] x: 0 (0)\n",
      "final out[9] x: 0 (0)\n",
      "final out idxes: [7]\n",
      "Time elapsed in generating pkey: 1.441003663s\n",
      "pkey size: 629294848 bytes\n",
      "Processing layer 0, type: FullyConnected, inp_idxes: [0, 3, 2], out_idxes: [5], layer_params: [1]\n",
      "Out 0 shape: [1, 56]\n",
      "\n",
      "Processing layer 1, type: FullyConnected, inp_idxes: [5, 4, 1], out_idxes: [6], layer_params: [0]\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "Processing layer 2, type: Softmax, inp_idxes: [6], out_idxes: [7], layer_params: []\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "final out[0] x: 0 (0)\n",
      "final out[1] x: 0 (0)\n",
      "final out[2] x: 0 (0)\n",
      "final out[3] x: 0 (0)\n",
      "final out[4] x: 0 (0)\n",
      "final out[5] x: 0 (0)\n",
      "final out[6] x: 0 (0)\n",
      "final out[7] x: 512 (1)\n",
      "final out[8] x: 0 (0)\n",
      "final out[9] x: 0 (0)\n",
      "final out idxes: [7]\n",
      "Time elapsed in filling circuit: 556.118759ms\n",
      "Public vals size: 320 bytes\n",
      "Processing layer 0, type: FullyConnected, inp_idxes: [0, 3, 2], out_idxes: [5], layer_params: [1]\n",
      "Out 0 shape: [1, 56]\n",
      "\n",
      "Processing layer 1, type: FullyConnected, inp_idxes: [5, 4, 1], out_idxes: [6], layer_params: [0]\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "Processing layer 2, type: Softmax, inp_idxes: [6], out_idxes: [7], layer_params: []\n",
      "Out 0 shape: [1, 10]\n",
      "\n",
      "final out[0] x: 0 (0)\n",
      "final out[1] x: 0 (0)\n",
      "final out[2] x: 0 (0)\n",
      "final out[3] x: 0 (0)\n",
      "final out[4] x: 0 (0)\n",
      "final out[5] x: 0 (0)\n",
      "final out[6] x: 0 (0)\n",
      "final out[7] x: 512 (1)\n",
      "final out[8] x: 0 (0)\n",
      "final out[9] x: 0 (0)\n",
      "final out idxes: [7]\n",
      "Proving time: 19.474252896s\n",
      "Proof size: 9536 bytes\n",
      "public vals: [0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000200, 0x0000000000000000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000000]\n",
      "Verifying time: 6.433303ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['./bin/time_circuit', 'converted_model.msgpack', 'img_0.msgpack', 'kzg'], returncode=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_circuit = \"./bin/time_circuit\"\n",
    "model_out_path = \"converted_model.msgpack\"\n",
    "img_out_path = \"img_\" + str(index) + \".msgpack\"\n",
    "\n",
    "command_4 = [time_circuit, model_out_path, img_out_path, \"kzg\"]\n",
    "\n",
    "subprocess.run(command_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
