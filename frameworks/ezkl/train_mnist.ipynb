{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LeNet-5 CNN Model (TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 11:29:42.328912: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-02 11:29:42.360313: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-02 11:29:42.360336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-02 11:29:42.361003: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-02 11:29:42.365618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 11:29:42.975449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 25, 25, 6)         102       \n",
      "                                                                 \n",
      " average_pooling2d_21 (Aver  (None, 12, 12, 6)         0         \n",
      " agePooling2D)                                                   \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 9, 9, 16)          1552      \n",
      "                                                                 \n",
      " average_pooling2d_22 (Aver  (None, 4, 4, 16)          0         \n",
      " agePooling2D)                                                   \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                1210      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33704 (131.66 KB)\n",
      "Trainable params: 33704 (131.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv1_filters=6\n",
    "conv2_filters=16\n",
    "fc1_units=120\n",
    "fc2_units=84\n",
    "\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(conv1_filters, kernel_size=(4, 4), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(conv2_filters, kernel_size=(4, 4), activation='relu'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(fc1_units),\n",
    "\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "    \n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.8230 - accuracy: 0.7356 - val_loss: 0.2241 - val_accuracy: 0.9308\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2116 - accuracy: 0.9355 - val_loss: 0.1359 - val_accuracy: 0.9595\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1367 - accuracy: 0.9579 - val_loss: 0.0967 - val_accuracy: 0.9688\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1028 - accuracy: 0.9684 - val_loss: 0.0789 - val_accuracy: 0.9770\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0829 - accuracy: 0.9744 - val_loss: 0.0714 - val_accuracy: 0.9788\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0715 - accuracy: 0.9779 - val_loss: 0.0707 - val_accuracy: 0.9772\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0633 - accuracy: 0.9806 - val_loss: 0.0500 - val_accuracy: 0.9860\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0560 - accuracy: 0.9822 - val_loss: 0.0531 - val_accuracy: 0.9848\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0504 - accuracy: 0.9844 - val_loss: 0.0529 - val_accuracy: 0.9835\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0451 - accuracy: 0.9858 - val_loss: 0.0434 - val_accuracy: 0.9880\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model_tf.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0444 - accuracy: 0.9863 - 241ms/epoch - 769us/step\n",
      "\n",
      "Test accuracy: 0.986299991607666\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 2 PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNetPT(nn.Module):\n",
    "    def __init__(self, conv1_filters, conv2_filters, fc1_units, fc2_units):\n",
    "        super(LeNetPT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, conv1_filters, 5)\n",
    "        self.conv2 = nn.Conv2d(conv1_filters, conv2_filters, 5)\n",
    "        # Calculate the flattened size after convolution and pooling\n",
    "        self.fc1 = nn.Linear(conv2_filters* 4 * 4, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(x.size(0),x.size(1),-1)  # 16 output channels\n",
    "        x = np.transpose(x, (0,2,1)).reshape(batch_size,-1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)  \n",
    "        return x\n",
    "\n",
    "# Instantiate the model with specific parameters\n",
    "model_pt = LeNetPT(conv1_filters, conv2_filters, fc1_units, fc2_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle multiple layers\n",
    "\n",
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[6].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the third dense layer (fc3) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[7].get_weights()\n",
    "model_pt.fc3.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc3.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled Input Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the image for TensorFlow\n",
    "controlled_input_tf = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "TF Basic Model Output: [[ -3.1242797   -0.2277504    3.8945203   -0.02813251  -7.1591253\n",
      "   -6.0940948  -11.932893     7.3735785   -2.5715997   -3.0597878 ]]\n",
      "PT Basic Model Output: [[ -3.124279    -0.22774982   3.8945203   -0.02813319  -7.159124\n",
      "   -6.0940943  -11.932896     7.3735785   -2.5715997   -3.059788  ]]\n"
     ]
    }
   ],
   "source": [
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "\n",
    "output_tf = model_tf.predict(controlled_input_tf) \n",
    "print(\"TF Basic Model Output:\", output_tf)\n",
    "print(\"PT Basic Model Output:\", output_pt.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Converted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the converted model on the test images: 98.6300%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming the TensorFlow MNIST data has already been loaded\n",
    "# Convert test_images to PyTorch tensor and permute\n",
    "test_images_pt = torch.tensor(test_images).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# Assuming test_labels are already loaded\n",
    "test_dataset = TensorDataset(test_images_pt, torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = evaluate_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the converted model on the test images: {accuracy:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0444 - accuracy: 0.9863 - 266ms/epoch - 849us/step\n",
      "Accuracy of the original model on the test images: 98.6300%\n"
     ]
    }
   ],
   "source": [
    "# Where the original TF model has:\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'Accuracy of the original model on the test images: {100 * test_acc:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mnist_images(dataset, indices = []):\n",
    "\n",
    "    if not indices:\n",
    "        fig, axes = plt.subplots(1, len(dataset), figsize=(12, 4))\n",
    "        \n",
    "        for i in range(len(axes)):\n",
    "            image, label = dataset[i]\n",
    "            ax = axes[i]\n",
    "            \n",
    "            image = image.squeeze()  # Remove channel dimension\n",
    "\n",
    "            ax.imshow(image, cmap='gray')\n",
    "            ax.set_title(f'Label: {label}')\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(indices), figsize=(12, 4))\n",
    "    if len(indices) == 1:  # If only one index is provided, wrap axes in a list\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, idx in zip(axes, indices):\n",
    "        image, label = dataset[idx]\n",
    "        image = image.squeeze()  # Remove channel dimension\n",
    "\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f'Label: {label}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mnist_images(test_dataset, [0,1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/model_tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/model_tf/assets\n",
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "folder = '../model/'\n",
    "# Original Model\n",
    "model_tf.save(folder + 'model_tf')\n",
    "model_tf.save(folder + 'model_tf.h5')\n",
    "\n",
    "# Converted Model\n",
    "torch.save(model_pt.state_dict(), folder + 'model_pt.pth')\n",
    "torch.save(model_pt, folder + 'model_pt_full.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = tf.keras.models.load_model(folder + 'model_tf')\n",
    "\n",
    "# model_tf = tf.keras.models.load_model(folder + 'model_tf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt = LeNetPT()\n",
    "model_pt.load_state_dict(torch.load('model_pt.pth'))\n",
    "model_pt.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# model_pt = torch.load('model_pt_full.pth')\n",
    "# model_pt.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl, os, json\n",
    "\n",
    "run_args = ezkl.PyRunArgs()\n",
    "run_args.input_visibility = \"public\"\n",
    "run_args.param_visibility = \"fixed\"\n",
    "run_args.output_visibility = \"public\"\n",
    "# run_args.num_inner_cols = 2\n",
    "run_args.variables = [(\"batch_size\", 0)]\n",
    "\n",
    "# Capture set of data points\n",
    "num_data_points = 8\n",
    "\n",
    "# Fetch data points from the train_dataset\n",
    "data_points = []\n",
    "for i, (data_point, _) in enumerate(test_loader):\n",
    "    if i >= num_data_points:\n",
    "        break\n",
    "    data_points.append(data_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(data_points).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_witness_LN(folder, model, data_point):\n",
    "    model_path = os.path.join(folder, 'network.onnx')\n",
    "    compiled_model_path = os.path.join(folder, 'network.compiled')\n",
    "    settings_path = os.path.join(folder, 'settings.json') \n",
    "    witness_path = os.path.join(folder, 'witness.json')\n",
    "    data_path = os.path.join(folder, 'input.json')\n",
    "\n",
    "    # cal_path = os.path.join(folder, \"cal_data.json\")\n",
    "    # srs_path = os.path.join(folder, 'kzg.srs')\n",
    "\n",
    "    model.eval()\n",
    "    # Verify the device (CPU or CUDA) and transfer the data point to the same device as the model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    data_point = data_point.to(device).unsqueeze(0)  # Add a batch dimension\n",
    "    # _, pred = torch.max(model(data_point),1)\n",
    "    # train_data_point = train_data_point.to(device)\n",
    "    # # Export the model to ONNX format\n",
    "    torch.onnx.export(model, \n",
    "                      data_point, \n",
    "                      model_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=10, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['input_0'], \n",
    "                      output_names=['output'])\n",
    "\n",
    "    # Convert the tensor to numpy array and reshape it for JSON serialization\n",
    "    x = (data_point.cpu().detach().numpy().reshape([-1])).tolist()\n",
    "    data = dict(input_data = [x])\n",
    "\n",
    "    # Serialize data into file:\n",
    "    json.dump(data, open(data_path, 'w'))\n",
    "\n",
    "    # data_points = []\n",
    "    # for i, (data_point, _) in enumerate(dataset):\n",
    "    #     if i >= num_data_points:\n",
    "    #         break\n",
    "    #     data_points.append(data_point)\n",
    "        \n",
    "    # # plot_mnist_images(data_points)\n",
    "\n",
    "    # # Stack the data points to create a batch\n",
    "    # train_data_batch = torch.stack(data_points)\n",
    "\n",
    "    # # Add a batch dimension if not already present\n",
    "    # if train_data_batch.dim() == 3: # dim == 5\n",
    "    #     train_data_batch = train_data_batch.unsqueeze(0)\n",
    "\n",
    "    # x = train_data_batch.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "\n",
    "    # data = dict(input_data = [x])\n",
    "    #res = ezkl.calibrate_settings(cal_data_path, model_path, settings_path, \"resources\", max_logrows = 12, scales = [2])\n",
    "\n",
    "    # cal_path = os.path.join('cal_data.json')\n",
    "    # # Serialize data into file:\n",
    "    # json.dump( data, open(cal_path, 'w' ))\n",
    "    os.environ['RUST_LOG'] = 'abc'\n",
    "    !RUST_LOG=none\n",
    "    res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "    assert res == True\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\", scales=[2,7])\n",
    "    assert res == True\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "    assert res == True\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    # srs path\n",
    "    res = ezkl.get_srs(settings_path)\n",
    "    os.environ['RUST_LOG'] = 'none'\n",
    "    !RUST_LOG=none\n",
    "    # now generate the witness file\n",
    "    res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "    assert os.path.isfile(witness_path)\n",
    "\n",
    "    with open(witness_path, \"r\") as f:\n",
    "        wit = json.load(f)\n",
    "\n",
    "    with open(settings_path, \"r\") as f:\n",
    "        setting = json.load(f)\n",
    "\n",
    "    prediction_array = []\n",
    "    for value in wit[\"outputs\"]:\n",
    "        for field_element in value:\n",
    "            prediction_array.append(ezkl.vecu64_to_float(field_element, setting['model_output_scales'][0]))\n",
    "    return torch.argmax(torch.Tensor([prediction_array]), dim=1)\n",
    "    #print ('Prediction:', torch.argmax(torch.Tensor([prediction_array]), dim=1) == label.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_accuracy(folder, model, dataset, size):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    # folder = \"./tmp/\"\n",
    "    # Create the directory 'tmp' in the current working directory\n",
    "    try:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        print(f\"Directory '{folder}' created successfully\")\n",
    "    except OSError as error:\n",
    "        print(f\"Directory '{folder}' cannot be created. Error: {error}\")\n",
    "\n",
    "    for image, _ in dataset:\n",
    "        pred_quantized = gen_witness_LN(folder, model, image)\n",
    "        outputs = model(image.unsqueeze(0).to(device))\n",
    "        pred = torch.argmax(outputs, 1)\n",
    "\n",
    "        total += 1\n",
    "        correct += (pred == pred_quantized.to(device))\n",
    "\n",
    "        if total > size:\n",
    "            break\n",
    "    \n",
    "    return 100*correct/total\n",
    "    #print (f'Test Accuracy: {accuracy:.2f}% (quantized)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './tmp/' created successfully\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'none' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport RUST_LOG=none\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcomputer_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tmp/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mcomputer_accuracy\u001b[0;34m(folder, model, dataset, size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot be created. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, _ \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 14\u001b[0m     pred_quantized \u001b[38;5;241m=\u001b[39m \u001b[43mgen_witness_LN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     16\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 59\u001b[0m, in \u001b[0;36mgen_witness_LN\u001b[0;34m(folder, model, data_point)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# data_points = []\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# for i, (data_point, _) in enumerate(dataset):\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     if i >= num_data_points:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# # Serialize data into file:\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# json.dump( data, open(cal_path, 'w' ))\u001b[39;00m\n\u001b[1;32m     58\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRUST_LOG\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 59\u001b[0m RUST_LOG\u001b[38;5;241m=\u001b[39m\u001b[43mnone\u001b[49m\n\u001b[1;32m     60\u001b[0m res \u001b[38;5;241m=\u001b[39m ezkl\u001b[38;5;241m.\u001b[39mgen_settings(model_path, settings_path, py_run_args\u001b[38;5;241m=\u001b[39mrun_args)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'none' is not defined"
     ]
    }
   ],
   "source": [
    "!export RUST_LOG=none\n",
    "computer_accuracy('./tmp/', model, test_dataset,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.], device='cuda:0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_LN_proof():\n",
    "    compiled_model_path = os.path.join(folder, 'network.compiled')\n",
    "    settings_path = os.path.join(folder, 'settings.json') \n",
    "    witness_path = os.path.join(folder, 'witness.json')\n",
    "    proof_path = os.path.join(folder, 'proof.json')\n",
    "    srs_path = os.path.join(folder, 'kzg.srs')\n",
    "\n",
    "    pk_path = os.path.join(folder, 'test.pk')\n",
    "    vk_path = os.path.join(folder, 'test.vk')\n",
    "\n",
    "\n",
    "    res = ezkl.mock(witness_path, compiled_model_path)\n",
    "    assert res == True\n",
    "\n",
    "    res = ezkl.setup(\n",
    "            compiled_model_path,\n",
    "            vk_path,\n",
    "            pk_path,\n",
    "        )\n",
    "\n",
    "\n",
    "    assert res == True\n",
    "    assert os.path.isfile(vk_path)\n",
    "    assert os.path.isfile(pk_path)\n",
    "    assert os.path.isfile(settings_path)\n",
    "\n",
    "    # Generate the proof\n",
    "    proof = ezkl.prove(\n",
    "            witness_path,\n",
    "            compiled_model_path,\n",
    "            pk_path,\n",
    "            proof_path,\n",
    "            \"single\",\n",
    "        )\n",
    "    print(proof)\n",
    "    assert os.path.isfile(proof_path)\n",
    "\n",
    "    # verify our proof\n",
    "    res = ezkl.verify(\n",
    "            proof_path,\n",
    "            settings_path,\n",
    "            vk_path,\n",
    "        )\n",
    "\n",
    "    assert res == True\n",
    "    print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawning module 2\n",
      "spawning module 2\n",
      "spawning module 2\n",
      "spawning module 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [[[10799958826997019694, 13732764088618192092, 9080164469342476020, 2707268858854354691], [14424721036421401993, 1925070949615222816, 15662063748651391231, 2658040235317113047], [1215616954971237343, 12781145858640582370, 3205897164118176512, 2866280021565530741], [16586418689708852361, 7719159825332846314, 11409209055533372999, 2915207530297009380], [6517656510695314680, 15914426195874827226, 4307566795511864752, 3240703511491837769], [4164547517116989137, 10669376283223625029, 6844937459063939938, 3458232284123655186], [10397520845017782036, 13129438026871282772, 16302847471085186583, 1825458023941667617], [6523463390768162622, 11459244905309910398, 17257037644834070972, 1430799349594821442], [13327914677011761971, 3991458216124611539, 4353392374763218262, 501949994929955151], [11035042354946947612, 2049718776522464053, 8894110146517096806, 3168295732512543866]]], 'proof': '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', 'transcript_type': 'EVM'}\n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "verify_LN_proof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
