{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most importantly, select the kernal at least 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "import torch, struct, os, psutil, subprocess, time, threading\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json, ezkl\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "test_images_pt = torch.tensor(test_images).float()\n",
    "test_labels_pt = torch.tensor(test_labels)\n",
    "# Flatten and normalize the images\n",
    "test_images_pt = test_images_pt.view(-1, 28*28) / 255.0  # Flatten and normalize\n",
    "\n",
    "# Assuming test_images_pt is your PyTorch tensor with shape [num_samples, 784]\n",
    "test_images_pt_reshaped = test_images_pt.view(-1, 1, 28, 28)  # Reshape to [num_samples, channels, height, width]\n",
    "\n",
    "# Downsample images\n",
    "test_images_pt_downsampled = F.interpolate(test_images_pt_reshaped, size=(14, 14), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Flatten the images back to [num_samples, 14*14]\n",
    "test_images_pt_downsampled = test_images_pt_downsampled.view(-1, 14*14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pytorch_model(model, datasets, labels):\n",
    "    # Create TensorDataset for test data\n",
    "    test_dataset = TensorDataset(datasets, labels)\n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, concurrent\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "def monitor_memory(pid, freq = 0.01):\n",
    "    p = psutil.Process(pid)\n",
    "    max_memory = 0\n",
    "    while True:\n",
    "        try:\n",
    "            mem = p.memory_info().rss / (1024 * 1024)\n",
    "            max_memory = max(max_memory, mem)\n",
    "        except psutil.NoSuchProcess:\n",
    "            break  # Process has finished\n",
    "        time.sleep(freq)  # Poll every second\n",
    "        \n",
    "    #print(f\"Maximum memory used: {max_memory} MB\")\n",
    "    return max_memory\n",
    "\n",
    "def execute_and_monitor(command, show = False):\n",
    "    start_time = time.time()\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(monitor_memory, process.pid)\n",
    "        stdout, stderr = process.communicate()\n",
    "        max_memory = future.result()\n",
    "    if show:\n",
    "        print(f\"Maximum memory used: {max_memory} MB\")\n",
    "        print(\"Total time:\", time.time() - start_time)\n",
    "    return stdout, stderr, max_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(test_images, predictions, model, output_folder='./tmp/'):\n",
    "    data_path = os.path.join(output_folder, 'input.json')\n",
    "    model_path = os.path.join(output_folder, 'network.onnx')\n",
    "\n",
    "    sampled_data = test_images[0]\n",
    "    torch.onnx.export(model, \n",
    "                        sampled_data, \n",
    "                        model_path, \n",
    "                        export_params=True, \n",
    "                        opset_version=10, \n",
    "                        do_constant_folding=True, \n",
    "                        input_names=['input_0'], \n",
    "                        output_names=['output'])\n",
    "    loss = 0\n",
    "    mem_usage = []\n",
    "    time_cost = []\n",
    "    benchmark_start_time = time.time()\n",
    "\n",
    "    for i, img in enumerate(test_images):\n",
    "        cost = 0\n",
    "        print (\"Process for image\", i)\n",
    "        start_time = time.time()\n",
    "        # Convert the tensor to numpy array and reshape it for JSON serialization\n",
    "        x = (img.cpu().detach().numpy().reshape([-1])).tolist()\n",
    "        data = dict(input_data = [x])\n",
    "\n",
    "        # Serialize data into file:\n",
    "        json.dump(data, open(data_path, 'w'))\n",
    "\n",
    "        command = [\"python\", \"gen_proof.py\", \"--model\", model_path, \"--data\", data_path, \"--output\", output_folder]\n",
    "\n",
    "        stdout, _, usage = execute_and_monitor(command)\n",
    "        pred = int(stdout[-2])\n",
    "\n",
    "        if pred != predictions[i]:\n",
    "            loss += 1\n",
    "            print (\"Loss happens on index\", i, \"predicted_class\", pred)\n",
    "        mem_usage.append(usage)\n",
    "        time_cost.append(time.time() - start_time)\n",
    "\n",
    "    print (\"Total time:\", time.time() - benchmark_start_time)\n",
    "    return loss, mem_usage, time_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../benchmarks/benchmark_results.csv' already exists.\n"
     ]
    }
   ],
   "source": [
    "csv_path = '../../benchmarks/benchmark_results.csv'\n",
    "\n",
    "columns = ['Framework', 'Architecture', '# Layers', '# Parameters', 'Testing Size', 'Accuracy Loss (%)', \n",
    "           'Avg Memory Usage (MB)', 'Std Memory Usage', 'Avg Proving Time (s)', 'Std Proving Time']\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.isfile(csv_path):\n",
    "    # Create a DataFrame with the specified columns\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    print(f\"File '{csv_path}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for 196_25_10 DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After login\n",
    "arch_folder = \"input-dense-dense/\"\n",
    "layers = [196,25,10]\n",
    "model_name = \"_\".join([str(x) for x in layers])\n",
    "model_path = \"../../models/\" + arch_folder + model_name\n",
    "\n",
    "state_dict = torch.load(model_path + \".pth\")\n",
    "\n",
    "output_folder = './tmp/' + \"_\".join([str(x) for x in layers]) + \"/\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])  # Flatten \n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model_pt = Net()\n",
    "model_pt.load_state_dict(state_dict)\n",
    "model_pt.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Ensure gradients are not computed\n",
    "    predictions = model_pt(test_images_pt_downsampled)\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "predicted_labels = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the PyTorch model on the test images: 95.41000000%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the PyTorch model\n",
    "accuracy = evaluate_pytorch_model(model_pt, test_images_pt_downsampled, test_labels_pt)\n",
    "print(f'Accuracy of the PyTorch model on the test images: {accuracy:.8f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process for image 0\n",
      "Process for image 1\n",
      "Process for image 2\n",
      "Process for image 3\n",
      "Process for image 4\n",
      "Process for image 5\n",
      "Process for image 6\n",
      "Process for image 7\n",
      "Process for image 8\n",
      "Process for image 9\n",
      "Total time: 7.1037983894348145\n"
     ]
    }
   ],
   "source": [
    "test_size = 10\n",
    "loss_, mem_usage_, time_cost_ = benchmark(test_images_pt_downsampled[:test_size], predicted_labels[:test_size], model_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    'Framework': ['ezkl (pytorch)'],\n",
    "    'Architecture': ['Input-Dense-Dense (196x24x14x10'],\n",
    "    '# Layers': [4],\n",
    "    '# Parameters': [5228],\n",
    "    'Testing Size': [test_size],\n",
    "    'Accuracy Loss (%)': [loss_/test_size*100],\n",
    "    'Avg Memory Usage (MB)': [sum(mem_usage_) / len(mem_usage_)],\n",
    "    'Std Memory Usage': [pd.Series(mem_usage_).std()],\n",
    "    'Avg Proving Time (s)': [sum(time_cost_) / len(time_cost_)],\n",
    "    'Std Proving Time': [pd.Series(time_cost_).std()]\n",
    "}\n",
    "\n",
    "new_row_df = pd.DataFrame(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for 196_24_14_10 DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After login\n",
    "arch_folder = \"input-dense-dense-dense/\"\n",
    "layers = [196,24,14,10]\n",
    "model_name = \"_\".join([str(x) for x in layers])\n",
    "model_path = \"../../models/\" + arch_folder + model_name\n",
    "\n",
    "state_dict = torch.load(model_path + \".pth\")\n",
    "\n",
    "output_folder = './tmp/' + \"_\".join([str(x) for x in layers]) + \"/\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])  # Flatten \n",
    "        self.fc2 = nn.Linear(layers[1], layers[2]) \n",
    "        self.fc3 = nn.Linear(layers[2], layers[3]) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model_pt = Net()\n",
    "model_pt.load_state_dict(state_dict)\n",
    "model_pt.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Ensure gradients are not computed\n",
    "    predictions = model_pt(test_images_pt_downsampled)\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "predicted_labels = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the PyTorch model on the test images: 95.56000000%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the PyTorch model\n",
    "accuracy = evaluate_pytorch_model(model_pt, test_images_pt_downsampled, test_labels_pt)\n",
    "print(f'Accuracy of the PyTorch model on the test images: {accuracy:.8f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process for image 0\n",
      "Process for image 1\n",
      "Process for image 2\n",
      "Process for image 3\n",
      "Process for image 4\n",
      "Process for image 5\n",
      "Process for image 6\n",
      "Process for image 7\n",
      "Process for image 8\n",
      "Process for image 9\n",
      "Total time: 7.431151866912842\n"
     ]
    }
   ],
   "source": [
    "test_size = 10\n",
    "loss_, mem_usage_, time_cost_ = benchmark(test_images_pt_downsampled[:test_size], predicted_labels[:test_size], model_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    'Framework': ['ezkl (pytorch)'],\n",
    "    'Architecture': ['Input-Dense-Dense (196x24x14x10'],\n",
    "    '# Layers': [4],\n",
    "    '# Parameters': [5228],\n",
    "    'Testing Size': [test_size],\n",
    "    'Accuracy Loss (%)': [loss_/test_size*100],\n",
    "    'Avg Memory Usage (MB)': [sum(mem_usage_) / len(mem_usage_)],\n",
    "    'Std Memory Usage': [pd.Series(mem_usage_).std()],\n",
    "    'Avg Proving Time (s)': [sum(time_cost_) / len(time_cost_)],\n",
    "    'Std Proving Time': [pd.Series(time_cost_).std()]\n",
    "}\n",
    "\n",
    "new_row_df = pd.DataFrame(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for 784_56_10 DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After login\n",
    "arch_folder = \"input-dense-dense/\"\n",
    "layers = [784,56,10]\n",
    "model_name = \"_\".join([str(x) for x in layers])\n",
    "model_path = \"../../models/\" + arch_folder + model_name\n",
    "\n",
    "state_dict = torch.load(model_path + \".pth\")\n",
    "\n",
    "output_folder = './tmp/' + \"_\".join([str(x) for x in layers]) + \"/\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])  # Flatten \n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model_pt = Net()\n",
    "model_pt.load_state_dict(state_dict)\n",
    "model_pt.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Ensure gradients are not computed\n",
    "    predictions = model_pt(test_images_pt)\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "predicted_labels = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the PyTorch model on the test images: 97.40000000%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the PyTorch model\n",
    "accuracy = evaluate_pytorch_model(model_pt, test_images_pt, test_labels_pt)\n",
    "print(f'Accuracy of the PyTorch model on the test images: {accuracy:.8f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process for image 0\n",
      "Process for image 1\n",
      "Process for image 2\n",
      "Process for image 3\n",
      "Process for image 4\n",
      "Process for image 5\n",
      "Process for image 6\n",
      "Process for image 7\n",
      "Process for image 8\n",
      "Process for image 9\n",
      "Total time: 30.195895195007324\n"
     ]
    }
   ],
   "source": [
    "test_size = 10\n",
    "loss_, mem_usage_, time_cost_ = benchmark(test_images_pt[:test_size], predicted_labels[:test_size], model_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    'Framework': ['ezkl (pytorch)'],\n",
    "    'Architecture': ['Input-Dense-Dense (196x24x14x10'],\n",
    "    '# Layers': [4],\n",
    "    '# Parameters': [5228],\n",
    "    'Testing Size': [test_size],\n",
    "    'Accuracy Loss (%)': [loss_/test_size*100],\n",
    "    'Avg Memory Usage (MB)': [sum(mem_usage_) / len(mem_usage_)],\n",
    "    'Std Memory Usage': [pd.Series(mem_usage_).std()],\n",
    "    'Avg Proving Time (s)': [sum(time_cost_) / len(time_cost_)],\n",
    "    'Std Proving Time': [pd.Series(time_cost_).std()]\n",
    "}\n",
    "\n",
    "new_row_df = pd.DataFrame(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
