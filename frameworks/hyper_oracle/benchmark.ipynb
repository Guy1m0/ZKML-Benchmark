{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 22:38:24.875431: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-03 22:38:24.908001: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-03 22:38:24.908026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-03 22:38:24.908709: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-03 22:38:24.913451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-03 22:38:25.585350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch, struct, os, psutil, subprocess, time, threading\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import concurrent.futures\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "test_images_pt = torch.tensor(test_images).float()\n",
    "test_labels_pt = torch.tensor(test_labels)\n",
    "# Flatten and normalize the images\n",
    "test_images_pt = test_images_pt.view(-1, 28*28) / 255.0  # Flatten and normalize\n",
    "\n",
    "# Assuming test_images_pt is your PyTorch tensor with shape [num_samples, 784]\n",
    "test_images_pt_reshaped = test_images_pt.view(-1, 1, 28, 28)  # Reshape to [num_samples, channels, height, width]\n",
    "\n",
    "# Downsample images\n",
    "test_images_pt_downsampled = F.interpolate(test_images_pt_reshaped, size=(14, 14), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Flatten the images back to [num_samples, 14*14]\n",
    "test_images_pt = test_images_pt_downsampled.view(-1, 14*14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset for test data\n",
    "test_dataset = TensorDataset(test_images_pt, test_labels_pt)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_from_file(data_file=\"input\", show=False):\n",
    "    try:\n",
    "        with open(data_file, 'rb') as file:\n",
    "            buf = file.read()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, e\n",
    "\n",
    "    digits = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "    l = int(np.sqrt(len(digits)))\n",
    "    if show:\n",
    "        c = \"\"\n",
    "        for row in range(l):\n",
    "            for col in range(l):\n",
    "                if buf[row * l + col] > 230:\n",
    "                    c += \"&\"\n",
    "                else:\n",
    "                    c += \"-\"\n",
    "            c += \"\\n\"\n",
    "        print(c)\n",
    "\n",
    "    return digits, None\n",
    "\n",
    "def save_img_to_file(image, data_file = \"input\"):\n",
    "    try:\n",
    "        # Convert to bytes\n",
    "        image_bytes = np.array(image*255).astype('uint8').tobytes()\n",
    "        \n",
    "        # Write to file\n",
    "        with open(data_file, 'wb') as file:\n",
    "            file.write(image_bytes)\n",
    "        \n",
    "        #print(f\"Image saved to {data_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_memory(pid, freq = 0.01):\n",
    "    p = psutil.Process(pid)\n",
    "    max_memory = 0\n",
    "    while True:\n",
    "        try:\n",
    "            mem = p.memory_info().rss / (1024 * 1024)\n",
    "            max_memory = max(max_memory, mem)\n",
    "        except psutil.NoSuchProcess:\n",
    "            break  # Process has finished\n",
    "        time.sleep(freq)  # Poll every second\n",
    "        \n",
    "    #print(f\"Maximum memory used: {max_memory} MB\")\n",
    "    return max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(test_images, tmp_folder, model_in_path, vm_file = \"./bin/vm\", program = \"./bin/mlgo_784.bin\", threaded = True):\n",
    "    print (vm_file, program)\n",
    "    \n",
    "    benchmark_start_time = time.time()\n",
    "    veri_infer = []\n",
    "    mem_usage = []\n",
    "    time_cost = []\n",
    "    for ind, img in enumerate(test_images):\n",
    "        img_out_path = tmp_folder + str(ind)\n",
    "        save_img_to_file(img, img_out_path)\n",
    "\n",
    "        # Exclusion of Pre-processing\n",
    "        start_time = time.time()\n",
    "        command = [f\"{vm_file}\", f\"--basedir={folder}\",\n",
    "                f\"--program={program}\", f\"--model={model_in_path}\", \n",
    "                f\"--data={img_out_path}\", \"--mipsVMCompatible\"]\n",
    "        \n",
    "        print (\"Process for image\", ind)\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Get the process ID\n",
    "        pid = process.pid\n",
    "\n",
    "        if threaded:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                future = executor.submit(monitor_memory, pid)\n",
    "                _, stderr = process.communicate()\n",
    "                max_memory = future.result()\n",
    "                #print(f\"Maximum memory used in multi-threaded mode: {max_memory} bytes\")\n",
    "        else:\n",
    "            max_memory = monitor_memory(pid)  # Run in the same thread\n",
    "            _, stderr = process.communicate()\n",
    "            #print(f\"Maximum memory used in single-threaded mode: {max_memory} bytes\")\n",
    "        veri_infer.append(int(stderr[-2]))\n",
    "        mem_usage.append(max_memory)\n",
    "        time_cost.append(time.time() - start_time)\n",
    "    \n",
    "    print (\"Total time:\", time.time() - benchmark_start_time)\n",
    "    #print (\"total mem:\", sum(mem))\n",
    "    return veri_infer, mem_usage, time_cost\n",
    "\n",
    "def calculate_loss(veri_infer, predicted_labels):\n",
    "    count = 0\n",
    "    for i in range(len(veri_infer)):\n",
    "        if veri_infer[i] != predicted_labels[i]:\n",
    "            count +=1\n",
    "            print (f\"Index {i} Not match!\")\n",
    "\n",
    "    return count/len(veri_infer)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../benchmarks/benchmark_results.csv' already exists.\n"
     ]
    }
   ],
   "source": [
    "csv_path = '../../benchmarks/benchmark_results.csv'\n",
    "\n",
    "columns = ['Framework', 'Architecture', '# Layers', '# Parameters', 'Testing Size', 'Accuracy Loss (%)', \n",
    "           'Avg Memory Usage (MB)', 'Std Memory Usage', 'Avg Proving Time (s)', 'Std Proving Time']\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.isfile(csv_path):\n",
    "    # Create a DataFrame with the specified columns\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    print(f\"File '{csv_path}' already exists.\")\n",
    "\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {} # Make a shared file for saving all the configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for 196_25_10 DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use json file for model (results, config etc) management\n",
    "arch_folder = \"input-dense-dense/\"\n",
    "layers = [196,25,10]\n",
    "model_name = \"_\".join([str(x) for x in layers])\n",
    "model_path = \"../../models/\"\n",
    "\n",
    "state_dict = torch.load(model_path + arch_folder+ model_name + \".pth\")\n",
    "list_vars = state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])  # Flatten \n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim = 1)\n",
    "    \n",
    "model_pt = Net()\n",
    "model_pt.load_state_dict(state_dict)\n",
    "model_pt.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Ensure gradients are not computed\n",
    "    predictions = model_pt(test_images_pt)\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "predicted_labels = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the PyTorch model on the test images: 95.41000000%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the PyTorch model\n",
    "accuracy = evaluate_pytorch_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the PyTorch model on the test images: {accuracy:.8f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./tmp/\"\n",
    "\n",
    "# Create the directory 'tmp' in the current working directory\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable: fc1.weight with shape:  (25, 196)\n",
      "Processing variable: fc1.bias with shape:  (25,)\n",
      "Processing variable: fc2.weight with shape:  (10, 25)\n",
      "Processing variable: fc2.bias with shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "fname_out = \"./bin/\" + arch_folder + \"ggml-model-\" + model_name + \".bin\"\n",
    "pack_fmt = \"!i\"\n",
    "\n",
    "os.makedirs(\"./bin/\" + arch_folder, exist_ok=True)\n",
    "\n",
    "fout = open(fname_out, \"w+b\")\n",
    "fout.write(struct.pack(pack_fmt, 0x67676d6c)) # magic: ggml in hex\n",
    "\n",
    "for name in list_vars.keys():\n",
    "    data = list_vars[name].squeeze().numpy()\n",
    "    print(\"Processing variable: \" + name + \" with shape: \", data.shape) \n",
    "    n_dims = len(data.shape)\n",
    "   \n",
    "    fout.write(struct.pack(pack_fmt, n_dims))\n",
    "    \n",
    "    data = data.astype(np.float32)\n",
    "    for i in range(n_dims):\n",
    "        fout.write(struct.pack(pack_fmt, data.shape[n_dims - 1 - i]))\n",
    "\n",
    "    # data\n",
    "    data = data.astype(\">f4\")\n",
    "    data.tofile(fout)\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 89\n",
    "img_out_path = folder + str(ind)\n",
    "save_img_to_file(test_images_pt[ind], img_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_file = \"./bin/vm\"\n",
    "program = \"./bin/mlgo_196.bin\"\n",
    "model_in_path = fname_out\n",
    "\n",
    "command = [f\"{vm_file}\", f\"--basedir={folder}\",\n",
    "                f\"--program={program}\", f\"--model={model_in_path}\", \n",
    "                f\"--data={img_out_path}\", \"--mipsVMCompatible\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 1727489\n",
      "opml outputs predicted class:1 where original model pred:1\n"
     ]
    }
   ],
   "source": [
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Get the process ID\n",
    "pid = process.pid\n",
    "print(f\"Process ID: {pid}\")\n",
    "\n",
    "# Start memory monitoring in a separate thread\n",
    "monitor_thread = threading.Thread(target=monitor_memory, args=(pid,))\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait for the process to complete and capture output\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "# Wait for the monitoring thread to finish\n",
    "monitor_thread.join()\n",
    "\n",
    "print (f\"opml outputs predicted class:{stderr[-2]} where original model pred:{predicted_labels[ind]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "--------------\n",
      "--------------\n",
      "--------------\n",
      "--------&-----\n",
      "--------------\n",
      "-------&------\n",
      "------&&------\n",
      "-------&------\n",
      "------&-------\n",
      "------&-------\n",
      "--------------\n",
      "--------------\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "digits, error = load_img_from_file(img_out_path,show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bin/vm ./bin/mlgo_196.bin\n",
      "Process for image 0\n",
      "Process for image 1\n",
      "Process for image 2\n",
      "Process for image 3\n",
      "Process for image 4\n",
      "Process for image 5\n",
      "Process for image 6\n",
      "Process for image 7\n",
      "Process for image 8\n",
      "Process for image 9\n",
      "Total time: 8.301805019378662\n"
     ]
    }
   ],
   "source": [
    "test_size = 10\n",
    "veri_infer, mem_usage, time_cost = benchmark(test_images_pt[:test_size], './tmp/',model_in_path, vm_file=vm_file, program=program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = calculate_loss(veri_infer, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    'Framework': ['opml (pytorch)'],\n",
    "    'Architecture': ['Input-Dense-Dense (196x25x10'],\n",
    "    '# Layers': [3],\n",
    "    '# Parameters': [5185],\n",
    "    'Testing Size': [len(veri_infer)],\n",
    "    'Accuracy Loss (%)': [loss_results],\n",
    "    'Avg Memory Usage (MB)': [sum(mem_usage) / len(mem_usage)],\n",
    "    'Std Memory Usage': [pd.Series(mem_usage).std()],\n",
    "    'Avg Proving Time (s)': [sum(time_cost) / len(time_cost)],\n",
    "    'Std Proving Time': [pd.Series(time_cost).std()]\n",
    "}\n",
    "\n",
    "new_row_df = pd.DataFrame(new_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Framework</th>\n",
       "      <th>Architecture</th>\n",
       "      <th># Layers</th>\n",
       "      <th># Parameters</th>\n",
       "      <th>Testing Size</th>\n",
       "      <th>Accuracy Loss (%)</th>\n",
       "      <th>Avg Memory Usage (MB)</th>\n",
       "      <th>Std Memory Usage</th>\n",
       "      <th>Avg Proving Time (s)</th>\n",
       "      <th>Std Proving Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>opml (pytorch)</td>\n",
       "      <td>Input-Dense-Dense (784 * 56 * 10)</td>\n",
       "      <td>3</td>\n",
       "      <td>44543</td>\n",
       "      <td>250</td>\n",
       "      <td>0.40</td>\n",
       "      <td>88.998094</td>\n",
       "      <td>2.285579</td>\n",
       "      <td>3.655122</td>\n",
       "      <td>0.440126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opml (pytorch)</td>\n",
       "      <td>Input-Dense-Dense (784 * 56 * 10, w/ relu)</td>\n",
       "      <td>3</td>\n",
       "      <td>44543</td>\n",
       "      <td>1000</td>\n",
       "      <td>20.90</td>\n",
       "      <td>89.122078</td>\n",
       "      <td>2.247846</td>\n",
       "      <td>3.664727</td>\n",
       "      <td>0.433921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>opml (pytorch)</td>\n",
       "      <td>Input-Dense-Dense (784 * 56 * 10)</td>\n",
       "      <td>3</td>\n",
       "      <td>44543</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.72</td>\n",
       "      <td>89.120883</td>\n",
       "      <td>2.254392</td>\n",
       "      <td>3.609974</td>\n",
       "      <td>0.421732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>opml (pytorch)</td>\n",
       "      <td>Input-Dense-Dense (196 * 25 * 10</td>\n",
       "      <td>3</td>\n",
       "      <td>5185</td>\n",
       "      <td>5000</td>\n",
       "      <td>3.52</td>\n",
       "      <td>74.351948</td>\n",
       "      <td>1.419725</td>\n",
       "      <td>0.809440</td>\n",
       "      <td>0.074357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Framework                                Architecture  # Layers  \\\n",
       "0  opml (pytorch)           Input-Dense-Dense (784 * 56 * 10)         3   \n",
       "1  opml (pytorch)  Input-Dense-Dense (784 * 56 * 10, w/ relu)         3   \n",
       "2  opml (pytorch)           Input-Dense-Dense (784 * 56 * 10)         3   \n",
       "3  opml (pytorch)            Input-Dense-Dense (196 * 25 * 10         3   \n",
       "\n",
       "   # Parameters  Testing Size  Accuracy Loss (%)  Avg Memory Usage (MB)  \\\n",
       "0         44543           250               0.40              88.998094   \n",
       "1         44543          1000              20.90              89.122078   \n",
       "2         44543          2500               0.72              89.120883   \n",
       "3          5185          5000               3.52              74.351948   \n",
       "\n",
       "   Std Memory Usage  Avg Proving Time (s)  Std Proving Time  \n",
       "0          2.285579              3.655122          0.440126  \n",
       "1          2.247846              3.664727          0.433921  \n",
       "2          2.254392              3.609974          0.421732  \n",
       "3          1.419725              0.809440          0.074357  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for 784_56_10 DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use json file for model (results, config etc) management\n",
    "arch_folder = \"input-dense-dense/\"\n",
    "layers = [196,25,10]\n",
    "model_name = \"_\".join([str(x) for x in layers])\n",
    "model_path = \"../../models/\"\n",
    "\n",
    "state_dict = torch.load(model_path + arch_folder+ model_name + \".pth\")\n",
    "list_vars = state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])  # Flatten \n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim = 1)\n",
    "    \n",
    "model_pt = Net()\n",
    "model_pt.load_state_dict(state_dict)\n",
    "model_pt.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Ensure gradients are not computed\n",
    "    predictions = model_pt(test_images_pt)\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "predicted_labels = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the PyTorch model\n",
    "accuracy = evaluate_pytorch_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the PyTorch model on the test images: {accuracy:.8f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fname_out = \"./bin/\" + arch_folder + \"ggml-model-\" + model_name + \".bin\"\n",
    "pack_fmt = \"!i\"\n",
    "\n",
    "os.makedirs(\"./bin/\" + arch_folder, exist_ok=True)\n",
    "\n",
    "fout = open(fname_out, \"w+b\")\n",
    "fout.write(struct.pack(pack_fmt, 0x67676d6c)) # magic: ggml in hex\n",
    "\n",
    "for name in list_vars.keys():\n",
    "    data = list_vars[name].squeeze().numpy()\n",
    "    print(\"Processing variable: \" + name + \" with shape: \", data.shape) \n",
    "    n_dims = len(data.shape)\n",
    "   \n",
    "    fout.write(struct.pack(pack_fmt, n_dims))\n",
    "    \n",
    "    data = data.astype(np.float32)\n",
    "    for i in range(n_dims):\n",
    "        fout.write(struct.pack(pack_fmt, data.shape[n_dims - 1 - i]))\n",
    "\n",
    "    # data\n",
    "    data = data.astype(\">f4\")\n",
    "    data.tofile(fout)\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10\n",
    "veri_infer, mem_usage, time_cost = benchmark(test_images_pt[:test_size], './tmp/',model_in_path, vm_file=vm_file, program=program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    'Framework': ['opml (pytorch)'],\n",
    "    'Architecture': ['Input-Dense-Dense (196x25x10'],\n",
    "    '# Layers': [3],\n",
    "    '# Parameters': [5185],\n",
    "    'Testing Size': [len(veri_infer)],\n",
    "    'Accuracy Loss (%)': [calculate_loss(veri_infer, predicted_labels)],\n",
    "    'Avg Memory Usage (MB)': [sum(mem_usage) / len(mem_usage)],\n",
    "    'Std Memory Usage': [pd.Series(mem_usage).std()],\n",
    "    'Avg Proving Time (s)': [sum(time_cost) / len(time_cost)],\n",
    "    'Std Proving Time': [pd.Series(time_cost).std()]\n",
    "}\n",
    "\n",
    "new_row_df = pd.DataFrame(new_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for 196_24_14_10 DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use json file for model (results, config etc) management\n",
    "arch_folder = \"input-dense-dense-dense/\"\n",
    "layers = [196,24,14,10]\n",
    "model_name = \"_\".join([str(x) for x in layers])\n",
    "model_path = \"../../models/\"\n",
    "\n",
    "state_dict = torch.load(model_path + arch_folder+ model_name + \".pth\")\n",
    "list_vars = state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 196_24_14_10\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])  # Flatten \n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc3 = nn.Linear(layers[2], num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim = 1)\n",
    "    \n",
    "model_pt = Net()\n",
    "model_pt.load_state_dict(state_dict)\n",
    "model_pt.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Ensure gradients are not computed\n",
    "    predictions = model_pt(test_images_pt)\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "predicted_labels = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_folder = './tmp/'\n",
    "os.makedirs(tmp_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_out = \"./bin/\" + arch_folder + \"ggml-model-\" + model_name + \".bin\"\n",
    "pack_fmt = \"!i\"\n",
    "\n",
    "os.makedirs(\"./bin/\" + arch_folder, exist_ok=True)\n",
    "\n",
    "fout = open(fname_out, \"w+b\")\n",
    "fout.write(struct.pack(pack_fmt, 0x67676d6c)) # magic: ggml in hex\n",
    "\n",
    "for name in list_vars.keys():\n",
    "    data = list_vars[name].squeeze().numpy()\n",
    "    print(\"Processing variable: \" + name + \" with shape: \", data.shape) \n",
    "    n_dims = len(data.shape)\n",
    "   \n",
    "    fout.write(struct.pack(pack_fmt, n_dims))\n",
    "    \n",
    "    data = data.astype(np.float32)\n",
    "    for i in range(n_dims):\n",
    "        fout.write(struct.pack(pack_fmt, data.shape[n_dims - 1 - i]))\n",
    "\n",
    "    # data\n",
    "    data = data.astype(\">f4\")\n",
    "    data.tofile(fout)\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = calculate_loss(veri_infer, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    'Framework': ['opml (pytorch)'],\n",
    "    'Architecture': ['Input-Dense-Dense (196x25x10'],\n",
    "    '# Layers': [3],\n",
    "    '# Parameters': [5185],\n",
    "    'Testing Size': [len(veri_infer)],\n",
    "    'Accuracy Loss (%)': [loss_results],\n",
    "    'Avg Memory Usage (MB)': [sum(mem_usage) / len(mem_usage)],\n",
    "    'Std Memory Usage': [pd.Series(mem_usage).std()],\n",
    "    'Avg Proving Time (s)': [sum(time_cost) / len(time_cost)],\n",
    "    'Std Proving Time': [pd.Series(time_cost).std()]\n",
    "}\n",
    "\n",
    "new_row_df = pd.DataFrame(new_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
