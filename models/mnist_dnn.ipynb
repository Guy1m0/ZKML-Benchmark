{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model for MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 13:51:35.651915: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-01 13:51:35.685223: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-01 13:51:35.685251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-01 13:51:35.685942: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-01 13:51:35.690704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 13:51:36.373427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and flatten the images\n",
    "train_images_tf = train_images.reshape((-1, 28*28)) / 255.0\n",
    "test_images_tf = test_images.reshape((-1, 28*28)) / 255.0\n",
    "\n",
    "# Convert to PyTorch format [batch_size, total pixels]\n",
    "# Since images are already normalized and flattened for TensorFlow, we can use the same arrays\n",
    "train_images_pt = torch.tensor(train_images_tf).float()\n",
    "test_images_pt = torch.tensor(test_images_tf).float()\n",
    "train_labels_pt = torch.tensor(train_labels)\n",
    "test_labels_pt = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "num_classes = 10\n",
    "\n",
    "model_tf = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28*28,)),  # Adjusted for 28x28 images\n",
    "    keras.layers.Dense(56, activation = 'relu'),                         # Additional hidden layer\n",
    "    keras.layers.Dense(num_classes, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "model_tf.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 56)                43960     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                570       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44530 (173.95 KB)\n",
      "Trainable params: 44530 (173.95 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "211/211 [==============================] - 1s 2ms/step - loss: 0.5871 - accuracy: 0.8424 - val_loss: 0.2383 - val_accuracy: 0.9363\n",
      "Epoch 2/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.9278 - val_loss: 0.1876 - val_accuracy: 0.9510\n",
      "Epoch 3/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9416 - val_loss: 0.1575 - val_accuracy: 0.9568\n",
      "Epoch 4/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9506 - val_loss: 0.1387 - val_accuracy: 0.9613\n",
      "Epoch 5/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9568 - val_loss: 0.1266 - val_accuracy: 0.9658\n",
      "Epoch 6/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9616 - val_loss: 0.1234 - val_accuracy: 0.9665\n",
      "Epoch 7/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9655 - val_loss: 0.1108 - val_accuracy: 0.9683\n",
      "Epoch 8/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9693 - val_loss: 0.1063 - val_accuracy: 0.9697\n",
      "Epoch 9/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9714 - val_loss: 0.1062 - val_accuracy: 0.9698\n",
      "Epoch 10/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9736 - val_loss: 0.1001 - val_accuracy: 0.9710\n",
      "Epoch 11/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9755 - val_loss: 0.0989 - val_accuracy: 0.9712\n",
      "Epoch 12/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9774 - val_loss: 0.0933 - val_accuracy: 0.9728\n",
      "Epoch 13/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9793 - val_loss: 0.0939 - val_accuracy: 0.9723\n",
      "Epoch 14/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9806 - val_loss: 0.0923 - val_accuracy: 0.9737\n",
      "Epoch 15/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9822 - val_loss: 0.0922 - val_accuracy: 0.9730\n",
      "Epoch 16/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.9829 - val_loss: 0.0889 - val_accuracy: 0.9740\n",
      "Epoch 17/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9840 - val_loss: 0.0871 - val_accuracy: 0.9747\n",
      "Epoch 18/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0523 - accuracy: 0.9857 - val_loss: 0.0867 - val_accuracy: 0.9733\n",
      "Epoch 19/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9864 - val_loss: 0.0894 - val_accuracy: 0.9733\n",
      "Epoch 20/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9874 - val_loss: 0.0849 - val_accuracy: 0.9752\n",
      "313/313 - 0s - loss: 0.0917 - accuracy: 0.9740 - 222ms/epoch - 709us/step\n",
      "\n",
      "Test accuracy: 0.9739999771118164\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_tf.fit(train_images_tf, train_labels, epochs=20, batch_size=256, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Pytorch DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 56)  # Flatten 28*28 and feed into 56 neurons\n",
    "        self.fc2 = nn.Linear(56, num_classes)  # 56 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim = 1)\n",
    "    \n",
    "model_pt = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[1].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "TensorFlow Basic Model Output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "PyTorch Basic Model Output: [[1.        2.7182817 1.        1.        1.        1.        1.\n",
      "  1.        1.        1.       ]]\n"
     ]
    }
   ],
   "source": [
    "# Select the image for TensorFlow and PyTorch\n",
    "controlled_input_tf = test_images[189].reshape(1, 28*28)  # Reshape to (1, 784) for DNN\n",
    "controlled_input_pt = torch.from_numpy(controlled_input_tf).float()\n",
    "\n",
    "# Test TensorFlow Model\n",
    "output_tf = model_tf.predict(controlled_input_tf) \n",
    "print(\"TensorFlow Basic Model Output:\", output_tf)\n",
    "\n",
    "# Test PyTorch Model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "print(\"PyTorch Basic Model Output:\", torch.exp(output_pt).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the PyTorch model on the test images: 97.40000000%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create TensorDataset for test data\n",
    "test_dataset = TensorDataset(test_images_pt, test_labels_pt)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the PyTorch model\n",
    "accuracy = evaluate_pytorch_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the PyTorch model on the test images: {accuracy:.8f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Converted Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_tf(model, test_images, batch_size=256):\n",
    "    predictions = []\n",
    "    for i in range(0, len(test_images), batch_size):\n",
    "        batch = test_images[i:i+batch_size]\n",
    "        pred = model.predict(batch)\n",
    "        predictions.extend(np.argmax(pred, axis=1))\n",
    "    return predictions\n",
    "\n",
    "def get_predictions_pt(model, test_images, batch_size=256):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_images), batch_size):\n",
    "            batch = test_images[i:i+batch_size]\n",
    "            pred = model(batch)\n",
    "            predictions.extend(torch.argmax(pred, axis=1).tolist())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 800us/step\n",
      "8/8 [==============================] - 0s 714us/step\n",
      "8/8 [==============================] - 0s 703us/step\n",
      "8/8 [==============================] - 0s 719us/step\n",
      "8/8 [==============================] - 0s 531us/step\n",
      "8/8 [==============================] - 0s 703us/step\n",
      "8/8 [==============================] - 0s 544us/step\n",
      "8/8 [==============================] - 0s 521us/step\n",
      "8/8 [==============================] - 0s 515us/step\n",
      "8/8 [==============================] - 0s 585us/step\n",
      "8/8 [==============================] - 0s 518us/step\n",
      "8/8 [==============================] - 0s 558us/step\n",
      "8/8 [==============================] - 0s 515us/step\n",
      "8/8 [==============================] - 0s 524us/step\n",
      "8/8 [==============================] - 0s 685us/step\n",
      "8/8 [==============================] - 0s 639us/step\n",
      "8/8 [==============================] - 0s 493us/step\n",
      "8/8 [==============================] - 0s 502us/step\n",
      "8/8 [==============================] - 0s 656us/step\n",
      "8/8 [==============================] - 0s 560us/step\n",
      "8/8 [==============================] - 0s 566us/step\n",
      "8/8 [==============================] - 0s 536us/step\n",
      "8/8 [==============================] - 0s 471us/step\n",
      "8/8 [==============================] - 0s 489us/step\n",
      "8/8 [==============================] - 0s 450us/step\n",
      "8/8 [==============================] - 0s 476us/step\n",
      "8/8 [==============================] - 0s 532us/step\n",
      "8/8 [==============================] - 0s 507us/step\n",
      "8/8 [==============================] - 0s 493us/step\n",
      "8/8 [==============================] - 0s 499us/step\n",
      "8/8 [==============================] - 0s 514us/step\n",
      "8/8 [==============================] - 0s 512us/step\n",
      "8/8 [==============================] - 0s 506us/step\n",
      "8/8 [==============================] - 0s 466us/step\n",
      "8/8 [==============================] - 0s 466us/step\n",
      "8/8 [==============================] - 0s 520us/step\n",
      "8/8 [==============================] - 0s 504us/step\n",
      "8/8 [==============================] - 0s 561us/step\n",
      "8/8 [==============================] - 0s 512us/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Number of mismatches: 0 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions_tf = get_predictions_tf(model_tf, test_images_tf)\n",
    "predictions_pt = get_predictions_pt(model_pt, test_images_pt)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(predictions_tf, predictions_pt))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images_tf)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orion Specialized q_aware_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 784)               3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_dense_4 (QuantizeWra  (None, 56)                43965     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_5 (QuantizeWra  (None, 10)                575       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44543 (174.00 KB)\n",
      "Trainable params: 44530 (173.95 KB)\n",
      "Non-trainable params: 13 (52.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Apply quantization to the layers\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model_tf)\n",
    "#q_aware_model.set_weights(model_tf.get_weights())\n",
    "\n",
    "# 'quantize_model' requires a recompile\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.0635 - accuracy: 0.9803 - val_loss: 0.1133 - val_accuracy: 0.9687\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0536 - accuracy: 0.9837 - val_loss: 0.0953 - val_accuracy: 0.9738\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0464 - accuracy: 0.9856 - val_loss: 0.0943 - val_accuracy: 0.9742\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0426 - accuracy: 0.9865 - val_loss: 0.1098 - val_accuracy: 0.9710\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0392 - accuracy: 0.9875 - val_loss: 0.0932 - val_accuracy: 0.9748\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0903 - val_accuracy: 0.9742\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0322 - accuracy: 0.9895 - val_loss: 0.1040 - val_accuracy: 0.9715\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.0937 - val_accuracy: 0.9762\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.0897 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0228 - accuracy: 0.9928 - val_loss: 0.0972 - val_accuracy: 0.9750\n",
      "313/313 - 0s - loss: 0.1078 - accuracy: 0.9746 - 189ms/epoch - 605us/step\n",
      "\n",
      "Test accuracy: 0.9746000170707703\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "history = q_aware_model.fit(train_images_tf, train_labels,\n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = q_aware_model.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 818us/step\n",
      "8/8 [==============================] - 0s 662us/step\n",
      "8/8 [==============================] - 0s 595us/step\n",
      "8/8 [==============================] - 0s 637us/step\n",
      "8/8 [==============================] - 0s 652us/step\n",
      "8/8 [==============================] - 0s 800us/step\n",
      "8/8 [==============================] - 0s 665us/step\n",
      "8/8 [==============================] - 0s 879us/step\n",
      "8/8 [==============================] - 0s 754us/step\n",
      "8/8 [==============================] - 0s 955us/step\n",
      "8/8 [==============================] - 0s 831us/step\n",
      "8/8 [==============================] - 0s 844us/step\n",
      "8/8 [==============================] - 0s 850us/step\n",
      "8/8 [==============================] - 0s 801us/step\n",
      "8/8 [==============================] - 0s 878us/step\n",
      "8/8 [==============================] - 0s 847us/step\n",
      "8/8 [==============================] - 0s 820us/step\n",
      "8/8 [==============================] - 0s 814us/step\n",
      "8/8 [==============================] - 0s 902us/step\n",
      "8/8 [==============================] - 0s 803us/step\n",
      "8/8 [==============================] - 0s 802us/step\n",
      "8/8 [==============================] - 0s 891us/step\n",
      "8/8 [==============================] - 0s 893us/step\n",
      "8/8 [==============================] - 0s 896us/step\n",
      "8/8 [==============================] - 0s 808us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 941us/step\n",
      "8/8 [==============================] - 0s 877us/step\n",
      "8/8 [==============================] - 0s 870us/step\n",
      "8/8 [==============================] - 0s 971us/step\n",
      "8/8 [==============================] - 0s 925us/step\n",
      "8/8 [==============================] - 0s 932us/step\n",
      "8/8 [==============================] - 0s 955us/step\n",
      "8/8 [==============================] - 0s 827us/step\n",
      "8/8 [==============================] - 0s 631us/step\n",
      "8/8 [==============================] - 0s 604us/step\n",
      "8/8 [==============================] - 0s 609us/step\n",
      "8/8 [==============================] - 0s 703us/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Number of mismatches: 149 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "q_predictions_tf = get_predictions_tf(q_aware_model, test_images_tf)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(q_predictions_tf, predictions_pt))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images_tf)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp__9wz2m0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp__9wz2m0/assets\n",
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-02-01 14:01:19.547451: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-01 14:01:19.547469: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-01 14:01:19.547708: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp__9wz2m0\n",
      "2024-02-01 14:01:19.548837: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-01 14:01:19.548848: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp__9wz2m0\n",
      "2024-02-01 14:01:19.551863: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-02-01 14:01:19.552803: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-01 14:01:19.594462: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp__9wz2m0\n",
      "2024-02-01 14:01:19.604902: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 57195 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 0, Total Ops 12, % non-converted = 0.00 %\n",
      " * \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2, uq_32: 2)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "\n",
    "# Indicate that you want to perform default optimizations,\n",
    "# which include quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Define a generator function that provides your test data's numpy arrays\n",
    "def representative_data_gen():\n",
    "  for i in range(500):\n",
    "    yield [np.array(train_images[i:i+1], dtype=np.float32)]\n",
    "\n",
    "# Use the generator function to guide the quantization process\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "q_aware_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Socathie Specialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Softmax\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# circom aware model\n",
    "inputs = Input(shape=(28*28,))\n",
    "out = Dense(56, activation='relu')(inputs)\n",
    "out = Dense(10)(out)\n",
    "out = Softmax()(out)\n",
    "\n",
    "c_aware_model = Model(inputs, out)\n",
    "c_aware_model.set_weights(model_tf.get_weights())\n",
    "\n",
    "c_aware_model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 821us/step\n",
      "8/8 [==============================] - 0s 764us/step\n",
      "8/8 [==============================] - 0s 720us/step\n",
      "8/8 [==============================] - 0s 735us/step\n",
      "8/8 [==============================] - 0s 669us/step\n",
      "8/8 [==============================] - 0s 590us/step\n",
      "8/8 [==============================] - 0s 503us/step\n",
      "8/8 [==============================] - 0s 524us/step\n",
      "8/8 [==============================] - 0s 499us/step\n",
      "8/8 [==============================] - 0s 579us/step\n",
      "8/8 [==============================] - 0s 569us/step\n",
      "8/8 [==============================] - 0s 503us/step\n",
      "8/8 [==============================] - 0s 483us/step\n",
      "8/8 [==============================] - 0s 678us/step\n",
      "8/8 [==============================] - 0s 645us/step\n",
      "8/8 [==============================] - 0s 670us/step\n",
      "8/8 [==============================] - 0s 668us/step\n",
      "8/8 [==============================] - 0s 703us/step\n",
      "8/8 [==============================] - 0s 699us/step\n",
      "8/8 [==============================] - 0s 702us/step\n",
      "8/8 [==============================] - 0s 737us/step\n",
      "8/8 [==============================] - 0s 677us/step\n",
      "8/8 [==============================] - 0s 655us/step\n",
      "8/8 [==============================] - 0s 683us/step\n",
      "8/8 [==============================] - 0s 658us/step\n",
      "8/8 [==============================] - 0s 642us/step\n",
      "8/8 [==============================] - 0s 707us/step\n",
      "8/8 [==============================] - 0s 754us/step\n",
      "8/8 [==============================] - 0s 726us/step\n",
      "8/8 [==============================] - 0s 680us/step\n",
      "8/8 [==============================] - 0s 654us/step\n",
      "8/8 [==============================] - 0s 718us/step\n",
      "8/8 [==============================] - 0s 686us/step\n",
      "8/8 [==============================] - 0s 657us/step\n",
      "8/8 [==============================] - 0s 721us/step\n",
      "8/8 [==============================] - 0s 660us/step\n",
      "8/8 [==============================] - 0s 686us/step\n",
      "8/8 [==============================] - 0s 711us/step\n",
      "8/8 [==============================] - 0s 686us/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Number of mismatches: 0 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "c_predictions_tf = get_predictions_tf(c_aware_model, test_images_tf)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(c_predictions_tf, predictions_tf))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images_tf)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp82wmzn8t/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp82wmzn8t/assets\n",
      "2024-02-01 14:05:09.465502: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-01 14:05:09.465521: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-01 14:05:09.465662: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp82wmzn8t\n",
      "2024-02-01 14:05:09.466128: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-01 14:05:09.466138: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp82wmzn8t\n",
      "2024-02-01 14:05:09.467372: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-01 14:05:09.492503: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp82wmzn8t\n",
      "2024-02-01 14:05:09.498890: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 33229 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 4, Total Ops 10, % non-converted = 40.00 %\n",
      " * 4 ARITH ops\n",
      "\n",
      "- arith.constant:    4 occurrences  (f32: 4)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Tensorflow\n",
    "arch_folder = \"./input-dense-dense/\"\n",
    "\n",
    "# Create the directory 'tmp' in the current working directory\n",
    "os.makedirs(arch_folder, exist_ok=True)\n",
    "\n",
    "model_name = \"784_56_10\"\n",
    "model_tf.save(arch_folder + model_name + '.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(arch_folder + model_name + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46944"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specialized Model\n",
    "\n",
    "# keras2circom\n",
    "c_aware_model.save(arch_folder + \"c_aware_\" + model_name + \".h5\")\n",
    "# kerase2cairo\n",
    "q_aware_model.save(arch_folder + \"q_aware_\" + model_name + \".h5\")\n",
    "open(arch_folder + \"q_aware_\"+model_name + \".tflite\", \"wb\").write(q_aware_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "\n",
    "# Save entire model\n",
    "torch.save(model_pt, arch_folder + model_name + \".pt\")\n",
    "# Save only the state_dict\n",
    "torch.save(model_pt.state_dict(), arch_folder + model_name + \".pth\")\n",
    "torch.onnx.export(model_pt, controlled_input_pt, arch_folder + model_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"input-dense-dense_784_56_10.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Number of mismatches: 0 out of 10000 samples\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# The input needs to be quantized, so we retrieve the quantization parameters\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "output_scale, output_zero_point = output_details[0]['quantization']\n",
    "\n",
    "# Normalize and quantize the test images\n",
    "test_images_quant = (test_images / input_scale + input_zero_point).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 3 but expected 2 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_images_quant[i], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set the value for the input tensor\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run the inference\u001b[39;00m\n\u001b[1;32m     10\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 3 but expected 2 for input 0."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate the quantized TFLite model\n",
    "correct_predictions = 0\n",
    "for i in range(len(test_images)):\n",
    "    test_image = np.expand_dims(test_images_quant[i], axis=0)\n",
    "    \n",
    "    # Set the value for the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    \n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Retrieve the output and dequantize\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output = np.argmax(output, axis=1)\n",
    "    predicted_class = output[0]\n",
    "    if predicted_class == test_labels[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / len(test_images) * 100\n",
    "print(f'Quantized model accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `orion` package.\n"
     ]
    }
   ],
   "source": [
    "!scarb new orion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Create an object with all tensors (an input + all weights and biases)\n",
    "tensors = {\n",
    "    \"fc1_weights\": interpreter.get_tensor(1), \n",
    "    \"fc1_bias\": interpreter.get_tensor(2), \n",
    "    \"fc2_weights\": interpreter.get_tensor(4), \n",
    "    \"fc2_bias\": interpreter.get_tensor(5)\n",
    "}\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('./orion_dnn/src/generated', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fixed_point(val, bits):\n",
    "    return round(val * (2**bits))\n",
    "\n",
    "def mnist_image_to_fixed_point(data):\n",
    "    return [to_fixed_point(val.item(), 16) for val in data]\n",
    "\n",
    "\n",
    "def generate_input_cairo(data):\n",
    "    values = mnist_image_to_fixed_point(data)\n",
    "    values = [f\"FixedTrait::<FP16x16>::new({val}, {'true' if val < 0 else 'false'})\" for val in values]\n",
    "    return \",\\n \".join(values)\n",
    "\n",
    "input_cairo = generate_input_cairo(controlled_input_tf[0])\n",
    "\n",
    "with open(\"orion_dnn/src/generated/input.cairo\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "use array::{SpanTrait, ArrayTrait};\n",
    "use orion::operators::tensor::{TensorTrait, FP16x16Tensor, Tensor};\n",
    "use orion::numbers::{FixedTrait, FP16x16};\n",
    "fn input() -> Tensor<FP16x16> {\n",
    "    TensorTrait::<FP16x16>::new(\n",
    "        array![196].span(),\n",
    "        array![\n",
    "    \"\"\")\n",
    "    f.write(input_cairo)\n",
    "    f.write(\"\"\"\n",
    "        ].span()\n",
    "    )\n",
    "}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tensor_name, tensor in tensors.items():\n",
    "    with open(os.path.join('./orion_dnn/src', 'generated', f\"{tensor_name}.cairo\"), \"w\") as f:\n",
    "        f.write(\n",
    "            \"use array::{ArrayTrait, SpanTrait};\\n\" +\n",
    "            \"use orion::operators::tensor::{core::{Tensor, TensorTrait}};\\n\" +\n",
    "            \"use orion::operators::tensor::FP16x16Tensor;\\n\" +\n",
    "            \"use orion::numbers::fixed_point::implementations::fp16x16::core::{FP16x16, FixedTrait};\\n\" +\n",
    "            \"\\n\" + f\"fn {tensor_name}() -> Tensor<FP16x16>\" + \"{\\n\\n\" + \n",
    "            \"let mut shape = ArrayTrait::new();\\n\"\n",
    "        )\n",
    "        for dim in tensor.shape:\n",
    "            f.write(f\"shape.append({dim});\\n\")\n",
    "        f.write(\"let mut data = ArrayTrait::new();\\n\")\n",
    "\n",
    "        for val in np.nditer(tensor.flatten()):\n",
    "            f.write(\"    data.append(i32 {{ mag: {0}, sign: {1} }});\\n\".format(abs(int(val)), str(val < 0).lower()))\n",
    "        f.write(\n",
    "            \"    TensorTrait::new(shape.span(), data.span())\\n\" +\n",
    "            \"}\\n\"\n",
    "        )\n",
    "\n",
    "with open(os.path.join('./orion_dnn/src', 'generated.cairo'), 'w') as f:\n",
    "    for param_name in tensors.keys():\n",
    "        f.write(f\"mod {param_name};\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 784)               3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrapp  (None, 56)                43965     \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWra  (None, 10)                575       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44543 (174.00 KB)\n",
      "Trainable params: 44530 (173.95 KB)\n",
      "Non-trainable params: 13 (52.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Apply quantization to the layers\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model_tf)\n",
    "\n",
    "# 'quantize_model' requires a recompile\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 12:38:59.657971: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.658006: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.659413: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.659424: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.659437: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.659808: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.659965: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.659974: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.857932: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.857941: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.859053: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.859349: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.859755: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.859765: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.859787: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-01-28 12:38:59.859798: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0963 - accuracy: 0.9720 - val_loss: 0.1078 - val_accuracy: 0.9672\n",
      "Epoch 2/3\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0788 - accuracy: 0.9758 - val_loss: 0.0958 - val_accuracy: 0.9718\n",
      "Epoch 3/3\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0657 - accuracy: 0.9806 - val_loss: 0.0903 - val_accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 3\n",
    "history = q_aware_model.fit(train_images_tf, train_labels,\n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4j4dhfnv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4j4dhfnv/assets\n",
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-01-28 12:39:13.507507: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-28 12:39:13.507524: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-28 12:39:13.507747: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp4j4dhfnv\n",
      "2024-01-28 12:39:13.508200: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-28 12:39:13.508209: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp4j4dhfnv\n",
      "2024-01-28 12:39:13.509435: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-01-28 12:39:13.509897: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-28 12:39:13.536105: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp4j4dhfnv\n",
      "2024-01-28 12:39:13.542437: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 34691 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 4, Total Ops 10, % non-converted = 40.00 %\n",
      " * 4 ARITH ops\n",
      "\n",
      "- arith.constant:    4 occurrences  (f32: 4)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got value of type FLOAT64 but expected type FLOAT32 for input 0, name: serving_default_input_1:0 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m converter\u001b[38;5;241m.\u001b[39minference_output_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mint8\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Convert the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Save the model to disk\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_aware_model.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite(tflite_model)\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1139\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1138\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1093\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1093\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1601\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1582\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1578\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1579\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_keras_to_saved_model(temp_dir)\n\u001b[1;32m   1580\u001b[0m   )\n\u001b[1;32m   1581\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[0;32m-> 1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1586\u001b[0m   shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1378\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m result \u001b[38;5;241m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1372\u001b[0m     input_data\u001b[38;5;241m=\u001b[39mgraph_def,\n\u001b[1;32m   1373\u001b[0m     input_tensors\u001b[38;5;241m=\u001b[39minput_tensors,\n\u001b[1;32m   1374\u001b[0m     output_tensors\u001b[38;5;241m=\u001b[39moutput_tensors,\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs,\n\u001b[1;32m   1376\u001b[0m )\n\u001b[0;32m-> 1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_tflite_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quant_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_new_quantizer\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1037\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   q_allow_float \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39mis_allow_float()\n\u001b[1;32m   1036\u001b[0m   q_variable_quantization \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39menable_mlir_variable_quantization\n\u001b[0;32m-> 1037\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_in_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_out_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_activations_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_bias_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_allow_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_variable_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m m_in_type \u001b[38;5;241m=\u001b[39m in_type \u001b[38;5;28;01mif\u001b[39;00m in_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m   1048\u001b[0m m_out_type \u001b[38;5;241m=\u001b[39m out_type \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:735\u001b[0m, in \u001b[0;36mTFLiteConverterBase._quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\u001b[0m\n\u001b[1;32m    731\u001b[0m calibrate_quantize \u001b[38;5;241m=\u001b[39m _calibrator\u001b[38;5;241m.\u001b[39mCalibrator(\n\u001b[1;32m    732\u001b[0m     result, custom_op_registerers_by_name, custom_op_registerers_by_func\n\u001b[1;32m    733\u001b[0m )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer:\n\u001b[0;32m--> 735\u001b[0m   calibrated \u001b[38;5;241m=\u001b[39m \u001b[43mcalibrate_quantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentative_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_gen\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only:\n\u001b[1;32m    740\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m calibrated\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:254\u001b[0m, in \u001b[0;36mCalibrator.calibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;129m@convert_phase\u001b[39m(Component\u001b[38;5;241m.\u001b[39mOPTIMIZE_TFLITE_MODEL, SubComponent\u001b[38;5;241m.\u001b[39mCALIBRATE)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalibrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_gen):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibrator\u001b[38;5;241m.\u001b[39mCalibrate()\n",
      "File \u001b[0;32m~/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:152\u001b[0m, in \u001b[0;36mCalibrator._feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibrator\u001b[38;5;241m.\u001b[39mFeedTensor(input_array, signature_key)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calibrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeedTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type FLOAT64 but expected type FLOAT32 for input 0, name: serving_default_input_1:0 "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "\n",
    "# Indicate that you want to perform default optimizations,\n",
    "# which include quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Define a generator function that provides your test data's numpy arrays\n",
    "def representative_data_gen():\n",
    "  for i in range(500):\n",
    "    yield [test_images_tf[i:i+1]]\n",
    "\n",
    "# Use the generator function to guide the quantization process\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"q_aware_model.tflite\", \"wb\").write(tflite_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the image for TensorFlow and PyTorch\n",
    "controlled_input_tf = test_images[34].reshape(1, 28*28)  # Reshape to (1, 784) for DNN\n",
    "controlled_input_pt = torch.from_numpy(controlled_input_tf).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object with all tensors \n",
    "#(an input + all weights and biases)\n",
    "tensors = {\n",
    "    \"input\": controlled_input_pt,\n",
    "    \"fc1_weights\": interpreter.get_tensor(1), \n",
    "    \"fc1_bias\": interpreter.get_tensor(2), \n",
    "    \"fc2_weights\": interpreter.get_tensor(4), \n",
    "    \"fc2_bias\": interpreter.get_tensor(5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('./orion_dnn/src/generated', exist_ok=True)\n",
    "\n",
    "for tensor_name, tensor in tensors.items():\n",
    "    with open(os.path.join('./orion_dnn/src', 'generated', f\"{tensor_name}.cairo\"), \"w\") as f:\n",
    "        f.write(\n",
    "            \"use core::array::ArrayTrait;\\n\" +\n",
    "            \"use orion::operators::tensor::{TensorTrait, Tensor, I32Tensor};\\n\" +\n",
    "            \"use orion::numbers::i32;\\n\\n\" +\n",
    "            \"\\nfn {0}() -> Tensor<i32> \".format(tensor_name) + \"{\\n\" +\n",
    "            \"    let mut shape = ArrayTrait::<usize>::new();\\n\"\n",
    "        )\n",
    "        for dim in tensor.shape:\n",
    "            f.write(\"    shape.append({0});\\n\".format(dim))\n",
    "        f.write(\n",
    "            \"    let mut data = ArrayTrait::<i32>::new();\\n\"\n",
    "        )\n",
    "        for val in np.nditer(tensor.flatten()):\n",
    "            f.write(\"    data.append(i32 {{ mag: {0}, sign: {1} }});\\n\".format(abs(int(val)), str(val < 0).lower()))\n",
    "        f.write(\n",
    "            \"    TensorTrait::new(shape.span(), data.span())\\n\" +\n",
    "            \"}\\n\"\n",
    "        )\n",
    "      \n",
    "with open(os.path.join('./orion_dnn/src', 'generated.cairo'), 'w') as f:\n",
    "    for param_name in tensors.keys():\n",
    "        f.write(f\"mod {param_name};\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch orion_dnn/src/nn.cairo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting orion_dnn/src/nn.cairo\n"
     ]
    }
   ],
   "source": [
    "%%writefile orion_dnn/src/nn.cairo\n",
    "use orion::operators::tensor::core::Tensor;\n",
    "use orion::numbers::signed_integer::{integer_trait::IntegerTrait, i32::i32};\n",
    "use orion::operators::nn::{NNTrait, I32NN};\n",
    "\n",
    "fn fc1(i: Tensor<i32>, w: Tensor<i32>, b: Tensor<i32>) -> Tensor<i32> {\n",
    "    let x = NNTrait::linear(i, w, b);\n",
    "    NNTrait::relu(@x)\n",
    "}\n",
    "\n",
    "fn fc2(i: Tensor<i32>, w: Tensor<i32>, b: Tensor<i32>) -> Tensor<i32> {\n",
    "    NNTrait::linear(i, w, b)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch orion_dnn/src/test.cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting orion_dnn/src/test.cairo\n"
     ]
    }
   ],
   "source": [
    "%%writefile orion_dnn/src/test.cairo\n",
    "use core::array::SpanTrait;\n",
    "\n",
    "use mnist_nn::nn::fc1;\n",
    "use mnist_nn::nn::fc2;\n",
    "use mnist_nn::generated::input::input;\n",
    "use mnist_nn::generated::fc1_bias::fc1_bias;\n",
    "use mnist_nn::generated::fc1_weights::fc1_weights;\n",
    "use mnist_nn::generated::fc2_bias::fc2_bias;\n",
    "use mnist_nn::generated::fc2_weights::fc2_weights;\n",
    "\n",
    "use orion::operators::tensor::I32Tensor;\n",
    "\n",
    "#[test]\n",
    "#[available_gas(99999999999999999)]\n",
    "fn mnist_nn_test() {\n",
    "    let input = input();\n",
    "    let fc1_bias = fc1_bias();\n",
    "    let fc1_weights = fc1_weights();\n",
    "    let fc2_bias = fc2_bias();\n",
    "    let fc2_weights = fc2_weights();\n",
    "\n",
    "    let x = fc1(input, fc1_weights, fc1_bias);\n",
    "    let x = fc2(x, fc2_weights, fc2_bias);\n",
    "\n",
    "    let x = *x.argmax(0, Option::None(()), Option::None(())).data.at(0);\n",
    "\n",
    "    assert(x == 3, 'should predict 2');\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31merror\u001b[0m: failed to read manifest at: /home/guy1m0/Desktop/ZKML-Benchmark/Milestone 2/MNIST-DNN/Scarb.toml\n",
      "\n",
      "Caused by:\n",
      "    No such file or directory (os error 2)\n"
     ]
    }
   ],
   "source": [
    "! cd orion_dnn & scarb run test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Giza-cli to automatically generate cairo script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fixed_point(val, bits):\n",
    "    return round(val * (2**bits))\n",
    "\n",
    "def mnist_image_to_fixed_point(data):\n",
    "    return [to_fixed_point(val.item(), 16) for val in data]\n",
    "\n",
    "\n",
    "def generate_input_cairo(data):\n",
    "    values = mnist_image_to_fixed_point(data)\n",
    "    values = [f\"FixedTrait::<FP16x16>::new({val}, {'true' if val < 0 else 'false'})\" for val in values]\n",
    "    return \",\\n \".join(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlled_input_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cairo = generate_input_cairo(controlled_input_pt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_cairo/src/input.cairo\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "use array::{SpanTrait, ArrayTrait};\n",
    "use orion::operators::tensor::{TensorTrait, FP16x16Tensor, Tensor};\n",
    "use orion::numbers::{FixedTrait, FP16x16};\n",
    "fn input() -> Tensor<FP16x16> {\n",
    "    TensorTrait::<FP16x16>::new(\n",
    "        array![196].span(),\n",
    "        array![\n",
    "    \"\"\")\n",
    "    f.write(input_cairo)\n",
    "    f.write(\"\"\"\n",
    "        ].span()\n",
    "    )\n",
    "}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
