{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model for MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 14:40:14.492325: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-02 14:40:14.524993: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-02 14:40:14.525018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-02 14:40:14.525678: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-02 14:40:14.530360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 14:40:15.205046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and flatten the images\n",
    "train_images_tf = train_images.reshape((-1, 28*28)) / 255.0\n",
    "test_images_tf = test_images.reshape((-1, 28*28)) / 255.0\n",
    "\n",
    "# Convert to PyTorch format [batch_size, total pixels]\n",
    "# Since images are already normalized and flattened for TensorFlow, we can use the same arrays\n",
    "train_images_pt = torch.tensor(train_images_tf).float()\n",
    "test_images_pt = torch.tensor(test_images_tf).float()\n",
    "train_labels_pt = torch.tensor(train_labels)\n",
    "test_labels_pt = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 14:40:21.494027: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.494664: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.494721: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.495320: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.495374: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.495411: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.546344: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.546428: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.546482: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 14:40:21.546524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1053 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# resize\n",
    "\n",
    "train_images_tf_reshaped = tf.reshape(train_images_tf, [-1, 28, 28, 1])  # Reshape to [num_samples, height, width, channels]\n",
    "test_images_tf_reshaped = tf.reshape(test_images_tf, [-1, 28, 28, 1])\n",
    "\n",
    "# Downsample images\n",
    "train_images_tf_downsampled = tf.image.resize(train_images_tf_reshaped, [14, 14], method='bilinear')\n",
    "test_images_tf_downsampled = tf.image.resize(test_images_tf_reshaped, [14, 14], method='bilinear')\n",
    "\n",
    "# Flatten the images back to [num_samples, 14*14]\n",
    "train_images_tf = tf.reshape(train_images_tf_downsampled, [-1, 14*14])\n",
    "test_images_tf = tf.reshape(test_images_tf_downsampled, [-1, 14*14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming test_images_pt is your PyTorch tensor with shape [num_samples, 784]\n",
    "test_images_pt_reshaped = test_images_pt.view(-1, 1, 28, 28)  # Reshape to [num_samples, channels, height, width]\n",
    "\n",
    "# Downsample images\n",
    "test_images_pt_downsampled = F.interpolate(test_images_pt_reshaped, size=(14, 14), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Flatten the images back to [num_samples, 14*14]\n",
    "test_images_pt = test_images_pt_downsampled.view(-1, 14*14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "#### Input-Dense-Dense 3_layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 25)                4925      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5185 (20.25 KB)\n",
      "Trainable params: 5185 (20.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "num_classes = 10\n",
    "input_size = 14 * 14\n",
    "layer_1 = 25\n",
    "\n",
    "model_tf = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(input_size,)),  # Adjusted for 28x28 images\n",
    "    keras.layers.Dense(layer_1, activation = 'relu'),                         # Additional hidden layer\n",
    "    keras.layers.Dense(num_classes, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "model_tf.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 14:40:35.840659: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-02 14:40:35.881460: I external/local_xla/xla/service/service.cc:168] XLA service 0x561a2de78a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-02 14:40:35.881474: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-02-02 14:40:35.884579: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-02 14:40:35.892674: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706856035.939233 1037340 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 1s 1ms/step - loss: 1.3213 - accuracy: 0.6456 - val_loss: 0.5634 - val_accuracy: 0.8847\n",
      "Epoch 2/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.4871 - accuracy: 0.8784 - val_loss: 0.3311 - val_accuracy: 0.9205\n",
      "Epoch 3/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.3598 - accuracy: 0.9029 - val_loss: 0.2686 - val_accuracy: 0.9320\n",
      "Epoch 4/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.3106 - accuracy: 0.9140 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
      "Epoch 5/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2822 - accuracy: 0.9210 - val_loss: 0.2193 - val_accuracy: 0.9410\n",
      "Epoch 6/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9263 - val_loss: 0.2073 - val_accuracy: 0.9432\n",
      "Epoch 7/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9299 - val_loss: 0.1959 - val_accuracy: 0.9467\n",
      "Epoch 8/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9327 - val_loss: 0.1888 - val_accuracy: 0.9487\n",
      "Epoch 9/20\n",
      "211/211 [==============================] - 0s 908us/step - loss: 0.2249 - accuracy: 0.9360 - val_loss: 0.1822 - val_accuracy: 0.9495\n",
      "Epoch 10/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9382 - val_loss: 0.1754 - val_accuracy: 0.9535\n",
      "Epoch 11/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9394 - val_loss: 0.1702 - val_accuracy: 0.9543\n",
      "Epoch 12/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2011 - accuracy: 0.9414 - val_loss: 0.1657 - val_accuracy: 0.9545\n",
      "Epoch 13/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9429 - val_loss: 0.1625 - val_accuracy: 0.9567\n",
      "Epoch 14/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1893 - accuracy: 0.9445 - val_loss: 0.1570 - val_accuracy: 0.9568\n",
      "Epoch 15/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9460 - val_loss: 0.1530 - val_accuracy: 0.9595\n",
      "Epoch 16/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9475 - val_loss: 0.1519 - val_accuracy: 0.9597\n",
      "Epoch 17/20\n",
      "211/211 [==============================] - 0s 994us/step - loss: 0.1739 - accuracy: 0.9492 - val_loss: 0.1475 - val_accuracy: 0.9605\n",
      "Epoch 18/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9496 - val_loss: 0.1456 - val_accuracy: 0.9605\n",
      "Epoch 19/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1656 - accuracy: 0.9512 - val_loss: 0.1422 - val_accuracy: 0.9617\n",
      "Epoch 20/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9522 - val_loss: 0.1404 - val_accuracy: 0.9620\n",
      "313/313 - 0s - loss: 0.1595 - accuracy: 0.9541 - 186ms/epoch - 594us/step\n",
      "\n",
      "Test accuracy: 0.9541000127792358\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_tf.fit(train_images_tf, train_labels, epochs=20, batch_size=256, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer_1)  # Flatten 28*28 and feed into 56 neurons\n",
    "        self.fc2 = nn.Linear(layer_1, num_classes)  # 56 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim = 1)\n",
    "    \n",
    "model_pt = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[1].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input-Dense-Dense-Dense 4_layers DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 24)                4728      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 14)                350       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                150       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5228 (20.42 KB)\n",
      "Trainable params: 5228 (20.42 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "num_classes = 10\n",
    "input_size = 14 * 14\n",
    "layer_1 = 24\n",
    "layer_2 = 14\n",
    "\n",
    "model_tf = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(input_size,)),  # Adjusted for 28x28 images\n",
    "    keras.layers.Dense(layer_1, activation = 'relu'),   \n",
    "    keras.layers.Dense(layer_2, activation = 'relu'),      # Additional hidden layer\n",
    "    keras.layers.Dense(num_classes, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "model_tf.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "211/211 [==============================] - 1s 1ms/step - loss: 1.3609 - accuracy: 0.5878 - val_loss: 0.5127 - val_accuracy: 0.8785\n",
      "Epoch 2/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.8743 - val_loss: 0.3067 - val_accuracy: 0.9163\n",
      "Epoch 3/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.3438 - accuracy: 0.9005 - val_loss: 0.2544 - val_accuracy: 0.9305\n",
      "Epoch 4/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.9130 - val_loss: 0.2260 - val_accuracy: 0.9380\n",
      "Epoch 5/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2740 - accuracy: 0.9211 - val_loss: 0.2048 - val_accuracy: 0.9422\n",
      "Epoch 6/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9277 - val_loss: 0.1912 - val_accuracy: 0.9442\n",
      "Epoch 7/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9312 - val_loss: 0.1796 - val_accuracy: 0.9480\n",
      "Epoch 8/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2225 - accuracy: 0.9357 - val_loss: 0.1701 - val_accuracy: 0.9498\n",
      "Epoch 9/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9388 - val_loss: 0.1627 - val_accuracy: 0.9518\n",
      "Epoch 10/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9423 - val_loss: 0.1607 - val_accuracy: 0.9533\n",
      "Epoch 11/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9431 - val_loss: 0.1495 - val_accuracy: 0.9560\n",
      "Epoch 12/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9460 - val_loss: 0.1450 - val_accuracy: 0.9570\n",
      "Epoch 13/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9482 - val_loss: 0.1397 - val_accuracy: 0.9588\n",
      "Epoch 14/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9494 - val_loss: 0.1371 - val_accuracy: 0.9587\n",
      "Epoch 15/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9514 - val_loss: 0.1349 - val_accuracy: 0.9583\n",
      "Epoch 16/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9528 - val_loss: 0.1303 - val_accuracy: 0.9612\n",
      "Epoch 17/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9535 - val_loss: 0.1309 - val_accuracy: 0.9620\n",
      "Epoch 18/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9546 - val_loss: 0.1268 - val_accuracy: 0.9635\n",
      "Epoch 19/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9556 - val_loss: 0.1269 - val_accuracy: 0.9630\n",
      "Epoch 20/20\n",
      "211/211 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9570 - val_loss: 0.1242 - val_accuracy: 0.9647\n",
      "313/313 - 0s - loss: 0.1456 - accuracy: 0.9556 - 239ms/epoch - 763us/step\n",
      "\n",
      "Test accuracy: 0.9556000232696533\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_tf.fit(train_images_tf, train_labels, epochs=20, batch_size=256, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer_1)  # Flatten 28*28 and feed into 56 neurons\n",
    "        self.fc2 = nn.Linear(layer_1, layer_2)\n",
    "        self.fc3 = nn.Linear(layer_2, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim = 1)\n",
    "    \n",
    "model_pt = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[1].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt.fc3.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc3.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Converted model_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "TensorFlow Basic Model Output: [[1.4794455e-04 9.7048104e-01 1.1314460e-03 8.0442689e-03 5.0906963e-03\n",
      "  7.1880864e-03 3.6779244e-03 1.4568124e-03 1.5915688e-03 1.1902171e-03]]\n",
      "PyTorch Basic Model Output: tensor([[1.4794e-04, 9.7048e-01, 1.1314e-03, 8.0443e-03, 5.0907e-03, 7.1881e-03,\n",
      "         3.6779e-03, 1.4568e-03, 1.5916e-03, 1.1902e-03]])\n"
     ]
    }
   ],
   "source": [
    "# Select the image for TensorFlow and PyTorch\n",
    "controlled_input_tf = test_images_tf[189:190]  # Reshape to (1, 784) for DNN\n",
    "controlled_input_pt = test_images_pt[189:190]\n",
    "\n",
    "# Test TensorFlow Model\n",
    "output_tf = model_tf.predict(controlled_input_tf) \n",
    "print(\"TensorFlow Basic Model Output:\", output_tf)\n",
    "\n",
    "# Test PyTorch Model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the PyTorch model on the test images: 95.56000000%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create TensorDataset for test data\n",
    "test_dataset = TensorDataset(test_images_pt, test_labels_pt)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the PyTorch model\n",
    "accuracy = evaluate_pytorch_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the PyTorch model on the test images: {accuracy:.8f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_tf(model, test_images, batch_size=256):\n",
    "    predictions = []\n",
    "    for i in range(0, len(test_images), batch_size):\n",
    "        batch = test_images[i:i+batch_size]\n",
    "        pred = model.predict(batch)\n",
    "        predictions.extend(np.argmax(pred, axis=1))\n",
    "    return predictions\n",
    "\n",
    "def get_predictions_pt(model, test_images, batch_size=256):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_images), batch_size):\n",
    "            batch = test_images[i:i+batch_size]\n",
    "            pred = model(batch)\n",
    "            predictions.extend(torch.argmax(pred, axis=1).tolist())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 895us/step\n",
      "8/8 [==============================] - 0s 701us/step\n",
      "8/8 [==============================] - 0s 730us/step\n",
      "8/8 [==============================] - 0s 755us/step\n",
      "8/8 [==============================] - 0s 768us/step\n",
      "8/8 [==============================] - 0s 800us/step\n",
      "8/8 [==============================] - 0s 714us/step\n",
      "8/8 [==============================] - 0s 728us/step\n",
      "8/8 [==============================] - 0s 732us/step\n",
      "8/8 [==============================] - 0s 752us/step\n",
      "8/8 [==============================] - 0s 736us/step\n",
      "8/8 [==============================] - 0s 705us/step\n",
      "8/8 [==============================] - 0s 701us/step\n",
      "8/8 [==============================] - 0s 684us/step\n",
      "8/8 [==============================] - 0s 759us/step\n",
      "8/8 [==============================] - 0s 700us/step\n",
      "8/8 [==============================] - 0s 736us/step\n",
      "8/8 [==============================] - 0s 778us/step\n",
      "8/8 [==============================] - 0s 712us/step\n",
      "8/8 [==============================] - 0s 723us/step\n",
      "8/8 [==============================] - 0s 799us/step\n",
      "8/8 [==============================] - 0s 708us/step\n",
      "8/8 [==============================] - 0s 672us/step\n",
      "8/8 [==============================] - 0s 733us/step\n",
      "8/8 [==============================] - 0s 663us/step\n",
      "8/8 [==============================] - 0s 728us/step\n",
      "8/8 [==============================] - 0s 656us/step\n",
      "8/8 [==============================] - 0s 475us/step\n",
      "8/8 [==============================] - 0s 840us/step\n",
      "8/8 [==============================] - 0s 640us/step\n",
      "8/8 [==============================] - 0s 644us/step\n",
      "8/8 [==============================] - 0s 688us/step\n",
      "8/8 [==============================] - 0s 638us/step\n",
      "8/8 [==============================] - 0s 472us/step\n",
      "8/8 [==============================] - 0s 863us/step\n",
      "8/8 [==============================] - 0s 680us/step\n",
      "8/8 [==============================] - 0s 700us/step\n",
      "8/8 [==============================] - 0s 726us/step\n",
      "8/8 [==============================] - 0s 773us/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Number of mismatches: 0 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions_tf = get_predictions_tf(model_tf, test_images_tf)\n",
    "predictions_pt = get_predictions_pt(model_pt, test_images_pt)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(predictions_tf, predictions_pt))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images_tf)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orion Specialized q_aware_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_6 (Quantize  (None, 196)               3         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " quant_dense_6 (QuantizeWra  (None, 24)                4733      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_7 (QuantizeWra  (None, 14)                355       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_8 (QuantizeWra  (None, 10)                155       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5246 (20.49 KB)\n",
      "Trainable params: 5228 (20.42 KB)\n",
      "Non-trainable params: 18 (72.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Apply quantization to the layers\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model_tf)\n",
    "#q_aware_model.set_weights(model_tf.get_weights())\n",
    "\n",
    "# 'quantize_model' requires a recompile\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1575 - accuracy: 0.9532 - val_loss: 0.1227 - val_accuracy: 0.9643\n",
      "313/313 - 0s - loss: 0.1431 - accuracy: 0.9582 - 227ms/epoch - 726us/step\n",
      "\n",
      "Test accuracy: 0.9581999778747559\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "history = q_aware_model.fit(train_images_tf, train_labels,\n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = q_aware_model.evaluate(test_images_tf, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 908us/step\n",
      "8/8 [==============================] - 0s 930us/step\n",
      "8/8 [==============================] - 0s 727us/step\n",
      "8/8 [==============================] - 0s 833us/step\n",
      "8/8 [==============================] - 0s 863us/step\n",
      "8/8 [==============================] - 0s 789us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 717us/step\n",
      "8/8 [==============================] - 0s 644us/step\n",
      "8/8 [==============================] - 0s 671us/step\n",
      "8/8 [==============================] - 0s 604us/step\n",
      "8/8 [==============================] - 0s 633us/step\n",
      "8/8 [==============================] - 0s 920us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 626us/step\n",
      "8/8 [==============================] - 0s 747us/step\n",
      "8/8 [==============================] - 0s 858us/step\n",
      "8/8 [==============================] - 0s 633us/step\n",
      "8/8 [==============================] - 0s 675us/step\n",
      "8/8 [==============================] - 0s 646us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 880us/step\n",
      "8/8 [==============================] - 0s 719us/step\n",
      "8/8 [==============================] - 0s 690us/step\n",
      "8/8 [==============================] - 0s 728us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 994us/step\n",
      "8/8 [==============================] - 0s 673us/step\n",
      "8/8 [==============================] - 0s 656us/step\n",
      "8/8 [==============================] - 0s 795us/step\n",
      "8/8 [==============================] - 0s 731us/step\n",
      "8/8 [==============================] - 0s 803us/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Number of mismatches: 138 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "q_predictions_tf = get_predictions_tf(q_aware_model, test_images_tf)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(q_predictions_tf, predictions_pt))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images_tf)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmprjafcazd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmprjafcazd/assets\n",
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-02-02 18:30:06.052654: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-02 18:30:06.052673: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-02 18:30:06.052761: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmprjafcazd\n",
      "2024-02-02 18:30:06.053851: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-02 18:30:06.053858: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmprjafcazd\n",
      "2024-02-02 18:30:06.056923: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-02 18:30:06.088081: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmprjafcazd\n",
      "2024-02-02 18:30:06.096709: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 43947 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 0, Total Ops 15, % non-converted = 0.00 %\n",
      " * \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n",
      "  (uq_8: 3)\n",
      "  (uq_8: 3, uq_32: 3)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "\n",
    "# Indicate that you want to perform default optimizations,\n",
    "# which include quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Define a generator function that provides your test data's numpy arrays\n",
    "def representative_data_gen():\n",
    "  for i in range(500):\n",
    "    yield [np.array(train_images[i:i+1], dtype=np.float32)]\n",
    "\n",
    "# Use the generator function to guide the quantization process\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "q_aware_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Models for 196_25_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgdyu27rf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgdyu27rf/assets\n",
      "2024-02-02 14:56:06.902758: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-02 14:56:06.902775: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-02 14:56:06.903007: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpgdyu27rf\n",
      "2024-02-02 14:56:06.903629: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-02 14:56:06.903639: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpgdyu27rf\n",
      "2024-02-02 14:56:06.905303: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-02-02 14:56:06.905787: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-02 14:56:06.932669: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpgdyu27rf\n",
      "2024-02-02 14:56:06.939168: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 36162 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 4, Total Ops 10, % non-converted = 40.00 %\n",
      " * 4 ARITH ops\n",
      "\n",
      "- arith.constant:    4 occurrences  (f32: 4)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Tensorflow\n",
    "arch_folder = \"./input-dense-dense/\"\n",
    "os.makedirs(arch_folder, exist_ok=True)\n",
    "\n",
    "model_name = \"196_25_10\"\n",
    "model_tf.save(arch_folder + model_name + '.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(arch_folder + model_name + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7456"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specialized Model\n",
    "\n",
    "# kerase2cairo\n",
    "q_aware_model.save(arch_folder + \"q_aware_\" + model_name + \".h5\")\n",
    "open(arch_folder + \"q_aware_\"+model_name + \".tflite\", \"wb\").write(q_aware_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "\n",
    "# Save entire model\n",
    "torch.save(model_pt, arch_folder + model_name + \".pt\")\n",
    "# Save only the state_dict\n",
    "torch.save(model_pt.state_dict(), arch_folder + model_name + \".pth\")\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model_pt, controlled_input_pt, arch_folder + model_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model for 196_24_14_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2xuk24hy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp2xuk24hy/assets\n",
      "2024-02-02 18:31:49.180569: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-02 18:31:49.180588: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-02 18:31:49.180678: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp2xuk24hy\n",
      "2024-02-02 18:31:49.181064: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-02 18:31:49.181070: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp2xuk24hy\n",
      "2024-02-02 18:31:49.182149: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-02 18:31:49.206977: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp2xuk24hy\n",
      "2024-02-02 18:31:49.212153: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 31475 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 13, % non-converted = 46.15 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (f32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Tensorflow\n",
    "arch_folder = \"./input-dense-dense-dense/\"\n",
    "os.makedirs(arch_folder, exist_ok=True)\n",
    "\n",
    "model_name = \"196_24_14_10\"\n",
    "model_tf.save(arch_folder + model_name + '.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(arch_folder + model_name + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy1m0/Desktop/ZKML-Benchmark/env/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8200"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specialized Model\n",
    "\n",
    "# kerase2cairo\n",
    "q_aware_model.save(arch_folder + \"q_aware_\" + model_name + \".h5\")\n",
    "open(arch_folder + \"q_aware_\"+model_name + \".tflite\", \"wb\").write(q_aware_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "\n",
    "# Save entire model\n",
    "torch.save(model_pt, arch_folder + model_name + \".pt\")\n",
    "# Save only the state_dict\n",
    "torch.save(model_pt.state_dict(), arch_folder + model_name + \".pth\")\n",
    "with torch.no_grad():   \n",
    "    torch.onnx.export(model_pt, controlled_input_pt, arch_folder + model_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
